diff --git a/config/stdint.m4 b/config/stdint.m4
index fbdd586..985a4a8 100644
--- a/config/stdint.m4
+++ b/config/stdint.m4
@@ -129,19 +129,19 @@ AC_MSG_RESULT($acx_cv_header_stdint $acx_cv_header_stdint_kind)
 
 # Lacking an uintptr_t?  Test size of void *
 case "$acx_cv_header_stdint:$ac_cv_type_uintptr_t" in
-  stddef.h:* | *:no) AC_CHECK_SIZEOF(void *) ;;
+  stddef.h:* | *:no) AC_CHECK_SIZEOF(void *,,/* no standard headers */) ;;
 esac
 
 # Lacking an uint64_t?  Test size of long
 case "$acx_cv_header_stdint:$ac_cv_type_uint64_t:$ac_cv_type_u_int64_t" in
-  stddef.h:*:* | *:no:no) AC_CHECK_SIZEOF(long) ;;
+  stddef.h:*:* | *:no:no) AC_CHECK_SIZEOF(long,,/* no standard headers */) ;;
 esac
 
 if test $acx_cv_header_stdint = stddef.h; then
   # Lacking a good header?  Test size of everything and deduce all types.
-  AC_CHECK_SIZEOF(int)
-  AC_CHECK_SIZEOF(short)
-  AC_CHECK_SIZEOF(char)
+  AC_CHECK_SIZEOF(int,,/* no standard headers */)
+  AC_CHECK_SIZEOF(short,,/* no standard headers */)
+  AC_CHECK_SIZEOF(char,,/* no standard headers */)
 
   AC_MSG_CHECKING(for type equivalent to int8_t)
   case "$ac_cv_sizeof_char" in
diff --git a/fixincludes/tests/base/sys/types.h b/fixincludes/tests/base/sys/types.h
index 683b5e9..0bb088c 100644
--- a/fixincludes/tests/base/sys/types.h
+++ b/fixincludes/tests/base/sys/types.h
@@ -28,3 +28,4 @@ typedef __WCHAR_TYPE__ wchar_t;
 
 #endif /* ushort_t */
 #endif  /* GNU_TYPES_CHECK */
+#if  !defined(__STRICT_ANSI__) && !defined(_NO_LONGLONG)
diff --git a/gcc/Makefile.in b/gcc/Makefile.in
index e0b952f..551d843 100644
--- a/gcc/Makefile.in
+++ b/gcc/Makefile.in
@@ -378,6 +378,8 @@ GCC_FOR_TARGET = $(STAGE_CC_WRAPPER) ./xgcc -B./ -B$(build_tooldir)/bin/ -isyste
 # It also specifies -isystem ./include to find, e.g., stddef.h.
 GCC_CFLAGS=$(CFLAGS_FOR_TARGET) $(INTERNAL_CFLAGS) $(T_CFLAGS) $(LOOSE_WARN) $(C_LOOSE_WARN) -Wold-style-definition $($@-warn) -isystem ./include $(TCFLAGS)
 
+EGLIBC_CONFIGS = @EGLIBC_CONFIGS@
+
 # ---------------------------------------------------
 # Programs which produce files for the target machine
 # ---------------------------------------------------
@@ -619,6 +621,23 @@ objext = .o
 exeext = @host_exeext@
 build_exeext = @build_exeext@
 
+licensedir = @licensedir@
+ifneq ($(licensedir),) 
+# Header files for licensing.
+CSL_LICENSEINC = -I $(licensedir)/include
+# Libraries for licensing.
+CSL_LICENSELIB = -L$(licensedir)/lib -lcsllicense
+# The licensing program.  If the program does not exist, assume that
+# it is not needed.
+CSL_LICENSE_PROG = cs-license$(exeext)
+$(CSL_LICENSE_PROG):
+	if [ -f "$(licensedir)/libexec/cs-license$(exeext)" ] ; then \
+	  $(LN_S) $(licensedir)/libexec/cs-license$(exeext) . ;  \
+        else \
+	  touch $@; \
+	fi
+endif
+
 # Directory in which to put man pages.
 mandir = @mandir@
 man1dir = $(mandir)/man1
@@ -682,10 +701,12 @@ CRTSTUFF_CFLAGS = -O2 $(GCC_CFLAGS) $(INCLUDES) $(MULTILIB_CFLAGS) -g0 \
 
 # Additional sources to handle exceptions; overridden by targets as needed.
 LIB2ADDEH = $(srcdir)/unwind-dw2.c $(srcdir)/unwind-dw2-fde.c \
+   $(srcdir)/unwind-compact.c \
    $(srcdir)/unwind-sjlj.c $(srcdir)/gthr-gnat.c $(srcdir)/unwind-c.c
 LIB2ADDEHSTATIC = $(LIB2ADDEH)
 LIB2ADDEHSHARED = $(LIB2ADDEH)
-LIB2ADDEHDEP = $(UNWIND_H) unwind-pe.h unwind.inc unwind-dw2-fde.h unwind-dw2.h
+LIB2ADDEHDEP = $(UNWIND_H) unwind-pe.h unwind.inc unwind-dw2-fde.h \
+  unwind-compact.h unwind-dw2.h
 
 # Don't build libunwind by default.
 LIBUNWIND =
@@ -766,7 +787,7 @@ COMPILERS = cc1$(exeext) @all_compilers@
 
 # List of things which should already be built whenever we try to use xgcc
 # to compile anything (without linking).
-GCC_PASSES=xgcc$(exeext) cc1$(exeext) specs $(EXTRA_PASSES)
+GCC_PASSES=xgcc$(exeext) cc1$(exeext) specs $(EXTRA_PASSES) $(CSL_LICENSE_PROG)
 
 # Directory to link to, when using the target `maketest'.
 DIR = ../gcc
@@ -1051,7 +1072,7 @@ BUILD_LIBDEPS= $(BUILD_LIBIBERTY)
 # How to link with both our special library facilities
 # and the system's installed libraries.
 LIBS = @LIBS@ $(CPPLIB) $(LIBINTL) $(LIBICONV) $(LIBIBERTY) $(LIBDECNUMBER) \
-	$(HOST_LIBS)
+	$(CSL_LICENSELIB) $(HOST_LIBS)
 BACKENDLIBS = $(CLOOGLIBS) $(PPLLIBS) $(GMPLIBS) $(PLUGINLIBS) $(HOST_LIBS) \
 	$(ZLIB)
 # Any system libraries needed just for GNAT.
@@ -1084,7 +1105,8 @@ BUILD_ERRORS = build/errors.o
 INCLUDES = -I. -I$(@D) -I$(srcdir) -I$(srcdir)/$(@D) \
 	   -I$(srcdir)/../include @INCINTL@ \
 	   $(CPPINC) $(GMPINC) $(DECNUMINC) \
-	   $(PPLINC) $(CLOOGINC)
+	   $(PPLINC) $(CLOOGINC) \
+	   $(CSL_LICENSEINC)
 
 .c.o:
 	$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $< $(OUTPUT_OPTION)
@@ -1241,6 +1263,7 @@ OBJS-common = \
 	dse.o \
 	dwarf2asm.o \
 	dwarf2out.o \
+	ee.o \
 	ebitmap.o \
 	emit-rtl.o \
 	et-forest.o \
@@ -1402,6 +1425,7 @@ OBJS-common = \
 	tree-profile.o \
 	tree-scalar-evolution.o \
 	tree-sra.o \
+	tree-if-switch-conversion.o \
 	tree-switch-conversion.o \
 	tree-ssa-address.o \
 	tree-ssa-alias.o \
@@ -1422,6 +1446,7 @@ OBJS-common = \
 	tree-ssa-loop-manip.o \
 	tree-ssa-loop-niter.o \
 	tree-ssa-loop-prefetch.o \
+	tree-ssa-loop-promote.o \
 	tree-ssa-loop-unswitch.o \
 	tree-ssa-loop.o \
 	tree-ssa-math-opts.o \
@@ -1431,6 +1456,7 @@ OBJS-common = \
 	tree-ssa-pre.o \
 	tree-ssa-propagate.o \
 	tree-ssa-reassoc.o \
+	tree-ssa-remove-local-statics.o \
 	tree-ssa-sccvn.o \
 	tree-ssa-sink.o \
 	tree-ssa-structalias.o \
@@ -1875,11 +1901,16 @@ cc1$(exeext): $(C_OBJS) cc1-checksum.o $(BACKEND) $(LIBDEPS)
 LIB2ADD = $(LIB2FUNCS_EXTRA)
 LIB2ADD_ST = $(LIB2FUNCS_STATIC_EXTRA)
 
-# All source files for libgcc are either in the source directory (in
-# which case they will start with $(srcdir)), or generated into the build
-# directory (in which case they will be relative paths).
-srcdirify = $(patsubst $(srcdir)%,$$(gcc_srcdir)%,$(filter $(srcdir)%,$(1))) \
-            $(patsubst %,$$(gcc_objdir)/%,$(filter-out $(srcdir)%,$(1)))
+# All source files for libgcc are either generated in the libgcc build
+# directory which will be substituted for $$(libgcc_objdir), in the
+# source directory (in which case they will start with $(srcdir)), or
+# generated into the build directory (in which case they will be
+# relative paths).
+srcdirify = $(patsubst $$(libgcc_objdir)/%,%, \
+		$(filter $$(libgcc_objdir)%,$(1))) \
+	    $(patsubst $(srcdir)%,$$(gcc_srcdir)%,$(filter $(srcdir)%,$(1))) \
+	    $(patsubst %,$$(gcc_objdir)/%, \
+		$(filter-out $(srcdir)% $$(libgcc_objdir)%,$(1)))
 
 # The distinction between these two variables is no longer relevant,
 # so we combine them.  Sort removes duplicates.
@@ -2195,7 +2226,7 @@ attribs.o : attribs.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(TREE_H) \
 
 incpath.o: incpath.c incpath.h $(CONFIG_H) $(SYSTEM_H) $(CPPLIB_H) \
 		intl.h prefix.h coretypes.h $(TM_H) cppdefault.h $(TARGET_H) \
-		$(MACHMODE_H)
+		$(MACHMODE_H) $(FLAGS_H) toplev.h
 
 prefix.o: prefix.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) prefix.h \
 	Makefile $(BASEVER)
@@ -2593,6 +2624,12 @@ tree-ssa-loop-prefetch.o: tree-ssa-loop-prefetch.c $(TREE_FLOW_H) $(CONFIG_H) \
    $(CFGLOOP_H) $(PARAMS_H) langhooks.h $(BASIC_BLOCK_H) \
    $(DIAGNOSTIC_CORE_H) langhooks.h $(TREE_INLINE_H) $(TREE_DATA_REF_H) \
    $(OPTABS_H) tree-pretty-print.h
+tree-ssa-loop-promote.o: tree-ssa-loop-promote.c \
+   coretypes.h $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(TOPLEV_H) \
+   $(RTL_H) $(TM_P_H) hard-reg-set.h $(OBSTACK_H) $(BASIC_BLOCK_H) \
+   pointer-set.h intl.h $(TREE_H) $(GIMPLE_H) $(HASHTAB_H) $(DIAGNOSTIC_H) \
+   $(TREE_FLOW_H) $(TREE_DUMP_H) $(CFGLOOP_H) $(FLAGS_H) $(TIMEVAR_H) \
+   tree-pass.h $(TM_H) tree-pretty-print.h
 tree-predcom.o: tree-predcom.c $(CONFIG_H) $(SYSTEM_H) $(TREE_H) $(TM_P_H) \
    $(CFGLOOP_H) $(TREE_FLOW_H) $(GGC_H) $(TREE_DATA_REF_H) \
    $(PARAMS_H) $(DIAGNOSTIC_H) $(TREE_PASS_H) $(TM_H) coretypes.h \
@@ -3100,6 +3137,11 @@ implicit-zee.o : implicit-zee.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RT
    $(DF_H) $(TIMEVAR_H) tree-pass.h $(RECOG_H) $(EXPR_H) \
    $(REGS_H) $(TREE_H) $(TM_P_H) insn-config.h $(INSN_ATTR_H) $(DIAGNOSTIC_CORE_H) \
    $(TARGET_H) $(OPTABS_H) insn-codes.h rtlhooks-def.h $(PARAMS_H) $(CGRAPH_H)
+ee.o : ee.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \
+   hard-reg-set.h $(FLAGS_H) $(BASIC_BLOCK_H) $(FUNCTION_H) output.h \
+   $(DF_H) $(TIMEVAR_H) tree-pass.h $(RECOG_H) $(EXPR_H) \
+   $(REGS_H) $(TREE_H) $(TM_P_H) insn-config.h $(INSN_ATTR_H) $(TOPLEV_H) $(DIAGNOSTIC_CORE_H) \
+   $(TARGET_H) $(OPTABS_H) insn-codes.h rtlhooks-def.h $(PARAMS_H) $(CGRAPH_H)
 gcse.o : gcse.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \
    $(REGS_H) hard-reg-set.h $(FLAGS_H) insn-config.h $(GGC_H) \
    $(RECOG_H) $(EXPR_H) $(BASIC_BLOCK_H) $(FUNCTION_H) output.h toplev.h $(DIAGNOSTIC_CORE_H) \
@@ -3120,7 +3162,8 @@ lcm.o : lcm.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) $(REGS_H) \
 mode-switching.o : mode-switching.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \
    $(TM_H) $(RTL_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) insn-config.h \
    $(INSN_ATTR_H) $(RECOG_H) $(BASIC_BLOCK_H) $(TM_P_H) $(FUNCTION_H) \
-   output.h $(TREE_PASS_H) $(TIMEVAR_H) $(DF_H) $(TARGET_H) $(EMIT_RTL_H)
+   output.h $(TREE_PASS_H) $(TIMEVAR_H) $(DF_H) $(TARGET_H) $(EMIT_RTL_H) \
+   integrate.h
 tree-ssa-dce.o : tree-ssa-dce.c $(CONFIG_H) $(SYSTEM_H) $(TREE_H) \
     $(TREE_FLOW_H) $(DIAGNOSTIC_H) $(TIMEVAR_H) $(TM_H) \
     coretypes.h $(TREE_DUMP_H) $(TREE_PASS_H) $(FLAGS_H) $(BASIC_BLOCK_H) \
@@ -3141,6 +3184,11 @@ tree-sra.o : tree-sra.c $(CONFIG_H) $(SYSTEM_H) coretypes.h alloc-pool.h \
    $(IPA_PROP_H) $(DIAGNOSTIC_H) statistics.h $(TREE_DUMP_H) $(TIMEVAR_H) \
    $(PARAMS_H) $(TARGET_H) $(FLAGS_H) $(EXPR_H) tree-pretty-print.h \
    $(DBGCNT_H) $(TREE_INLINE_H) gimple-pretty-print.h
+tree-if-switch-conversion.o : tree-if-switch-conversion.c $(CONFIG_H) \
+    $(SYSTEM_H) $(TREE_H) $(TM_P_H) $(TREE_FLOW_H) $(DIAGNOSTIC_H) \
+    $(TREE_INLINE_H) $(TIMEVAR_H) $(TM_H) coretypes.h $(TREE_DUMP_H) \
+    $(GIMPLE_H) $(TREE_PASS_H) $(FLAGS_H) $(EXPR_H) $(BASIC_BLOCK_H) output.h \
+    $(GGC_H) $(OBSTACK_H) $(PARAMS_H) $(CPPLIB_H) $(PARAMS_H)
 tree-switch-conversion.o : tree-switch-conversion.c $(CONFIG_H) $(SYSTEM_H) \
     $(TREE_H) $(TM_P_H) $(TREE_FLOW_H) $(DIAGNOSTIC_H) $(TREE_INLINE_H) \
     $(TIMEVAR_H) $(TM_H) coretypes.h $(TREE_DUMP_H) $(GIMPLE_H) \
@@ -3297,7 +3345,7 @@ postreload.o : postreload.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \
    $(RTL_H) $(FLAGS_H) $(EXPR_H) $(OPTABS_H) reload.h $(REGS_H) \
    hard-reg-set.h insn-config.h $(BASIC_BLOCK_H) $(RECOG_H) output.h \
    $(FUNCTION_H) $(DIAGNOSTIC_CORE_H) cselib.h $(TM_P_H) $(EXCEPT_H) $(TREE_H) $(MACHMODE_H) \
-   $(OBSTACK_H) $(TARGET_H) $(TIMEVAR_H) $(TREE_PASS_H) $(DF_H) $(DBGCNT_H)
+   $(OBSTACK_H) $(TARGET_H) $(TIMEVAR_H) $(TREE_PASS_H) addresses.h $(DF_H) $(DBGCNT_H)
 postreload-gcse.o : postreload-gcse.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \
    $(TM_H) $(RTL_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) insn-config.h \
    $(RECOG_H) $(EXPR_H) $(BASIC_BLOCK_H) $(FUNCTION_H) output.h $(DIAGNOSTIC_CORE_H) \
@@ -3354,7 +3402,7 @@ ira-lives.o: ira-lives.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \
    $(DF_H) sparseset.h $(IRA_INT_H)
 ira.o: ira.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \
    $(TM_H) $(REGS_H) $(RTL_H) $(TM_P_H) $(TARGET_H) $(FLAGS_H) $(OBSTACK_H) \
-   $(BITMAP_H) hard-reg-set.h $(BASIC_BLOCK_H) \
+   $(BITMAP_H) hard-reg-set.h $(BASIC_BLOCK_H) $(DBGCNT_H) \
    $(EXPR_H) $(RECOG_H) $(PARAMS_H) $(TIMEVAR_H) $(TREE_PASS_H) output.h \
    $(EXCEPT_H) reload.h toplev.h $(DIAGNOSTIC_CORE_H) $(INTEGRATE_H) $(DF_H) $(GGC_H) $(IRA_INT_H)
 regmove.o : regmove.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \
@@ -3383,7 +3431,7 @@ modulo-sched.o : modulo-sched.c $(DDG_H) $(CONFIG_H) $(CONFIG_H) $(SYSTEM_H) \
 haifa-sched.o : haifa-sched.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \
    $(SCHED_INT_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) insn-config.h $(FUNCTION_H) \
    $(INSN_ATTR_H) $(DIAGNOSTIC_CORE_H) $(RECOG_H) $(EXCEPT_H) $(TM_P_H) $(TARGET_H) output.h \
-   $(PARAMS_H) $(DBGCNT_H) $(CFGLOOP_H) ira.h $(EMIT_RTL_H)
+   $(PARAMS_H) $(DBGCNT_H) $(CFGLOOP_H) ira.h $(EMIT_RTL_H) $(HASHTAB_H)
 sched-deps.o : sched-deps.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \
    $(RTL_H) $(SCHED_INT_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) insn-config.h \
    $(FUNCTION_H) $(INSN_ATTR_H) $(DIAGNOSTIC_CORE_H) $(RECOG_H) $(EXCEPT_H) cselib.h \
@@ -3461,7 +3509,7 @@ cfglayout.o : cfglayout.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \
 timevar.o : timevar.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \
    $(TIMEVAR_H) $(FLAGS_H) intl.h toplev.h $(DIAGNOSTIC_CORE_H) $(RTL_H) timevar.def
 regcprop.o : regcprop.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \
-   $(RTL_H) insn-config.h $(BASIC_BLOCK_H) $(REGS_H) hard-reg-set.h \
+   $(RTL_H) insn-config.h $(BASIC_BLOCK_H) $(REGS_H) hard-reg-set.h dce.h \
    output.h $(RECOG_H) $(FUNCTION_H) $(OBSTACK_H) $(FLAGS_H) $(TM_P_H) \
    addresses.h reload.h $(DIAGNOSTIC_CORE_H) $(TIMEVAR_H) $(TREE_PASS_H) $(DF_H)
 regrename.o : regrename.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \
@@ -3766,6 +3814,7 @@ GTFILES = $(CPP_ID_DATA_H) $(srcdir)/input.h $(srcdir)/coretypes.h \
   $(srcdir)/ipa-prop.h \
   $(srcdir)/lto-streamer.h \
   $(srcdir)/target-globals.h \
+  $(srcdir)/final.c \
   @all_gtfiles@
 
 # Compute the list of GT header files from the corresponding C sources,
diff --git a/gcc/acinclude.m4 b/gcc/acinclude.m4
index 3eec559..532c5dc 100644
--- a/gcc/acinclude.m4
+++ b/gcc/acinclude.m4
@@ -521,3 +521,53 @@ dnl Make sure that build_exeext is looked for
 AC_DEFUN([gcc_AC_BUILD_EXEEXT], [
 ac_executable_extensions="$build_exeext"])
 
+
+# --with-license=PATH
+AC_DEFUN([CSL_AC_LICENSE],[
+  AC_ARG_WITH(license,
+    AC_HELP_STRING([--with-license],
+                   [the path to the installed license component]),
+    [case "$withval" in
+     (yes) AC_MSG_ERROR([license not specified]) ;;
+     (no)  with_license= ;;
+     (*) ;;
+  esac],
+  [with_license=])
+  AC_SUBST(licensedir, $with_license)
+])
+
+# --with-csl-license-feature=FOO
+AC_DEFUN([CSL_AC_LICENSE_FEATURE],[
+  AC_ARG_WITH(csl-license-feature,
+    AC_HELP_STRING([--with-csl-license-feature=FEATURE],
+                   [Use FEATURE to communicate with the license manager]),
+    [case "$withval" in
+      (yes) AC_MSG_ERROR([license feature not specified]) ;;
+      (no)  CSL_LICENSE_FEATURE="" ;; 
+      (*)   CSL_LICENSE_FEATURE="$withval" ;;
+     esac],
+     CSL_LICENSE_FEATURE=""
+  )
+  if test x"$CSL_LICENSE_FEATURE" != x; then
+    AC_DEFINE_UNQUOTED(CSL_LICENSE_FEATURE, "$CSL_LICENSE_FEATURE",
+                       [Required license feature])
+  fi
+])
+
+# --with-csl-license-version=VERSION
+AC_DEFUN([CSL_AC_LICENSE_VERSION],[
+  AC_ARG_WITH(csl-license-version,
+    AC_HELP_STRING([--with-csl-license-version=VERSION],
+                   [Use VERSION to communicate with the license manager]),
+    [case "$withval" in
+      (yes) AC_MSG_ERROR([license version not specified]) ;;
+      (no)  CSL_LICENSE_VERSION="" ;; 
+      (*)   CSL_LICENSE_VERSION="$withval" ;;
+     esac],
+     CSL_LICENSE_VERSION=""
+  )
+  if test x"$CSL_LICENSE_VERSION" != x; then
+    AC_DEFINE_UNQUOTED(CSL_LICENSE_VERSION, "$CSL_LICENSE_VERSION",
+                       [Required license version])
+  fi
+])
diff --git a/gcc/addresses.h b/gcc/addresses.h
index b229f17..bb30108 100644
--- a/gcc/addresses.h
+++ b/gcc/addresses.h
@@ -79,3 +79,42 @@ regno_ok_for_base_p (unsigned regno, enum machine_mode mode,
 
   return ok_for_base_p_1 (regno, mode, outer_code, index_code);
 }
+
+/* Wrapper function to unify target macros MODE_INDEX_REG_CLASS and
+   INDEX_REG_CLASS.  Arguments as for the MODE_INDEX_REG_CLASS macro.  */
+
+static inline enum reg_class
+index_reg_class (enum machine_mode mode ATTRIBUTE_UNUSED)
+{
+#ifdef MODE_INDEX_REG_CLASS
+  return MODE_INDEX_REG_CLASS (mode);
+#else
+  return INDEX_REG_CLASS;
+#endif
+}
+
+/* Wrapper function to unify target macros REGNO_MODE_OK_FOR_INDEX_P
+   and REGNO_OK_FOR_INDEX_P.  Arguments as for the
+   REGNO_MODE_OK_FOR_INDEX_P macro.  */
+
+static inline bool
+ok_for_index_p_1 (unsigned regno, enum machine_mode mode ATTRIBUTE_UNUSED)
+{
+#ifdef REGNO_MODE_OK_FOR_INDEX_P
+  return REGNO_MODE_OK_FOR_INDEX_P (regno, mode);
+#else
+  return REGNO_OK_FOR_INDEX_P (regno);
+#endif
+}
+
+/* Wrapper around ok_for_index_p_1, for use after register allocation is
+   complete.  Arguments as for the called function.  */
+
+static inline bool
+regno_ok_for_index_p (unsigned regno, enum machine_mode mode)
+{
+  if (regno >= FIRST_PSEUDO_REGISTER && reg_renumber[regno] >= 0)
+    regno = reg_renumber[regno];
+
+  return ok_for_index_p_1 (regno, mode);
+}
diff --git a/gcc/attribs.c b/gcc/attribs.c
index fee1499..33b8bac 100644
--- a/gcc/attribs.c
+++ b/gcc/attribs.c
@@ -303,8 +303,9 @@ decl_attributes (tree *node, tree attributes, int flags)
 
       if (spec == NULL)
 	{
-	  warning (OPT_Wattributes, "%qE attribute directive ignored",
-		   name);
+	  if (!(flags & (int) ATTR_FLAG_BUILT_IN))
+	    warning (OPT_Wattributes, "%qE attribute directive ignored",
+		     name);
 	  continue;
 	}
       else if (list_length (args) < spec->min_length
diff --git a/gcc/basic-block.h b/gcc/basic-block.h
index 3594eea..0585ff2 100644
--- a/gcc/basic-block.h
+++ b/gcc/basic-block.h
@@ -795,6 +795,7 @@ extern void flow_edge_list_print (const char *, const edge *, int, FILE *);
 
 /* In cfgrtl.c  */
 extern basic_block force_nonfallthru (edge);
+extern basic_block force_nonfallthru_and_redirect (edge, basic_block, rtx);
 extern rtx block_label (basic_block);
 extern bool purge_all_dead_edges (void);
 extern bool purge_dead_edges (basic_block);
diff --git a/gcc/builtin-attrs.def b/gcc/builtin-attrs.def
index d0c3d96..0a17c6e 100644
--- a/gcc/builtin-attrs.def
+++ b/gcc/builtin-attrs.def
@@ -59,6 +59,14 @@ DEF_ATTR_FOR_INT (5)
 DEF_ATTR_FOR_INT (6)
 #undef DEF_ATTR_FOR_INT
 
+/* Construct a tree for a given string and a list containing it.  */
+#define DEF_ATTR_FOR_STRING(ENUM, VALUE)					\
+  DEF_ATTR_STRING (ATTR_##ENUM, VALUE)			\
+  DEF_ATTR_TREE_LIST (ATTR_LIST_##ENUM, ATTR_NULL,	\
+		      ATTR_##ENUM, ATTR_NULL)
+DEF_ATTR_FOR_STRING (STR1, "1")
+#undef DEF_ATTR_FOR_STRING
+
 /* Construct a tree for a list of two integers.  */
 #define DEF_LIST_INT_INT(VALUE1, VALUE2)				 \
   DEF_ATTR_TREE_LIST (ATTR_LIST_##VALUE1##_##VALUE2, ATTR_NULL,		 \
@@ -84,6 +92,7 @@ DEF_ATTR_IDENT (ATTR_NONNULL, "nonnull")
 DEF_ATTR_IDENT (ATTR_NORETURN, "noreturn")
 DEF_ATTR_IDENT (ATTR_NOTHROW, "nothrow")
 DEF_ATTR_IDENT (ATTR_LEAF, "leaf")
+DEF_ATTR_IDENT (ATTR_FNSPEC, "fn spec")
 DEF_ATTR_IDENT (ATTR_PRINTF, "printf")
 DEF_ATTR_IDENT (ATTR_ASM_FPRINTF, "asm_fprintf")
 DEF_ATTR_IDENT (ATTR_GCC_DIAG, "gcc_diag")
@@ -167,6 +176,10 @@ DEF_ATTR_TREE_LIST (ATTR_NOTHROW_NONNULL_5, ATTR_NONNULL, ATTR_LIST_5, \
 /* Nothrow const functions whose pointer parameter(s) are all nonnull.  */
 DEF_ATTR_TREE_LIST (ATTR_CONST_NOTHROW_NONNULL, ATTR_CONST, ATTR_NULL, \
 			ATTR_NOTHROW_NONNULL)
+/* Nothrow leaf functions whose pointer parameter(s) are all nonnull,
+   and which return their first argument.  */
+DEF_ATTR_TREE_LIST (ATTR_RET1_NOTHROW_NONNULL_LEAF, ATTR_FNSPEC, ATTR_LIST_STR1, \
+			ATTR_NOTHROW_NONNULL_LEAF)
 /* Nothrow const leaf functions whose pointer parameter(s) are all nonnull.  */
 DEF_ATTR_TREE_LIST (ATTR_CONST_NOTHROW_NONNULL_LEAF, ATTR_CONST, ATTR_NULL, \
 			ATTR_NOTHROW_NONNULL_LEAF)
diff --git a/gcc/builtins.c b/gcc/builtins.c
index 4d8c1b7..705b9f8 100644
--- a/gcc/builtins.c
+++ b/gcc/builtins.c
@@ -13894,6 +13894,7 @@ is_inexpensive_builtin (tree decl)
       case BUILT_IN_ISUNORDERED:
       case BUILT_IN_VA_ARG_PACK:
       case BUILT_IN_VA_ARG_PACK_LEN:
+      case BUILT_IN_VA_START:
       case BUILT_IN_VA_COPY:
       case BUILT_IN_TRAP:
       case BUILT_IN_SAVEREGS:
diff --git a/gcc/builtins.def b/gcc/builtins.def
index 3e102e3..f670371 100644
--- a/gcc/builtins.def
+++ b/gcc/builtins.def
@@ -510,8 +510,8 @@ DEF_EXT_LIB_BUILTIN    (BUILT_IN_BZERO, "bzero", BT_FN_VOID_PTR_SIZE, ATTR_NOTHR
 DEF_EXT_LIB_BUILTIN    (BUILT_IN_INDEX, "index", BT_FN_STRING_CONST_STRING_INT, ATTR_PURE_NOTHROW_NONNULL_LEAF)
 DEF_LIB_BUILTIN        (BUILT_IN_MEMCHR, "memchr", BT_FN_PTR_CONST_PTR_INT_SIZE, ATTR_PURE_NOTHROW_NONNULL_LEAF)
 DEF_LIB_BUILTIN        (BUILT_IN_MEMCMP, "memcmp", BT_FN_INT_CONST_PTR_CONST_PTR_SIZE, ATTR_PURE_NOTHROW_NONNULL_LEAF)
-DEF_LIB_BUILTIN        (BUILT_IN_MEMCPY, "memcpy", BT_FN_PTR_PTR_CONST_PTR_SIZE, ATTR_NOTHROW_NONNULL_LEAF)
-DEF_LIB_BUILTIN        (BUILT_IN_MEMMOVE, "memmove", BT_FN_PTR_PTR_CONST_PTR_SIZE, ATTR_NOTHROW_NONNULL_LEAF)
+DEF_LIB_BUILTIN        (BUILT_IN_MEMCPY, "memcpy", BT_FN_PTR_PTR_CONST_PTR_SIZE, ATTR_RET1_NOTHROW_NONNULL_LEAF)
+DEF_LIB_BUILTIN        (BUILT_IN_MEMMOVE, "memmove", BT_FN_PTR_PTR_CONST_PTR_SIZE, ATTR_RET1_NOTHROW_NONNULL_LEAF)
 DEF_EXT_LIB_BUILTIN    (BUILT_IN_MEMPCPY, "mempcpy", BT_FN_PTR_PTR_CONST_PTR_SIZE, ATTR_NOTHROW_NONNULL_LEAF)
 DEF_LIB_BUILTIN        (BUILT_IN_MEMSET, "memset", BT_FN_PTR_PTR_INT_SIZE, ATTR_NOTHROW_NONNULL_LEAF)
 DEF_EXT_LIB_BUILTIN    (BUILT_IN_RINDEX, "rindex", BT_FN_STRING_CONST_STRING_INT, ATTR_PURE_NOTHROW_NONNULL_LEAF)
@@ -724,8 +724,8 @@ DEF_BUILTIN_STUB (BUILT_IN_STACK_RESTORE, "__builtin_stack_restore")
 
 /* Object size checking builtins.  */
 DEF_GCC_BUILTIN	       (BUILT_IN_OBJECT_SIZE, "object_size", BT_FN_SIZE_CONST_PTR_INT, ATTR_PURE_NOTHROW_LEAF_LIST)
-DEF_EXT_LIB_BUILTIN    (BUILT_IN_MEMCPY_CHK, "__memcpy_chk", BT_FN_PTR_PTR_CONST_PTR_SIZE_SIZE, ATTR_NOTHROW_NONNULL_LEAF)
-DEF_EXT_LIB_BUILTIN    (BUILT_IN_MEMMOVE_CHK, "__memmove_chk", BT_FN_PTR_PTR_CONST_PTR_SIZE_SIZE, ATTR_NOTHROW_NONNULL_LEAF)
+DEF_EXT_LIB_BUILTIN    (BUILT_IN_MEMCPY_CHK, "__memcpy_chk", BT_FN_PTR_PTR_CONST_PTR_SIZE_SIZE, ATTR_RET1_NOTHROW_NONNULL_LEAF)
+DEF_EXT_LIB_BUILTIN    (BUILT_IN_MEMMOVE_CHK, "__memmove_chk", BT_FN_PTR_PTR_CONST_PTR_SIZE_SIZE, ATTR_RET1_NOTHROW_NONNULL_LEAF)
 DEF_EXT_LIB_BUILTIN    (BUILT_IN_MEMPCPY_CHK, "__mempcpy_chk", BT_FN_PTR_PTR_CONST_PTR_SIZE_SIZE, ATTR_NOTHROW_NONNULL_LEAF)
 DEF_EXT_LIB_BUILTIN    (BUILT_IN_MEMSET_CHK, "__memset_chk", BT_FN_PTR_PTR_INT_SIZE_SIZE, ATTR_NOTHROW_NONNULL_LEAF)
 DEF_EXT_LIB_BUILTIN    (BUILT_IN_STPCPY_CHK, "__stpcpy_chk", BT_FN_STRING_STRING_CONST_STRING_SIZE, ATTR_NOTHROW_NONNULL_LEAF)
@@ -748,6 +748,13 @@ DEF_BUILTIN (BUILT_IN_PROFILE_FUNC_ENTER, "__cyg_profile_func_enter", BUILT_IN_N
 DEF_BUILTIN (BUILT_IN_PROFILE_FUNC_EXIT, "__cyg_profile_func_exit", BUILT_IN_NORMAL, BT_FN_VOID_PTR_PTR, BT_LAST,
 	     false, false, false, ATTR_NULL, true, true)
 
+DEF_BUILTIN (BUILT_IN_PROFILE_CALL_ENTER, "__cyg_profile_call_enter", BUILT_IN_NORMAL, BT_FN_VOID_PTR, BT_LAST,
+	     false, false, false, ATTR_NULL, true, true)
+DEF_BUILTIN (BUILT_IN_PROFILE_CALL_INSIDE, "__cyg_profile_call_inside", BUILT_IN_NORMAL, BT_FN_VOID_PTR, BT_LAST,
+	     false, false, false, ATTR_NULL, true, true)
+DEF_BUILTIN (BUILT_IN_PROFILE_CALL_EXIT, "__cyg_profile_call_exit", BUILT_IN_NORMAL, BT_FN_VOID_PTR, BT_LAST,
+	     false, false, false, ATTR_NULL, true, true)
+
 /* TLS emulation.  */
 DEF_BUILTIN (BUILT_IN_EMUTLS_GET_ADDRESS, targetm.emutls.get_address,
 	     BUILT_IN_NORMAL,
diff --git a/gcc/c-family/c-common.c b/gcc/c-family/c-common.c
index eaf1293..a20c477 100644
--- a/gcc/c-family/c-common.c
+++ b/gcc/c-family/c-common.c
@@ -4348,11 +4348,13 @@ enum built_in_attribute
 {
 #define DEF_ATTR_NULL_TREE(ENUM) ENUM,
 #define DEF_ATTR_INT(ENUM, VALUE) ENUM,
+#define DEF_ATTR_STRING(ENUM, VALUE) ENUM,
 #define DEF_ATTR_IDENT(ENUM, STRING) ENUM,
 #define DEF_ATTR_TREE_LIST(ENUM, PURPOSE, VALUE, CHAIN) ENUM,
 #include "builtin-attrs.def"
 #undef DEF_ATTR_NULL_TREE
 #undef DEF_ATTR_INT
+#undef DEF_ATTR_STRING
 #undef DEF_ATTR_IDENT
 #undef DEF_ATTR_TREE_LIST
   ATTR_LAST
@@ -5677,6 +5679,8 @@ c_init_attributes (void)
   built_in_attributes[(int) ENUM] = NULL_TREE;
 #define DEF_ATTR_INT(ENUM, VALUE)				\
   built_in_attributes[(int) ENUM] = build_int_cst (NULL_TREE, VALUE);
+#define DEF_ATTR_STRING(ENUM, VALUE)				\
+  built_in_attributes[(int) ENUM] = build_string (strlen (VALUE), VALUE);
 #define DEF_ATTR_IDENT(ENUM, STRING)				\
   built_in_attributes[(int) ENUM] = get_identifier (STRING);
 #define DEF_ATTR_TREE_LIST(ENUM, PURPOSE, VALUE, CHAIN)	\
diff --git a/gcc/c-family/c-pch.c b/gcc/c-family/c-pch.c
index b429d9d..0aaba2a 100644
--- a/gcc/c-family/c-pch.c
+++ b/gcc/c-family/c-pch.c
@@ -180,6 +180,8 @@ c_common_write_pch (void)
 
   timevar_push (TV_PCH_SAVE);
 
+  targetm.prepare_pch_save ();
+
   (*debug_hooks->handle_pch) (1);
 
   cpp_write_pch_deps (parse_in, pch_outfile);
diff --git a/gcc/c-family/c.opt b/gcc/c-family/c.opt
index 4c4727f..aa3585f 100644
--- a/gcc/c-family/c.opt
+++ b/gcc/c-family/c.opt
@@ -929,6 +929,10 @@ fpretty-templates
 C++ ObjC++ Var(flag_pretty_templates) Init(1)
 -fno-pretty-templates Do not pretty-print template specializations as the template signature followed by the arguments
 
+fremove-local-statics
+C C++ Var(flag_remove_local_statics) Optimization
+Convert function-local static variables to automatic variables when it is safe to do so
+
 freplace-objc-classes
 ObjC ObjC++ Var(flag_replace_objc_classes)
 Used in Fix-and-Continue mode to indicate that object files may be swapped in at runtime
diff --git a/gcc/caller-save.c b/gcc/caller-save.c
index 160d2e9..92cbd8c 100644
--- a/gcc/caller-save.c
+++ b/gcc/caller-save.c
@@ -441,7 +441,7 @@ setup_save_areas (void)
       freq = REG_FREQ_FROM_BB (BLOCK_FOR_INSN (insn));
       REG_SET_TO_HARD_REG_SET (hard_regs_to_save,
 			       &chain->live_throughout);
-      COPY_HARD_REG_SET (used_regs, call_used_reg_set);
+      get_call_reg_set_usage (insn, &used_regs, call_used_reg_set);
 
       /* Record all registers set in this call insn.  These don't
 	 need to be saved.  N.B. the call insn might set a subreg
@@ -516,7 +516,7 @@ setup_save_areas (void)
 	    continue;
 	  REG_SET_TO_HARD_REG_SET (hard_regs_to_save,
 				   &chain->live_throughout);
-	  COPY_HARD_REG_SET (used_regs, call_used_reg_set);
+	  get_call_reg_set_usage (insn, &used_regs, call_used_reg_set);
 
 	  /* Record all registers set in this call insn.  These don't
 	     need to be saved.  N.B. the call insn might set a subreg
@@ -795,6 +795,7 @@ save_call_clobbered_regs (void)
 	    {
 	      unsigned regno;
 	      HARD_REG_SET hard_regs_to_save;
+	      HARD_REG_SET call_def_reg_set;
 	      reg_set_iterator rsi;
 
 	      /* Use the register life information in CHAIN to compute which
@@ -840,7 +841,9 @@ save_call_clobbered_regs (void)
 	      AND_COMPL_HARD_REG_SET (hard_regs_to_save, call_fixed_reg_set);
 	      AND_COMPL_HARD_REG_SET (hard_regs_to_save, this_insn_sets);
 	      AND_COMPL_HARD_REG_SET (hard_regs_to_save, hard_regs_saved);
-	      AND_HARD_REG_SET (hard_regs_to_save, call_used_reg_set);
+	      get_call_reg_set_usage (insn, &call_def_reg_set,
+				      call_used_reg_set);
+	      AND_HARD_REG_SET (hard_regs_to_save, call_def_reg_set);
 
 	      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
 		if (TEST_HARD_REG_BIT (hard_regs_to_save, regno))
diff --git a/gcc/calls.c b/gcc/calls.c
index 4ad6c3f..8bcf6bb 100644
--- a/gcc/calls.c
+++ b/gcc/calls.c
@@ -543,6 +543,41 @@ special_function_p (const_tree fndecl, int flags)
   return flags;
 }
 
+/* Similar to special_function_p; return a set of ERF_ flags for the
+   function FNDECL.  */
+static int
+decl_return_flags (tree fndecl)
+{
+  tree attr;
+  tree type = TREE_TYPE (fndecl);
+  if (!type)
+    return 0;
+
+  attr = lookup_attribute ("fn spec", TYPE_ATTRIBUTES (type));
+  if (!attr)
+    return 0;
+
+  attr = TREE_VALUE (TREE_VALUE (attr));
+  if (!attr || TREE_STRING_LENGTH (attr) < 1)
+    return 0;
+
+  switch (TREE_STRING_POINTER (attr)[0])
+    {
+    case '1':
+    case '2':
+    case '3':
+    case '4':
+      return ERF_RETURNS_ARG | (TREE_STRING_POINTER (attr)[0] - '1');
+
+    case 'm':
+      return ERF_NOALIAS;
+
+    case '.':
+    default:
+      return 0;
+    }
+}
+
 /* Return nonzero when FNDECL represents a call to setjmp.  */
 
 int
@@ -715,7 +750,9 @@ precompute_register_parameters (int num_actuals, struct arg_data *args,
 
 	   For small register classes, also do this if this call uses
 	   register parameters.  This is to avoid reload conflicts while
-	   loading the parameters registers.  */
+	   loading the parameters registers.
+	   
+	   Avoid creating the extra move if optimizing for size.  */
 
 	else if ((! (REG_P (args[i].value)
 		     || (GET_CODE (args[i].value) == SUBREG
@@ -723,6 +760,7 @@ precompute_register_parameters (int num_actuals, struct arg_data *args,
 		 && args[i].mode != BLKmode
 		 && rtx_cost (args[i].value, SET, optimize_insn_for_speed_p ())
 		    > COSTS_N_INSNS (1)
+		 && !optimize_size
 		 && ((*reg_parm_seen
 		      && targetm.small_register_classes_for_mode_p (args[i].mode))
 		     || optimize))
@@ -905,8 +943,8 @@ store_unaligned_arguments_into_pseudos (struct arg_data *args, int num_actuals)
 	    emit_move_insn (reg, const0_rtx);
 
 	    bytes -= bitsize / BITS_PER_UNIT;
-	    store_bit_field (reg, bitsize, endian_correction, word_mode,
-			     word);
+	    store_bit_field (reg, bitsize, endian_correction, false,
+			     word_mode, word);
 	  }
       }
 }
@@ -2137,8 +2175,9 @@ expand_call (tree exp, rtx target, int ignore)
      (on machines that lack push insns), or 0 if space not preallocated.  */
   rtx argblock = 0;
 
-  /* Mask of ECF_ flags.  */
+  /* Mask of ECF_ and ERF_ flags.  */
   int flags = 0;
+  int return_flags = 0;
 #ifdef REG_PARM_STACK_SPACE
   /* Define the boundary of the register parm stack space that needs to be
      saved, if any.  */
@@ -2183,6 +2222,7 @@ expand_call (tree exp, rtx target, int ignore)
     {
       fntype = TREE_TYPE (fndecl);
       flags |= flags_from_decl_or_type (fndecl);
+      return_flags |= decl_return_flags (fndecl);
     }
   else
     {
@@ -2998,6 +3038,20 @@ expand_call (tree exp, rtx target, int ignore)
 						   VOIDmode, void_type_node,
 						   true);
 
+      if (pass == 1 && (return_flags & ERF_RETURNS_ARG))
+	{
+	  int arg_nr = return_flags & ERF_RETURN_ARG_MASK;
+	  if (PUSH_ARGS_REVERSED)
+	    arg_nr = num_actuals - arg_nr - 1;
+	  if (args[arg_nr].reg
+	      && valreg
+	      && REG_P (valreg)
+	      && GET_MODE (args[arg_nr].reg) == GET_MODE (valreg))
+	  call_fusage
+	    = gen_rtx_EXPR_LIST (TYPE_MODE (TREE_TYPE (args[arg_nr].tree_value)),
+				 gen_rtx_SET (VOIDmode, valreg, args[arg_nr].reg),
+				 call_fusage);
+	}
       /* All arguments and registers used for the call must be set up by
 	 now!  */
 
@@ -3611,6 +3665,7 @@ emit_library_call_value_1 (int retval, rtx orgfun, rtx value,
     {
       rtx val = va_arg (p, rtx);
       enum machine_mode mode = (enum machine_mode) va_arg (p, int);
+      int unsigned_p = 0;
 
       /* We cannot convert the arg value to the mode the library wants here;
 	 must do it earlier where we know the signedness of the arg.  */
@@ -3658,29 +3713,38 @@ emit_library_call_value_1 (int retval, rtx orgfun, rtx value,
 	  val = force_operand (XEXP (slot, 0), NULL_RTX);
 	}
 
-      argvec[count].value = val;
+      mode = promote_function_mode (NULL_TREE, mode, &unsigned_p, NULL_TREE, 0);
       argvec[count].mode = mode;
-
+      argvec[count].value = convert_modes (mode, GET_MODE (val), val, unsigned_p);
       argvec[count].reg = targetm.calls.function_arg (&args_so_far, mode,
 						      NULL_TREE, true);
 
       argvec[count].partial
 	= targetm.calls.arg_partial_bytes (&args_so_far, mode, NULL_TREE, 1);
 
-      locate_and_pad_parm (mode, NULL_TREE,
+      if (argvec[count].reg == 0
+	  || argvec[count].partial != 0
+	  || reg_parm_stack_space > 0)
+	{
+	  locate_and_pad_parm (mode, NULL_TREE,
 #ifdef STACK_PARMS_IN_REG_PARM_AREA
-			   1,
+			       1,
 #else
-			   argvec[count].reg != 0,
+			       argvec[count].reg != 0,
+#endif
+			       argvec[count].partial,
+			       NULL_TREE, &args_size, &argvec[count].locate);
+	  args_size.constant += argvec[count].locate.size.constant;
+	  gcc_assert (!argvec[count].locate.size.var);
+	}
+#ifdef BLOCK_REG_PADDING
+      else
+	/* The argument is passed entirely in registers.  See at which
+	   end it should be padded.  */
+	argvec[count].locate.where_pad =
+	  BLOCK_REG_PADDING (mode, NULL_TREE,
+			     GET_MODE_SIZE (mode) <= UNITS_PER_WORD);
 #endif
-			   argvec[count].partial,
-			   NULL_TREE, &args_size, &argvec[count].locate);
-
-      gcc_assert (!argvec[count].locate.size.var);
-
-      if (argvec[count].reg == 0 || argvec[count].partial != 0
-	  || reg_parm_stack_space > 0)
-	args_size.constant += argvec[count].locate.size.constant;
 
       targetm.calls.function_arg_advance (&args_so_far, mode, (tree) 0, true);
     }
@@ -3931,13 +3995,44 @@ emit_library_call_value_1 (int retval, rtx orgfun, rtx value,
       rtx val = argvec[argnum].value;
       rtx reg = argvec[argnum].reg;
       int partial = argvec[argnum].partial;
-
+#ifdef BLOCK_REG_PADDING
+      int size = 0;
+#endif
+      
       /* Handle calls that pass values in multiple non-contiguous
 	 locations.  The PA64 has examples of this for library calls.  */
       if (reg != 0 && GET_CODE (reg) == PARALLEL)
 	emit_group_load (reg, val, NULL_TREE, GET_MODE_SIZE (mode));
       else if (reg != 0 && partial == 0)
-	emit_move_insn (reg, val);
+        {
+	  emit_move_insn (reg, val);
+#ifdef BLOCK_REG_PADDING
+	  size = GET_MODE_SIZE (argvec[argnum].mode);
+
+	  /* Copied from load_register_parameters.  */
+
+	  /* Handle case where we have a value that needs shifting
+	     up to the msb.  eg. a QImode value and we're padding
+	     upward on a BYTES_BIG_ENDIAN machine.  */
+	  if (size < UNITS_PER_WORD
+	      && (argvec[argnum].locate.where_pad
+		  == (BYTES_BIG_ENDIAN ? upward : downward)))
+	    {
+	      rtx x;
+	      int shift = (UNITS_PER_WORD - size) * BITS_PER_UNIT;
+
+	      /* Assigning REG here rather than a temp makes CALL_FUSAGE
+		 report the whole reg as used.  Strictly speaking, the
+		 call only uses SIZE bytes at the msb end, but it doesn't
+		 seem worth generating rtl to say that.  */
+	      reg = gen_rtx_REG (word_mode, REGNO (reg));
+	      x = expand_shift (LSHIFT_EXPR, word_mode, reg,
+				build_int_cst (NULL_TREE, shift), reg, 1);
+	      if (x != reg)
+		emit_move_insn (reg, x);
+	    }
+#endif
+	}
 
       NO_DEFER_POP;
     }
@@ -4003,6 +4098,15 @@ emit_library_call_value_1 (int retval, rtx orgfun, rtx value,
 	       valreg,
 	       old_inhibit_defer_pop + 1, call_fusage, flags, & args_so_far);
 
+  /* Right-shift returned value if necessary.  */
+  if (!pcc_struct_value
+      && TYPE_MODE (tfom) != BLKmode
+      && targetm.calls.return_in_msb (tfom))
+    {
+      shift_return_value (TYPE_MODE (tfom), false, valreg);
+      valreg = gen_rtx_REG (TYPE_MODE (tfom), REGNO (valreg));
+    }
+
   /* For calls to `setjmp', etc., inform function.c:setjmp_warnings
      that it should complain if nonvolatile values are live.  For
      functions that cannot return, inform flow that control does not
@@ -4223,8 +4327,17 @@ store_one_arg (struct arg_data *arg, rtx argblock, int flags,
 	      /* We need to make a save area.  */
 	      unsigned int size = arg->locate.size.constant * BITS_PER_UNIT;
 	      enum machine_mode save_mode = mode_for_size (size, MODE_INT, 1);
-	      rtx adr = memory_address (save_mode, XEXP (arg->stack_slot, 0));
-	      rtx stack_area = gen_rtx_MEM (save_mode, adr);
+	      rtx adr;
+	      rtx stack_area;
+
+	      /* We can only use save_mode if the arg is sufficiently
+	         aligned.  */
+	      if (STRICT_ALIGNMENT
+		  && GET_MODE_ALIGNMENT (save_mode) > arg->locate.boundary)
+		save_mode = BLKmode;
+
+	      adr = memory_address (save_mode, XEXP (arg->stack_slot, 0));
+	      stack_area = gen_rtx_MEM (save_mode, adr);
 
 	      if (save_mode == BLKmode)
 		{
diff --git a/gcc/cfgexpand.c b/gcc/cfgexpand.c
index 6fa9d30..d05228b 100644
--- a/gcc/cfgexpand.c
+++ b/gcc/cfgexpand.c
@@ -212,6 +212,9 @@ static unsigned int
 get_decl_align_unit (tree decl)
 {
   unsigned int align = LOCAL_DECL_ALIGNMENT (decl);
+
+  align = alignment_for_aligned_arrays (TREE_TYPE (decl), align);
+  
   return align / BITS_PER_UNIT;
 }
 
@@ -372,8 +375,9 @@ add_alias_set_conflicts (void)
 		 to elements will conflict.  In case of unions we have
 		 to be careful as type based aliasing rules may say
 		 access to the same memory does not conflict.  So play
-		 safe and add a conflict in this case.  */
-	      || contains_union)
+		 safe and add a conflict in this case when
+                 -fstrict-aliasing is used.  */
+              || (contains_union && flag_strict_aliasing))
 	    add_stack_var_conflict (i, j);
 	}
     }
diff --git a/gcc/cfglayout.c b/gcc/cfglayout.c
index f7d4d10..f3f0f3b 100644
--- a/gcc/cfglayout.c
+++ b/gcc/cfglayout.c
@@ -609,8 +609,14 @@ reemit_insn_block_notes (void)
 
 	  this_block = NULL;
 	  for (i = 0; i < XVECLEN (body, 0); i++)
-	    this_block = choose_inner_scope (this_block,
-					 insn_scope (XVECEXP (body, 0, i)));
+	    {
+	      rtx subinsn = XVECEXP (body, 0, i);
+	      if (LABEL_P (subinsn) || DELETED_NOTE_P (subinsn))
+		continue;
+
+	      this_block
+		= choose_inner_scope (this_block, insn_scope (subinsn));
+	    }
 	}
       if (! this_block)
 	continue;
@@ -767,6 +773,7 @@ fixup_reorder_chain (void)
     {
       edge e_fall, e_taken, e;
       rtx bb_end_insn;
+      rtx ret_label = NULL_RTX;
       basic_block nb;
       edge_iterator ei;
 
@@ -786,6 +793,7 @@ fixup_reorder_chain (void)
       bb_end_insn = BB_END (bb);
       if (JUMP_P (bb_end_insn))
 	{
+	  ret_label = JUMP_LABEL (bb_end_insn);
 	  if (any_condjump_p (bb_end_insn))
 	    {
 	      /* This might happen if the conditional jump has side
@@ -896,7 +904,7 @@ fixup_reorder_chain (void)
 	}
 
       /* We got here if we need to add a new jump insn.  */
-      nb = force_nonfallthru (e_fall);
+      nb = force_nonfallthru_and_redirect (e_fall, e_fall->dest, ret_label);
       if (nb)
 	{
 	  nb->il.rtl->visited = 1;
@@ -1130,24 +1138,30 @@ extern bool cfg_layout_can_duplicate_bb_p (const_basic_block);
 bool
 cfg_layout_can_duplicate_bb_p (const_basic_block bb)
 {
+  rtx insn;
+
   /* Do not attempt to duplicate tablejumps, as we need to unshare
      the dispatch table.  This is difficult to do, as the instructions
      computing jump destination may be hoisted outside the basic block.  */
   if (tablejump_p (BB_END (bb), NULL, NULL))
     return false;
 
-  /* Do not duplicate blocks containing insns that can't be copied.  */
-  if (targetm.cannot_copy_insn_p)
+  insn = BB_HEAD (bb);
+  while (1)
     {
-      rtx insn = BB_HEAD (bb);
-      while (1)
-	{
-	  if (INSN_P (insn) && targetm.cannot_copy_insn_p (insn))
-	    return false;
-	  if (insn == BB_END (bb))
-	    break;
-	  insn = NEXT_INSN (insn);
-	}
+      /* Do not duplicate blocks containing insns that can't be copied.  */
+      if (INSN_P (insn) && targetm.cannot_copy_insn_p
+	  && targetm.cannot_copy_insn_p (insn))
+	return false;
+      /* dwarf2out expects that these notes are always paired with a
+	 returnjump or sibling call.  */
+      if (NOTE_P (insn) && NOTE_KIND (insn) == NOTE_INSN_EPILOGUE_BEG
+	  && !returnjump_p (BB_END (bb))
+	  && (!CALL_P (BB_END (bb)) || !SIBLING_CALL_P (BB_END (bb))))
+	return false;
+      if (insn == BB_END (bb))
+	break;
+      insn = NEXT_INSN (insn);
     }
 
   return true;
@@ -1192,6 +1206,9 @@ duplicate_insn_chain (rtx from, rtx to)
 	      break;
 	    }
 	  copy = emit_copy_of_insn_after (insn, get_last_insn ());
+	  if (JUMP_P (insn) && JUMP_LABEL (insn) != NULL_RTX
+	      && ANY_RETURN_P (JUMP_LABEL (insn)))
+	    JUMP_LABEL (copy) = JUMP_LABEL (insn);
           maybe_copy_prologue_epilogue_insn (insn, copy);
 	  break;
 
diff --git a/gcc/cfgrtl.c b/gcc/cfgrtl.c
index f86f0a3..74da18e 100644
--- a/gcc/cfgrtl.c
+++ b/gcc/cfgrtl.c
@@ -843,11 +843,10 @@ try_redirect_by_replacing_jump (edge e, basic_block target, bool in_cfglayout)
       if (dump_file)
 	fprintf (dump_file, "Redirecting jump %i from %i to %i.\n",
 		 INSN_UID (insn), e->dest->index, target->index);
-      if (!redirect_jump (insn, block_label (target), 0))
-	{
-	  gcc_assert (target == EXIT_BLOCK_PTR);
-	  return NULL;
-	}
+      if (target == EXIT_BLOCK_PTR)
+	return NULL;
+      if (! redirect_jump (insn, block_label (target), 0))
+	gcc_unreachable ();
     }
 
   /* Cannot do anything for target exit block.  */
@@ -1027,11 +1026,10 @@ patch_jump_insn (rtx insn, rtx old_label, basic_block new_bb)
 	  /* If the substitution doesn't succeed, die.  This can happen
 	     if the back end emitted unrecognizable instructions or if
 	     target is exit block on some arches.  */
-	  if (!redirect_jump (insn, block_label (new_bb), 0))
-	    {
-	      gcc_assert (new_bb == EXIT_BLOCK_PTR);
-	      return false;
-	    }
+	  if (new_bb == EXIT_BLOCK_PTR)
+	    return false;
+	  if (! redirect_jump (insn, block_label (new_bb), 0))
+	    gcc_unreachable ();
 	}
     }
   return true;
@@ -1114,10 +1112,13 @@ rtl_redirect_edge_and_branch (edge e, basic_block target)
 }
 
 /* Like force_nonfallthru below, but additionally performs redirection
-   Used by redirect_edge_and_branch_force.  */
+   Used by redirect_edge_and_branch_force.  JUMP_LABEL is used only
+   when redirecting to the EXIT_BLOCK, it is either a return or a
+   simple_return rtx indicating which kind of returnjump to create.
+   It should be NULL otherwise.  */
 
-static basic_block
-force_nonfallthru_and_redirect (edge e, basic_block target)
+basic_block
+force_nonfallthru_and_redirect (edge e, basic_block target, rtx jump_label)
 {
   basic_block jump_block, new_bb = NULL, src = e->src;
   rtx note;
@@ -1284,11 +1285,25 @@ force_nonfallthru_and_redirect (edge e, basic_block target)
   e->flags &= ~EDGE_FALLTHRU;
   if (target == EXIT_BLOCK_PTR)
     {
+      if (jump_label == ret_rtx)
+	{
 #ifdef HAVE_return
-	emit_jump_insn_after_setloc (gen_return (), BB_END (jump_block), loc);
+	  emit_jump_insn_after_setloc (gen_return (), BB_END (jump_block),
+				       loc);
 #else
-	gcc_unreachable ();
+	  gcc_unreachable ();
+#endif
+	}
+      else
+	{
+	  gcc_assert (jump_label == simple_return_rtx);
+#ifdef HAVE_simple_return
+	  emit_jump_insn_after_setloc (gen_simple_return (),
+				       BB_END (jump_block), loc);
+#else
+	  gcc_unreachable ();
 #endif
+	}
     }
   else
     {
@@ -1315,7 +1330,7 @@ force_nonfallthru_and_redirect (edge e, basic_block target)
 basic_block
 force_nonfallthru (edge e)
 {
-  return force_nonfallthru_and_redirect (e, e->dest);
+  return force_nonfallthru_and_redirect (e, e->dest, NULL_RTX);
 }
 
 /* Redirect edge even at the expense of creating new jump insn or
@@ -1332,7 +1347,7 @@ rtl_redirect_edge_and_branch_force (edge e, basic_block target)
   /* In case the edge redirection failed, try to force it to be non-fallthru
      and redirect newly created simplejump.  */
   df_set_bb_dirty (e->src);
-  return force_nonfallthru_and_redirect (e, target);
+  return force_nonfallthru_and_redirect (e, target, NULL_RTX);
 }
 
 /* The given edge should potentially be a fallthru edge.  If that is in
diff --git a/gcc/cgraph.c b/gcc/cgraph.c
index 201e77d..8637440 100644
--- a/gcc/cgraph.c
+++ b/gcc/cgraph.c
@@ -515,8 +515,10 @@ cgraph_node (tree decl)
   if (DECL_CONTEXT (decl) && TREE_CODE (DECL_CONTEXT (decl)) == FUNCTION_DECL)
     {
       node->origin = cgraph_node (DECL_CONTEXT (decl));
+      node->origin->ever_was_nested = 1;
       node->next_nested = node->origin->nested;
       node->origin->nested = node;
+      node->ever_was_nested = 1;
     }
   if (assembler_name_hash)
     {
diff --git a/gcc/cgraph.h b/gcc/cgraph.h
index bad1bb9..e84f846 100644
--- a/gcc/cgraph.h
+++ b/gcc/cgraph.h
@@ -284,6 +284,8 @@ struct GTY((chain_next ("%h.next"), chain_prev ("%h.previous"))) cgraph_node {
   unsigned process : 1;
   /* Set for aliases once they got through assemble_alias.  */
   unsigned alias : 1;
+  /* Set if the function is a nested function or has nested functions.  */
+  unsigned ever_was_nested : 1;
   /* Set for nodes that was constructed and finalized by frontend.  */
   unsigned finalized_by_frontend : 1;
   /* Set for alias and thunk nodes, same_body points to the node they are alias
diff --git a/gcc/cgraphunit.c b/gcc/cgraphunit.c
index 7ed75be..48b5297e 100644
--- a/gcc/cgraphunit.c
+++ b/gcc/cgraphunit.c
@@ -2067,6 +2067,10 @@ cgraph_function_versioning (struct cgraph_node *old_version_node,
   SET_DECL_ASSEMBLER_NAME (new_decl, DECL_NAME (new_decl));
   SET_DECL_RTL (new_decl, NULL);
 
+  /* When the old decl was a con-/destructor make sure the clone isn't.  */
+  DECL_STATIC_CONSTRUCTOR(new_decl) = 0;
+  DECL_STATIC_DESTRUCTOR(new_decl) = 0;
+
   /* Create the new version's call-graph node.
      and update the edges of the new node. */
   new_version_node =
diff --git a/gcc/collect2.c b/gcc/collect2.c
index 42e35b6..2b7afa4 100644
--- a/gcc/collect2.c
+++ b/gcc/collect2.c
@@ -177,7 +177,6 @@ struct head
 bool vflag;				/* true if -v or --version */ 
 static int rflag;			/* true if -r */
 static int strip_flag;			/* true if -s */
-static const char *demangle_flag;
 #ifdef COLLECT_EXPORT_LIST
 static int export_flag;                 /* true if -bE */
 static int aix64_flag;			/* true if -b64 */
@@ -1159,10 +1158,12 @@ main (int argc, char **argv)
 
   num_c_args = argc + 9;
 
+#ifndef HAVE_LD_DEMANGLE
   no_demangle = !! getenv ("COLLECT_NO_DEMANGLE");
 
   /* Suppress demangling by the real linker, which may be broken.  */
-  putenv (xstrdup ("COLLECT_NO_DEMANGLE="));
+  putenv (xstrdup ("COLLECT_NO_DEMANGLE=1"));
+#endif
 
 #if defined (COLLECT2_HOST_INITIALIZATION)
   /* Perform system dependent initialization, if necessary.  */
@@ -1450,12 +1451,6 @@ main (int argc, char **argv)
   /* After the first file, put in the c++ rt0.  */
 
   first_file = 1;
-#ifdef HAVE_LD_DEMANGLE
-  if (!demangle_flag && !no_demangle)
-    demangle_flag = "--demangle";
-  if (demangle_flag)
-    *ld1++ = *ld2++ = demangle_flag;
-#endif
   while ((arg = *++argv) != (char *) 0)
     {
       *ld1++ = *ld2++ = arg;
@@ -1559,16 +1554,16 @@ main (int argc, char **argv)
 	    case '-':
 	      if (strcmp (arg, "--no-demangle") == 0)
 		{
-		  demangle_flag = arg;
+#ifndef HAVE_LD_DEMANGLE
 		  no_demangle = 1;
 		  ld1--;
 		  ld2--;
+#endif
 		}
 	      else if (strncmp (arg, "--demangle", 10) == 0)
 		{
-		  demangle_flag = arg;
-		  no_demangle = 0;
 #ifndef HAVE_LD_DEMANGLE
+		  no_demangle = 0;
 		  if (arg[10] == '=')
 		    {
 		      enum demangling_styles style
@@ -1578,9 +1573,9 @@ main (int argc, char **argv)
 		      else
 			current_demangling_style = style;
 		    }
-#endif
 		  ld1--;
 		  ld2--;
+#endif
 		}
 	      else if (strncmp (arg, "--sysroot=", 10) == 0)
 		target_system_root = arg + 10;
diff --git a/gcc/combine.c b/gcc/combine.c
index 0eed43a..96d99af 100644
--- a/gcc/combine.c
+++ b/gcc/combine.c
@@ -391,8 +391,8 @@ static rtx try_combine (rtx, rtx, rtx, rtx, int *, rtx);
 static void undo_all (void);
 static void undo_commit (void);
 static rtx *find_split_point (rtx *, rtx, bool);
-static rtx subst (rtx, rtx, rtx, int, int);
-static rtx combine_simplify_rtx (rtx, enum machine_mode, int);
+static rtx subst (rtx, rtx, rtx, int, int, int);
+static rtx combine_simplify_rtx (rtx, enum machine_mode, int, int);
 static rtx simplify_if_then_else (rtx);
 static rtx simplify_set (rtx);
 static rtx simplify_logical (rtx);
@@ -1821,6 +1821,10 @@ can_combine_p (rtx insn, rtx i3, rtx pred ATTRIBUTE_UNUSED,
      get_last_value, so set safe guard here.  */
   subst_low_luid = DF_INSN_LUID (insn);
 
+  /* The simplification in expand_field_assignment may call back to
+     get_last_value, so set safe guard here.  */
+  subst_low_luid = DF_INSN_LUID (insn);
+
   set = expand_field_assignment (set);
   src = SET_SRC (set), dest = SET_DEST (set);
 
@@ -3153,12 +3157,12 @@ try_combine (rtx i3, rtx i2, rtx i1, rtx i0, int *new_direct_jump_p,
 	  if (i1)
 	    {
 	      subst_low_luid = DF_INSN_LUID (i1);
-	      i1src = subst (i1src, pc_rtx, pc_rtx, 0, 0);
+	      i1src = subst (i1src, pc_rtx, pc_rtx, 0, 0, 0);
 	    }
 	  else
 	    {
 	      subst_low_luid = DF_INSN_LUID (i2);
-	      i2src = subst (i2src, pc_rtx, pc_rtx, 0, 0);
+	      i2src = subst (i2src, pc_rtx, pc_rtx, 0, 0, 0);
 	    }
 	}
 
@@ -3170,7 +3174,7 @@ try_combine (rtx i3, rtx i2, rtx i1, rtx i0, int *new_direct_jump_p,
 	 self-referential RTL when we will be substituting I1SRC for I1DEST
 	 later.  Likewise if I0 feeds into I2, either directly or indirectly
 	 through I1, and I0DEST is in I0SRC.  */
-      newpat = subst (PATTERN (i3), i2dest, i2src, 0,
+      newpat = subst (PATTERN (i3), i2dest, i2src, 0, 0,
 		      (i1_feeds_i2_n && i1dest_in_i1src)
 		      || ((i0_feeds_i2_n || (i0_feeds_i1_n && i1_feeds_i2_n))
 			  && i0dest_in_i0src));
@@ -3214,7 +3218,7 @@ try_combine (rtx i3, rtx i2, rtx i1, rtx i0, int *new_direct_jump_p,
 	 copy of I1SRC each time we substitute it, in order to avoid creating
 	 self-referential RTL when we will be substituting I0SRC for I0DEST
 	 later.  */
-      newpat = subst (newpat, i1dest, i1src, 0,
+      newpat = subst (newpat, i1dest, i1src, 0, 0,
 		      i0_feeds_i1_n && i0dest_in_i0src);
       substed_i1 = 1;
 
@@ -3248,7 +3252,7 @@ try_combine (rtx i3, rtx i2, rtx i1, rtx i0, int *new_direct_jump_p,
 
       n_occurrences = 0;
       subst_low_luid = DF_INSN_LUID (i0);
-      newpat = subst (newpat, i0dest, i0src, 0, 0);
+      newpat = subst (newpat, i0dest, i0src, 0, 0, 0);
       substed_i0 = 1;
     }
 
@@ -3310,7 +3314,7 @@ try_combine (rtx i3, rtx i2, rtx i1, rtx i0, int *new_direct_jump_p,
 	{
 	  rtx t = i1pat;
 	  if (i0_feeds_i1_n)
-	    t = subst (t, i0dest, i0src_copy ? i0src_copy : i0src, 0, 0);
+	    t = subst (t, i0dest, i0src_copy ? i0src_copy : i0src, 0, 0, 0);
 
 	  XVECEXP (newpat, 0, --total_sets) = t;
 	}
@@ -3318,10 +3322,10 @@ try_combine (rtx i3, rtx i2, rtx i1, rtx i0, int *new_direct_jump_p,
 	{
 	  rtx t = i2pat;
 	  if (i1_feeds_i2_n)
-	    t = subst (t, i1dest, i1src_copy ? i1src_copy : i1src, 0,
+	    t = subst (t, i1dest, i1src_copy ? i1src_copy : i1src, 0, 0,
 		       i0_feeds_i1_n && i0dest_in_i0src);
 	  if ((i0_feeds_i1_n && i1_feeds_i2_n) || i0_feeds_i2_n)
-	    t = subst (t, i0dest, i0src_copy2 ? i0src_copy2 : i0src, 0, 0);
+	    t = subst (t, i0dest, i0src_copy2 ? i0src_copy2 : i0src, 0, 0, 0);
 
 	  XVECEXP (newpat, 0, --total_sets) = t;
 	}
@@ -4998,11 +5002,13 @@ find_split_point (rtx *loc, rtx insn, bool set_src)
 
    IN_DEST is nonzero if we are processing the SET_DEST of a SET.
 
+   IN_COND is nonzero if we are at the top level of a condition.
+
    UNIQUE_COPY is nonzero if each substitution must be unique.  We do this
    by copying if `n_occurrences' is nonzero.  */
 
 static rtx
-subst (rtx x, rtx from, rtx to, int in_dest, int unique_copy)
+subst (rtx x, rtx from, rtx to, int in_dest, int in_cond, int unique_copy)
 {
   enum rtx_code code = GET_CODE (x);
   enum machine_mode op0_mode = VOIDmode;
@@ -5063,7 +5069,7 @@ subst (rtx x, rtx from, rtx to, int in_dest, int unique_copy)
       && GET_CODE (XVECEXP (x, 0, 0)) == SET
       && GET_CODE (SET_SRC (XVECEXP (x, 0, 0))) == ASM_OPERANDS)
     {
-      new_rtx = subst (XVECEXP (x, 0, 0), from, to, 0, unique_copy);
+      new_rtx = subst (XVECEXP (x, 0, 0), from, to, 0, 0, unique_copy);
 
       /* If this substitution failed, this whole thing fails.  */
       if (GET_CODE (new_rtx) == CLOBBER
@@ -5080,7 +5086,7 @@ subst (rtx x, rtx from, rtx to, int in_dest, int unique_copy)
 	      && GET_CODE (dest) != CC0
 	      && GET_CODE (dest) != PC)
 	    {
-	      new_rtx = subst (dest, from, to, 0, unique_copy);
+	      new_rtx = subst (dest, from, to, 0, 0, unique_copy);
 
 	      /* If this substitution failed, this whole thing fails.  */
 	      if (GET_CODE (new_rtx) == CLOBBER
@@ -5126,8 +5132,8 @@ subst (rtx x, rtx from, rtx to, int in_dest, int unique_copy)
 		    }
 		  else
 		    {
-		      new_rtx = subst (XVECEXP (x, i, j), from, to, 0,
-				   unique_copy);
+		      new_rtx = subst (XVECEXP (x, i, j), from, to, 0, 0,
+				       unique_copy);
 
 		      /* If this substitution failed, this whole thing
 			 fails.  */
@@ -5204,7 +5210,9 @@ subst (rtx x, rtx from, rtx to, int in_dest, int unique_copy)
 				&& (code == SUBREG || code == STRICT_LOW_PART
 				    || code == ZERO_EXTRACT))
 			       || code == SET)
-			      && i == 0), unique_copy);
+			      && i == 0),
+				 code == IF_THEN_ELSE && i == 0,
+				 unique_copy);
 
 	      /* If we found that we will have to reject this combination,
 		 indicate that by returning the CLOBBER ourselves, rather than
@@ -5261,7 +5269,7 @@ subst (rtx x, rtx from, rtx to, int in_dest, int unique_copy)
       /* If X is sufficiently simple, don't bother trying to do anything
 	 with it.  */
       if (code != CONST_INT && code != REG && code != CLOBBER)
-	x = combine_simplify_rtx (x, op0_mode, in_dest);
+	x = combine_simplify_rtx (x, op0_mode, in_dest, in_cond);
 
       if (GET_CODE (x) == code)
 	break;
@@ -5281,10 +5289,12 @@ subst (rtx x, rtx from, rtx to, int in_dest, int unique_copy)
    expression.
 
    OP0_MODE is the original mode of XEXP (x, 0).  IN_DEST is nonzero
-   if we are inside a SET_DEST.  */
+   if we are inside a SET_DEST.  IN_COND is nonzero if we are at the top level
+   of a condition.  */
 
 static rtx
-combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int in_dest)
+combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int in_dest,
+		      int in_cond)
 {
   enum rtx_code code = GET_CODE (x);
   enum machine_mode mode = GET_MODE (x);
@@ -5339,8 +5349,8 @@ combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int in_dest)
 	     false arms to store-flag values.  Be careful to use copy_rtx
 	     here since true_rtx or false_rtx might share RTL with x as a
 	     result of the if_then_else_cond call above.  */
-	  true_rtx = subst (copy_rtx (true_rtx), pc_rtx, pc_rtx, 0, 0);
-	  false_rtx = subst (copy_rtx (false_rtx), pc_rtx, pc_rtx, 0, 0);
+	  true_rtx = subst (copy_rtx (true_rtx), pc_rtx, pc_rtx, 0, 0, 0);
+	  false_rtx = subst (copy_rtx (false_rtx), pc_rtx, pc_rtx, 0, 0, 0);
 
 	  /* If true_rtx and false_rtx are not general_operands, an if_then_else
 	     is unlikely to be simpler.  */
@@ -5684,7 +5694,7 @@ combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int in_dest)
 	{
 	  /* Try to simplify the expression further.  */
 	  rtx tor = simplify_gen_binary (IOR, mode, XEXP (x, 0), XEXP (x, 1));
-	  temp = combine_simplify_rtx (tor, VOIDmode, in_dest);
+	  temp = combine_simplify_rtx (tor, VOIDmode, in_dest, 0);
 
 	  /* If we could, great.  If not, do not go ahead with the IOR
 	     replacement, since PLUS appears in many special purpose
@@ -5775,9 +5785,17 @@ combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int in_dest)
 	     Remove any ZERO_EXTRACT we made when thinking this was a
 	     comparison.  It may now be simpler to use, e.g., an AND.  If a
 	     ZERO_EXTRACT is indeed appropriate, it will be placed back by
-	     the call to make_compound_operation in the SET case.  */
+	     the call to make_compound_operation in the SET case.
+
+	     Don't apply these optimizations if the caller would
+	     prefer a comparison rather than a value.
+	     E.g., for the condition in an IF_THEN_ELSE most targets need
+	     an explicit comparison.  */
+
+	  if (in_cond)
+	    ;
 
-	  if (STORE_FLAG_VALUE == 1
+	  else if (STORE_FLAG_VALUE == 1
 	      && new_code == NE && GET_MODE_CLASS (mode) == MODE_INT
 	      && op1 == const0_rtx
 	      && mode == GET_MODE (op0)
@@ -5823,7 +5841,10 @@ combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int in_dest)
 
 	  /* If STORE_FLAG_VALUE is -1, we have cases similar to
 	     those above.  */
-	  if (STORE_FLAG_VALUE == -1
+	  if (in_cond)
+	    ;
+
+	  else if (STORE_FLAG_VALUE == -1
 	      && new_code == NE && GET_MODE_CLASS (mode) == MODE_INT
 	      && op1 == const0_rtx
 	      && (num_sign_bit_copies (op0, mode)
@@ -6021,11 +6042,11 @@ simplify_if_then_else (rtx x)
       if (reg_mentioned_p (from, true_rtx))
 	true_rtx = subst (known_cond (copy_rtx (true_rtx), true_code,
 				      from, true_val),
-		      pc_rtx, pc_rtx, 0, 0);
+			  pc_rtx, pc_rtx, 0, 0, 0);
       if (reg_mentioned_p (from, false_rtx))
 	false_rtx = subst (known_cond (copy_rtx (false_rtx), false_code,
 				   from, false_val),
-		       pc_rtx, pc_rtx, 0, 0);
+			   pc_rtx, pc_rtx, 0, 0, 0);
 
       SUBST (XEXP (x, 1), swapped ? false_rtx : true_rtx);
       SUBST (XEXP (x, 2), swapped ? true_rtx : false_rtx);
@@ -6242,11 +6263,11 @@ simplify_if_then_else (rtx x)
 	{
 	  temp = subst (simplify_gen_relational (true_code, m, VOIDmode,
 						 cond_op0, cond_op1),
-			pc_rtx, pc_rtx, 0, 0);
+			pc_rtx, pc_rtx, 0, 0, 0);
 	  temp = simplify_gen_binary (MULT, m, temp,
 				      simplify_gen_binary (MULT, m, c1,
 							   const_true_rtx));
-	  temp = subst (temp, pc_rtx, pc_rtx, 0, 0);
+	  temp = subst (temp, pc_rtx, pc_rtx, 0, 0, 0);
 	  temp = simplify_gen_binary (op, m, gen_lowpart (m, z), temp);
 
 	  if (extend_op != UNKNOWN)
@@ -6326,10 +6347,18 @@ simplify_set (rtx x)
       enum rtx_code new_code;
       rtx op0, op1, tmp;
       int other_changed = 0;
+      rtx inner_compare = NULL_RTX;
       enum machine_mode compare_mode = GET_MODE (dest);
 
       if (GET_CODE (src) == COMPARE)
-	op0 = XEXP (src, 0), op1 = XEXP (src, 1);
+	{
+	  op0 = XEXP (src, 0), op1 = XEXP (src, 1);
+	  if (GET_CODE (op0) == COMPARE && op1 == const0_rtx)
+	    {
+	      inner_compare = op0;
+	      op0 = XEXP (inner_compare, 0), op1 = XEXP (inner_compare, 1);
+	    }
+	}
       else
 	op0 = src, op1 = CONST0_RTX (GET_MODE (src));
 
@@ -6371,6 +6400,12 @@ simplify_set (rtx x)
 	 need to use a different CC mode here.  */
       if (GET_MODE_CLASS (GET_MODE (op0)) == MODE_CC)
 	compare_mode = GET_MODE (op0);
+      else if (inner_compare
+	       && GET_MODE_CLASS (GET_MODE (inner_compare)) == MODE_CC
+	       && new_code == old_code
+	       && op0 == XEXP (inner_compare, 0)
+	       && op1 == XEXP (inner_compare, 1))
+	compare_mode = GET_MODE (inner_compare);
       else
 	compare_mode = SELECT_CC_MODE (new_code, op0, op1);
 
diff --git a/gcc/common.opt b/gcc/common.opt
index 75e29de..fcfac61 100644
--- a/gcc/common.opt
+++ b/gcc/common.opt
@@ -212,6 +212,12 @@ Driver Alias(c)
 -coverage
 Driver Alias(coverage)
 
+flicense-me
+Common RejectNegative Undocumented Var(license_me_flag)
+
+ffeature-proxy
+Common Undocumented Var(feature_proxy_flag) Init(1)
+
 -debug
 Common Alias(g)
 
@@ -570,6 +576,10 @@ Wpadded
 Common Var(warn_padded) Warning
 Warn when padding is required to align structure members
 
+Wpoison-system-directories
+Common Var(flag_poison_system_directories) Init(1) Warning
+Warn for -I and -L options using system directories if cross compiling
+
 Wshadow
 Common Var(warn_shadow) Warning
 Warn when one local variable shadows another
@@ -763,6 +773,12 @@ Driver Undocumented
 fabi-version=
 Common Joined RejectNegative UInteger Var(flag_abi_version) Init(2)
 
+falign-arrays
+Target Report Var(flag_align_arrays)
+Set the minimum alignment for array variables to be the largest power
+of two less than or equal to their total storage size, or the biggest
+alignment used on the machine, whichever is smaller.
+
 falign-functions
 Common Report Var(align_functions,0) Optimization UInteger
 Align the start of functions
@@ -1004,6 +1020,14 @@ fearly-inlining
 Common Report Var(flag_early_inlining) Init(1) Optimization
 Perform early inlining
 
+fextension-elimination
+Common Report Var(flag_ee) Init(0) Optimization
+Perform extension elimination
+
+feglibc=
+Common Report Joined Undocumented
+EGLIBC configuration specifier, serves multilib purposes.
+
 feliminate-dwarf2-dups
 Common Report Var(flag_eliminate_dwarf2_dups)
 Perform DWARF2 duplicate elimination
@@ -1227,6 +1251,10 @@ finstrument-functions
 Common Report Var(flag_instrument_function_entry_exit)
 Instrument function entry and exit with profiling calls
 
+finstrument-function-calls
+Common Report Var(flag_instrument_call_entry_exit)
+Instrument call entry and exit with profiling calls
+
 finstrument-functions-exclude-function-list=
 Common RejectNegative Joined
 -finstrument-functions-exclude-function-list=name,...  Do not instrument listed functions
@@ -1556,6 +1584,10 @@ fprofile-values
 Common Report Var(flag_profile_values)
 Insert code to profile values of expressions
 
+fpromote-loop-indices
+Common Report Var(flag_promote_loop_indices) Optimization
+Promote loop indices to word-sized indices when safe
+
 frandom-seed
 Common Var(common_deferred_options) Defer
 
@@ -1734,6 +1766,11 @@ fshow-column
 Common Report Var(flag_show_column) Init(1)
 Show column numbers in diagnostics, when available.  Default on
 
+fshrink-wrap
+Common Report Var(flag_shrink_wrap) Optimization
+Emit function prologues only before parts of the function that need it,
+rather than at the top of the function.
+
 fsignaling-nans
 Common Report Var(flag_signaling_nans) Optimization SetByCombined
 Disable optimizations observable by IEEE signaling NaNs
@@ -1899,6 +1936,10 @@ ftree-switch-conversion
 Common Report Var(flag_tree_switch_conversion) Optimization
 Perform conversions of switch initializations.
 
+ftree-if-to-switch-conversion
+Common Report Var(flag_tree_if_to_switch_conversion) Optimization
+Perform conversions of chains of ifs into switches.
+
 ftree-dce
 Common Report Var(flag_tree_dce) Optimization
 Enable SSA dead code elimination optimization on trees
@@ -1959,6 +2000,15 @@ ftree-pta
 Common Report Var(flag_tree_pta) Init(1) Optimization
 Perform function-local points-to analysis on trees.
 
+ftree-pre-partial-partial
+Common Report Var(flag_tree_pre_partial_partial) Optimization
+In SSA-PRE optimization on trees, enable partial-partial redundancy elimination.
+
+ftree-pre-partial-partial-obliviously
+Common Report Var(flag_tree_pre_partial_partial_obliviously) Optimization
+In SSA-PRE optimization on trees, enable partial-partial redundancy
+elimination without regard for the cost of the inserted phi nodes.
+
 ftree-reassoc
 Common Report Var(flag_tree_reassoc) Init(1) Optimization
 Enable reassociation on tree level
@@ -2360,4 +2410,8 @@ Create a position independent executable
 z
 Driver Joined Separate
 
+fuse-caller-save
+Common Report Var(flag_use_caller_save) Optimization
+Use caller save register across calls if possible
+
 ; This comment is to ensure we retain the blank line above.
diff --git a/gcc/config.gcc b/gcc/config.gcc
index 6dc2427..ffc20de 100644
--- a/gcc/config.gcc
+++ b/gcc/config.gcc
@@ -896,9 +923,8 @@ arm*-*-eabi* | arm*-*-symbianelf* | arm*-*-rtems*)
 	  ;;
 	arm*-*-symbianelf*)
 	  tm_file="${tm_file} arm/symbian.h"
-	  # We do not include t-bpabi for Symbian OS because the system
-	  # provides its own implementation of the BPABI functions.
-	  tmake_file="${tmake_file} arm/t-symbian"
+	  tmake_file="${tmake_file} t-slibgcc-symbian t-libc-ok"
+	  tmake_file="${tmake_file} arm/t-bpabi arm/t-symbian"
 	  ;;
 	esac
 	tm_file="${tm_file} arm/aout.h arm/arm.h"
@@ -3196,7 +3289,7 @@ case "${target}" in
 		;;
 
 	arm*-*-*)
-		supported_defaults="arch cpu float tune fpu abi mode"
+		supported_defaults="arch cpu float tune fpu abi mode tls"
 		for which in cpu tune; do
 			# See if it matches any of the entries in arm-cores.def
 			eval "val=\$with_$which"
@@ -3279,6 +3372,17 @@ case "${target}" in
 			;;
 		esac
 
+		case "$with_tls" in
+		"" \
+		| gnu | gnu2)
+			# OK
+			;;
+		*)
+			echo "Unknown TLS method used in --with-tls=$with_tls" 1>&2
+			exit 1
+			;;
+		esac
+
 		if test "x$with_arch" != x && test "x$with_cpu" != x; then
 			echo "Warning: --with-arch overrides --with-cpu=$with_cpu" 1>&2
 		fi
@@ -3759,8 +3870,24 @@ case ${target} in
 		;;
 esac
 
+case ${target} in
+    *-eglibc-*-*)
+	tmake_file="${tmake_file} t-eglibc"
+
+	case ${target} in
+	    arm-*)
+	     # ARM already includes below.
+		;;
+	    *)
+		tmake_file="${tmake_file} t-sysroot-suffix"
+		tm_file="${tm_file} ./sysroot-suffix.h"
+		;;
+	esac
+	;;
+esac
+
 t=
-all_defaults="abi cpu cpu_32 cpu_64 arch arch_32 arch_64 tune tune_32 tune_64 schedule float mode fpu divide llsc mips-plt synci"
+all_defaults="abi cpu cpu_32 cpu_64 arch arch_32 arch_64 tune tune_32 tune_64 schedule float mode fpu divide llsc mips-plt synci tls"
 for option in $all_defaults
 do
 	eval "val=\$with_"`echo $option | sed s/-/_/g`
diff --git a/gcc/config.in b/gcc/config.in
index 584ec65..129819d 100644
--- a/gcc/config.in
+++ b/gcc/config.in
@@ -18,6 +18,18 @@
 #endif
 
 
+/* Required license feature */
+#ifndef USED_FOR_TARGET
+#undef CSL_LICENSE_FEATURE
+#endif
+
+
+/* Required license version */
+#ifndef USED_FOR_TARGET
+#undef CSL_LICENSE_VERSION
+#endif
+
+
 /* Define to enable the use of a default assembler. */
 #ifndef USED_FOR_TARGET
 #undef DEFAULT_ASSEMBLER
@@ -144,6 +156,12 @@
 #endif
 
 
+/* Define to warn for use of native system header directories */
+#ifndef USED_FOR_TARGET
+#undef ENABLE_POISON_SYSTEM_DIRECTORIES
+#endif
+
+
 /* Define if you want all operations on RTL (the basic data structure of the
    optimizer and back end) to be checked for dynamic type safety at runtime.
    This is quite expensive. */
@@ -956,6 +974,13 @@
 #endif
 
 
+/* Define if your assembler supports generation of .eh_frame_entry from CFI
+   directives. */
+#ifndef USED_FOR_TARGET
+#undef HAVE_GAS_EH_FRAME_ENTRY
+#endif
+
+
 /* Define if your assembler supports @gnu_unique_object. */
 #ifndef USED_FOR_TARGET
 #undef HAVE_GAS_GNU_UNIQUE_OBJECT
diff --git a/gcc/config/arm/arm-modes.def b/gcc/config/arm/arm-modes.def
index 24e3d90..7f19ebe 100644
--- a/gcc/config/arm/arm-modes.def
+++ b/gcc/config/arm/arm-modes.def
@@ -70,6 +70,12 @@ VECTOR_MODES (INT, 16);       /* V16QI V8HI V4SI V2DI */
 VECTOR_MODES (FLOAT, 8);      /*            V4HF V2SF */
 VECTOR_MODES (FLOAT, 16);     /*       V8HF V4SF V2DF */
 
+/* Fraction and accumulator vector modes.  */
+VECTOR_MODES (FRACT, 4);      /* V4QQ  V2HQ */
+VECTOR_MODES (UFRACT, 4);     /* V4UQQ V2UHQ */
+VECTOR_MODES (ACCUM, 4);      /*       V2HA */
+VECTOR_MODES (UACCUM, 4);     /*       V2UHA */
+
 /* Opaque integer modes for 3, 4, 6 or 8 Neon double registers (2 is
    TImode).  */
 INT_MODE (EI, 24);
diff --git a/gcc/config/arm/arm-protos.h b/gcc/config/arm/arm-protos.h
index f037a45..b6d2fe5 100644
--- a/gcc/config/arm/arm-protos.h
+++ b/gcc/config/arm/arm-protos.h
@@ -24,6 +24,7 @@
 #define GCC_ARM_PROTOS_H
 
 extern int use_return_insn (int, rtx);
+extern bool use_simple_return_p (void);
 extern enum reg_class arm_regno_class (int);
 extern void arm_load_pic_register (unsigned long);
 extern int arm_volatile_func (void);
@@ -54,6 +55,7 @@ extern rtx legitimize_pic_address (rtx, enum machine_mode, rtx);
 extern rtx legitimize_tls_address (rtx, rtx);
 extern int arm_legitimate_address_outer_p (enum machine_mode, rtx, RTX_CODE, int);
 extern int thumb_legitimate_offset_p (enum machine_mode, HOST_WIDE_INT);
+extern int thumb1_legitimate_address_p (enum machine_mode, rtx, int);
 extern bool arm_legitimize_reload_address (rtx *, enum machine_mode, int, int,
 					   int);
 extern rtx thumb_legitimize_reload_address (rtx *, enum machine_mode, int, int,
@@ -135,7 +137,7 @@ extern int arm_address_offset_is_imm (rtx);
 extern const char *output_add_immediate (rtx *);
 extern const char *arithmetic_instr (rtx, int);
 extern void output_ascii_pseudo_op (FILE *, const unsigned char *, int);
-extern const char *output_return_instruction (rtx, int, int);
+extern const char *output_return_instruction (rtx, bool, bool, bool);
 extern void arm_poke_function_name (FILE *, const char *);
 extern void arm_final_prescan_insn (rtx);
 extern int arm_debugger_arg_offset (int, rtx);
@@ -175,6 +177,7 @@ extern int is_called_in_ARM_mode (tree);
 #endif
 extern int thumb_shiftable_const (unsigned HOST_WIDE_INT);
 #ifdef RTX_CODE
+extern enum arm_cond_code maybe_get_arm_condition_code (rtx);
 extern void thumb1_final_prescan_insn (rtx);
 extern void thumb2_final_prescan_insn (rtx);
 extern const char *thumb_load_double_from_address (rtx *);
@@ -220,9 +223,15 @@ struct tune_params
   bool (*rtx_costs) (rtx, RTX_CODE, RTX_CODE, int *, bool);
   bool (*sched_adjust_cost) (rtx, rtx, rtx, int *);
   int constant_limit;
+  /* Maximum number of instructions to conditionalise in
+     arm_final_prescan_insn.  */
+  int max_insns_skipped;
   int num_prefetch_slots;
   int l1_cache_size;
   int l1_cache_line_size;
+  bool prefer_constant_pool;
+  int (*branch_cost) (bool, bool);
+  bool aggressive_unrolling;
 };
 
 extern const struct tune_params *current_tune;
diff --git a/gcc/config/arm/arm-tune.md b/gcc/config/arm/arm-tune.md
index 9b664e7..bd10833 100644
--- a/gcc/config/arm/arm-tune.md
+++ b/gcc/config/arm/arm-tune.md
@@ -1,5 +1,5 @@
 ;; -*- buffer-read-only: t -*-
 ;; Generated automatically by gentune.sh from arm-cores.def
 (define_attr "tune"
-	"arm2,arm250,arm3,arm6,arm60,arm600,arm610,arm620,arm7,arm7d,arm7di,arm70,arm700,arm700i,arm710,arm720,arm710c,arm7100,arm7500,arm7500fe,arm7m,arm7dm,arm7dmi,arm8,arm810,strongarm,strongarm110,strongarm1100,strongarm1110,fa526,fa626,arm7tdmi,arm7tdmis,arm710t,arm720t,arm740t,arm9,arm9tdmi,arm920,arm920t,arm922t,arm940t,ep9312,arm10tdmi,arm1020t,arm9e,arm946es,arm966es,arm968es,arm10e,arm1020e,arm1022e,xscale,iwmmxt,iwmmxt2,fa606te,fa626te,fmp626,fa726te,arm926ejs,arm1026ejs,arm1136js,arm1136jfs,arm1176jzs,arm1176jzfs,mpcorenovfp,mpcore,arm1156t2s,arm1156t2fs,cortexa5,cortexa8,cortexa9,cortexa15,cortexr4,cortexr4f,cortexm4,cortexm3,cortexm1,cortexm0"
+	"arm2,arm250,arm3,arm6,arm60,arm600,arm610,arm620,arm7,arm7d,arm7di,arm70,arm700,arm700i,arm710,arm720,arm710c,arm7100,arm7500,arm7500fe,arm7m,arm7dm,arm7dmi,arm8,arm810,strongarm,strongarm110,strongarm1100,strongarm1110,fa526,fa626,arm7tdmi,arm7tdmis,arm710t,arm720t,arm740t,arm9,arm9tdmi,arm920,arm920t,arm922t,arm940t,ep9312,arm10tdmi,arm1020t,arm9e,arm946es,arm966es,arm968es,arm10e,arm1020e,arm1022e,xscale,iwmmxt,iwmmxt2,fa606te,fa626te,fmp626,fa726te,arm926ejs,arm1026ejs,arm1136js,arm1136jfs,arm1176jzs,arm1176jzfs,mpcorenovfp,mpcore,arm1156t2s,arm1156t2fs,cortexa5,cortexa8,cortexa9,cortexa15,cortexr4,cortexr4f,cortexr5,cortexm4,cortexm3,cortexm1,cortexm0"
 	(const (symbol_ref "((enum attr_tune) arm_tune)")))
diff --git a/gcc/config/arm/arm.c b/gcc/config/arm/arm.c
index c3c5aa1..3740ff8 100644
--- a/gcc/config/arm/arm.c
+++ b/gcc/config/arm/arm.c
@@ -145,6 +145,7 @@ static void arm_output_mi_thunk (FILE *, tree, HOST_WIDE_INT, HOST_WIDE_INT,
 static bool arm_have_conditional_execution (void);
 static bool arm_rtx_costs_1 (rtx, enum rtx_code, int*, bool);
 static bool arm_size_rtx_costs (rtx, enum rtx_code, enum rtx_code, int *);
+static bool thumb2_size_rtx_costs (rtx, enum rtx_code, enum rtx_code, int *);
 static bool arm_slowmul_rtx_costs (rtx, enum rtx_code, enum rtx_code, int *, bool);
 static bool arm_fastmul_rtx_costs (rtx, enum rtx_code, enum rtx_code, int *, bool);
 static bool arm_xscale_rtx_costs (rtx, enum rtx_code, enum rtx_code, int *, bool);
@@ -227,6 +228,7 @@ static int arm_issue_rate (void);
 static void arm_output_dwarf_dtprel (FILE *, int, rtx) ATTRIBUTE_UNUSED;
 static bool arm_output_addr_const_extra (FILE *, rtx);
 static bool arm_allocate_stack_slots_for_args (void);
+static bool arm_warn_func_result (void);
 static const char *arm_invalid_parameter_type (const_tree t);
 static const char *arm_invalid_return_type (const_tree t);
 static tree arm_promoted_type (const_tree t);
@@ -251,6 +253,8 @@ static bool arm_builtin_support_vector_misalignment (enum machine_mode mode,
 						     bool is_packed);
 static void arm_conditional_register_usage (void);
 static reg_class_t arm_preferred_rename_class (reg_class_t rclass);
+static int arm_default_branch_cost (bool, bool);
+static int arm_cortex_a5_branch_cost (bool, bool);
 
 
 /* Table of machine attributes.  */
@@ -297,6 +301,11 @@ static const struct default_options arm_option_optimization_table[] =
     /* Enable section anchors by default at -O1 or higher.  */
     { OPT_LEVELS_1_PLUS, OPT_fsection_anchors, NULL, 1 },
     { OPT_LEVELS_1_PLUS, OPT_fomit_frame_pointer, NULL, 1 },
+    /* CSL LOCAL */
+    { OPT_LEVELS_2_PLUS, OPT_funroll_loops, NULL, 2 },
+    /* Promote loop indices to int where possible.  Consider moving this
+       to -Os, also.  */
+    { OPT_LEVELS_1_PLUS_SPEED_ONLY, OPT_fpromote_loop_indices, NULL, 1 },
     { OPT_LEVELS_NONE, 0, NULL, 0 }
   };
 
@@ -436,6 +445,9 @@ static const struct default_options arm_option_optimization_table[] =
 #undef TARGET_TRAMPOLINE_ADJUST_ADDRESS
 #define TARGET_TRAMPOLINE_ADJUST_ADDRESS arm_trampoline_adjust_address
 
+#undef TARGET_WARN_FUNC_RESULT
+#define TARGET_WARN_FUNC_RESULT arm_warn_func_result
+
 #undef TARGET_DEFAULT_SHORT_ENUMS
 #define TARGET_DEFAULT_SHORT_ENUMS arm_default_short_enums
 
@@ -634,6 +646,9 @@ enum arm_abi_type arm_abi;
 /* Which thread pointer model to use.  */
 enum arm_tp_type target_thread_pointer = TP_AUTO;
 
+/* Which tls dialect to use.  */
+enum arm_tls_type target_tls_dialect = TLS_GNU;
+
 /* Used to parse -mstructure_size_boundary command line option.  */
 int    arm_structure_size_boundary = DEFAULT_STRUCTURE_SIZE_BOUNDARY;
 
@@ -663,12 +678,13 @@ static int thumb_call_reg_needed;
 #define FL_THUMB2     (1 << 16)	      /* Thumb-2.  */
 #define FL_NOTM	      (1 << 17)	      /* Instructions not present in the 'M'
 					 profile.  */
-#define FL_DIV	      (1 << 18)	      /* Hardware divide.  */
+#define FL_THUMB_DIV  (1 << 18)	      /* Hardware divide (Thumb mode).  */
 #define FL_VFPV3      (1 << 19)       /* Vector Floating Point V3.  */
 #define FL_NEON       (1 << 20)       /* Neon instructions.  */
 #define FL_ARCH7EM    (1 << 21)	      /* Instructions present in the ARMv7E-M
 					 architecture.  */
 #define FL_ARCH7      (1 << 22)       /* Architecture 7.  */
+#define FL_ARM_DIV    (1 << 23)	      /* Hardware divide (ARM mode).  */
 
 #define FL_IWMMXT     (1 << 29)	      /* XScale v2 or "Intel Wireless MMX technology".  */
 
@@ -1044,7 +1142,8 @@ enum tls_reloc {
   TLS_LDM32,
   TLS_LDO32,
   TLS_IE32,
-  TLS_LE32
+  TLS_LE32,
+  TLS_DESCSEQ	/* GNU scheme */
 };
 
 /* The maximum number of insns to be used when loading a constant.  */
@@ -1077,6 +1176,49 @@ bit_count (unsigned long value)
   return count;
 }
 
+typedef struct
+{
+  enum machine_mode mode;
+  const char *name;
+} arm_fixed_mode_set;
+
+/* A small helper for setting fixed-point library libfuncs.  */
+
+static void
+arm_set_fixed_optab_libfunc (optab optable, enum machine_mode mode,
+			     const char *funcname, const char *modename,
+			     int num_suffix)
+{
+  char buffer[50];
+  
+  if (num_suffix == 0)
+    sprintf (buffer, "__gnu_%s%s", funcname, modename);
+  else
+    sprintf (buffer, "__gnu_%s%s%d", funcname, modename, num_suffix);
+  
+  set_optab_libfunc (optable, mode, buffer);
+}
+
+static void
+arm_set_fixed_conv_libfunc (convert_optab optable, enum machine_mode to,
+			    enum machine_mode from, const char *funcname,
+			    const char *toname, const char *fromname)
+{
+  char buffer[50];
+  char *maybe_suffix_2 = "";
+  
+  /* Follow the logic for selecting a "2" suffix in fixed-bit.h.  */
+  if (ALL_FIXED_POINT_MODE_P (from) && ALL_FIXED_POINT_MODE_P (to)
+      && UNSIGNED_FIXED_POINT_MODE_P (from) == UNSIGNED_FIXED_POINT_MODE_P (to)
+      && ALL_FRACT_MODE_P (from) == ALL_FRACT_MODE_P (to))
+    maybe_suffix_2 = "2";
+  
+  sprintf (buffer, "__gnu_%s%s%s%s", funcname, fromname, toname,
+	   maybe_suffix_2);
+
+  set_conv_libfunc (optable, to, from, buffer);
+}
+
 /* Set up library functions unique to ARM.  */
 
 static void
@@ -1194,12 +1336,12 @@ arm_init_libfuncs (void)
       /* Conversions.  */
       set_conv_libfunc (trunc_optab, HFmode, SFmode,
 			(arm_fp16_format == ARM_FP16_FORMAT_IEEE
-			 ? "__gnu_f2h_ieee"
-			 : "__gnu_f2h_alternative"));
+			 ? "__aeabi_f2h"
+			 : "__aeabi_f2h_alt"));
       set_conv_libfunc (sext_optab, SFmode, HFmode, 
 			(arm_fp16_format == ARM_FP16_FORMAT_IEEE
-			 ? "__gnu_h2f_ieee"
-			 : "__gnu_h2f_alternative"));
+			 ? "__aeabi_h2f"
+			 : "__aeabi_h2f_alt"));
       
       /* Arithmetic.  */
       set_optab_libfunc (add_optab, HFmode, NULL);
@@ -1222,6 +1364,137 @@ arm_init_libfuncs (void)
       break;
     }
 
+  /* Use names prefixed with __gnu_ for fixed-point helper functions.  */
+  {
+    const arm_fixed_mode_set fixed_arith_modes[] =
+      {
+	{ QQmode, "qq" },
+	{ UQQmode, "uqq" },
+	{ HQmode, "hq" },
+	{ UHQmode, "uhq" },
+	{ SQmode, "sq" },
+	{ USQmode, "usq" },
+	{ DQmode, "dq" },
+	{ UDQmode, "udq" },
+	{ TQmode, "tq" },
+	{ UTQmode, "utq" },
+	{ HAmode, "ha" },
+	{ UHAmode, "uha" },
+	{ SAmode, "sa" },
+	{ USAmode, "usa" },
+	{ DAmode, "da" },
+	{ UDAmode, "uda" },
+	{ TAmode, "ta" },
+	{ UTAmode, "uta" }
+      };
+    const arm_fixed_mode_set fixed_conv_modes[] =
+      {
+	{ QQmode, "qq" },
+	{ UQQmode, "uqq" },
+	{ HQmode, "hq" },
+	{ UHQmode, "uhq" },
+	{ SQmode, "sq" },
+	{ USQmode, "usq" },
+	{ DQmode, "dq" },
+	{ UDQmode, "udq" },
+	{ TQmode, "tq" },
+	{ UTQmode, "utq" },
+	{ HAmode, "ha" },
+	{ UHAmode, "uha" },
+	{ SAmode, "sa" },
+	{ USAmode, "usa" },
+	{ DAmode, "da" },
+	{ UDAmode, "uda" },
+	{ TAmode, "ta" },
+	{ UTAmode, "uta" },
+	{ QImode, "qi" },
+	{ HImode, "hi" },
+	{ SImode, "si" },
+	{ DImode, "di" },
+	{ TImode, "ti" },
+	{ SFmode, "sf" },
+	{ DFmode, "df" }
+      };
+    unsigned int i, j;
+
+    for (i = 0; i < ARRAY_SIZE (fixed_arith_modes); i++)
+      {
+	arm_set_fixed_optab_libfunc (add_optab, fixed_arith_modes[i].mode,
+				     "add", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (ssadd_optab, fixed_arith_modes[i].mode,
+				     "ssadd", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (usadd_optab, fixed_arith_modes[i].mode,
+				     "usadd", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (sub_optab, fixed_arith_modes[i].mode,
+				     "sub", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (sssub_optab, fixed_arith_modes[i].mode,
+				     "sssub", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (ussub_optab, fixed_arith_modes[i].mode,
+				     "ussub", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (smul_optab, fixed_arith_modes[i].mode,
+				     "mul", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (ssmul_optab, fixed_arith_modes[i].mode,
+				     "ssmul", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (usmul_optab, fixed_arith_modes[i].mode,
+				     "usmul", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (sdiv_optab, fixed_arith_modes[i].mode,
+				     "div", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (udiv_optab, fixed_arith_modes[i].mode,
+				     "udiv", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (ssdiv_optab, fixed_arith_modes[i].mode,
+				     "ssdiv", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (usdiv_optab, fixed_arith_modes[i].mode,
+				     "usdiv", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (neg_optab, fixed_arith_modes[i].mode,
+				     "neg", fixed_arith_modes[i].name, 2);
+	arm_set_fixed_optab_libfunc (ssneg_optab, fixed_arith_modes[i].mode,
+				     "ssneg", fixed_arith_modes[i].name, 2);
+	arm_set_fixed_optab_libfunc (usneg_optab, fixed_arith_modes[i].mode,
+				     "usneg", fixed_arith_modes[i].name, 2);
+	arm_set_fixed_optab_libfunc (ashl_optab, fixed_arith_modes[i].mode,
+				     "ashl", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (ashr_optab, fixed_arith_modes[i].mode,
+				     "ashr", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (lshr_optab, fixed_arith_modes[i].mode,
+				     "lshr", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (ssashl_optab, fixed_arith_modes[i].mode,
+				     "ssashl", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (usashl_optab, fixed_arith_modes[i].mode,
+				     "usashl", fixed_arith_modes[i].name, 3);
+	arm_set_fixed_optab_libfunc (cmp_optab, fixed_arith_modes[i].mode,
+				     "cmp", fixed_arith_modes[i].name, 2);
+      }
+
+    for (i = 0; i < ARRAY_SIZE (fixed_conv_modes); i++)
+      for (j = 0; j < ARRAY_SIZE (fixed_conv_modes); j++)
+	{
+	  if (i == j
+	      || (!ALL_FIXED_POINT_MODE_P (fixed_conv_modes[i].mode)
+		  && !ALL_FIXED_POINT_MODE_P (fixed_conv_modes[j].mode)))
+	    continue;
+
+	  arm_set_fixed_conv_libfunc (fract_optab, fixed_conv_modes[i].mode,
+				      fixed_conv_modes[j].mode, "fract",
+				      fixed_conv_modes[i].name,
+				      fixed_conv_modes[j].name);
+	  arm_set_fixed_conv_libfunc (satfract_optab,
+				      fixed_conv_modes[i].mode,
+				      fixed_conv_modes[j].mode, "satfract",
+				      fixed_conv_modes[i].name,
+				      fixed_conv_modes[j].name);
+	  arm_set_fixed_conv_libfunc (fractuns_optab,
+				      fixed_conv_modes[i].mode,
+				      fixed_conv_modes[j].mode, "fractuns",
+				      fixed_conv_modes[i].name,
+				      fixed_conv_modes[j].name);
+	  arm_set_fixed_conv_libfunc (satfractuns_optab,
+				      fixed_conv_modes[i].mode,
+				      fixed_conv_modes[j].mode, "satfractuns",
+				      fixed_conv_modes[i].name,
+				      fixed_conv_modes[j].name);
+	}
+  }
+
   if (TARGET_AAPCS_BASED)
     synchronize_libfunc = init_one_libfunc ("__sync_synchronize");
 }
@@ -1583,6 +1856,16 @@ arm_option_override (void)
   tune_flags = arm_selected_tune->flags;
   current_tune = arm_selected_tune->tune;
 
+  if (arm_tune == cortexa8 && optimize >= 3)
+    {
+      /* These alignments were experimentally determined to improve SPECint
+         performance on SPECCPU 2000.  */
+      if (align_functions <= 0)
+	align_functions = 16;
+      if (align_jumps <= 0)
+	align_jumps = 16;
+    }
+
   if (target_fp16_format_name)
     {
       for (i = 0; i < ARRAY_SIZE (all_fp16_formats); i++)
@@ -1702,7 +1985,8 @@ arm_option_override (void)
   arm_tune_wbuf = (tune_flags & FL_WBUF) != 0;
   arm_tune_xscale = (tune_flags & FL_XSCALE) != 0;
   arm_arch_iwmmxt = (insn_flags & FL_IWMMXT) != 0;
-  arm_arch_hwdiv = (insn_flags & FL_DIV) != 0;
+  arm_arch_thumb_hwdiv = (insn_flags & FL_THUMB_DIV) != 0;
+  arm_arch_arm_hwdiv = (insn_flags & FL_ARM_DIV) != 0;
   arm_tune_cortex_a9 = (arm_tune == cortexa9) != 0;
 
   /* If we are not using the default (ARM mode) section anchor offset
@@ -1895,6 +2179,17 @@ arm_option_override (void)
 	error ("invalid thread pointer option: -mtp=%s", target_thread_switch);
     }
 
+  if (target_tls_dialect_switch)
+    {
+      if (strcmp (target_tls_dialect_switch, "gnu") == 0)
+	target_tls_dialect = TLS_GNU;
+      else if (strcmp (target_tls_dialect_switch, "gnu2") == 0)
+	target_tls_dialect = TLS_GNU2;
+      else
+	error ("invalid thread dialect option: -mtls-dialect=%s",
+	       target_tls_dialect_switch);
+    }
+
   /* Use the cp15 method if it is available.  */
   if (target_thread_pointer == TP_AUTO)
     {
@@ -1969,6 +2264,32 @@ arm_option_override (void)
 	fix_cm3_ldrd = 0;
     }
 
+  /* Disable -fsched-interblock for Cortex-M4.  */
+  if (arm_selected_tune->core == cortexm4)
+    flag_schedule_interblock = 0;
+
+  /* Enable -munaligned-access by default for
+     - all ARMv6 architecture-based processors
+     - ARMv7-A, ARMv7-R, and ARMv7-M architecture-based processors.
+
+     Disable -munaligned-access by default for
+     - all pre-ARMv6 architecture-based processors
+     - ARMv6-M architecture-based processors.  */
+
+  if (unaligned_access == 2)
+    {
+      if (arm_arch6 && (arm_arch_notm || arm_arch7))
+	unaligned_access = 1;
+      else
+	unaligned_access = 0;
+    }
+  else if (unaligned_access == 1
+	   && !(arm_arch6 && (arm_arch_notm || arm_arch7)))
+    {
+      warning (0, "target CPU does not support unaligned accesses");
+      unaligned_access = 0;
+    }
+
   if (TARGET_THUMB1 && flag_schedule_insns)
     {
       /* Don't warn since it's on by default in -O2.  */
@@ -1982,11 +2303,27 @@ arm_option_override (void)
       max_insns_skipped = 6;
     }
   else
+    max_insns_skipped = current_tune->max_insns_skipped;
+
+  if (!optimize_size)
     {
-      /* StrongARM has early execution of branches, so a sequence
-         that is worth skipping is shorter.  */
-      if (arm_tune_strongarm)
-        max_insns_skipped = 3;
+      /* CSL LOCAL */
+      /* Loop unrolling can be a substantial win.  At -O2, limit to 2x
+	 unrolling by default to prevent excessive code growth; at -O3,
+	 limit to 4x unrolling by default.  */
+      if (current_tune->aggressive_unrolling)
+        {
+	  if (optimize == 2)
+	    maybe_set_param_value (PARAM_MAX_UNROLL_TIMES, 2,
+				   global_options.x_param_values,
+				   global_options_set.x_param_values);
+	  else if (optimize > 2)
+	    maybe_set_param_value (PARAM_MAX_UNROLL_TIMES, 4,
+				   global_options.x_param_values,
+				   global_options_set.x_param_values);
+	}
+      else if (flag_unroll_loops == 2)
+        flag_unroll_loops = 0;
     }
 
   /* Hot/Cold partitioning is not currently supported, since we can't
@@ -2039,6 +2376,38 @@ arm_option_override (void)
                            global_options.x_param_values,
                            global_options_set.x_param_values);
 
+  if (optimize_size)
+    {
+      /* Select optimizations that are a win for code size.
+
+	 The inlining options set below have two important
+	 consequences for functions not explicitly marked
+	 inline:
+	 - Static functions used once are inlined if
+	 sufficiently small.  Static functions used twice
+	 are not inlined.
+	 - Non-static functions are never inlined.
+	 So in effect, inlining will never cause two copies
+	 of function bodies to be created.
+	 Empirical results show that these options benefit code
+	 size on arm.  */
+      flag_move_loop_invariants = 0;
+      /* In Thumb mode the function call code size overhead is typically very
+	 small, and narrow branch instructions have very limited range.
+	 Inlining even medium sized functions tends to bloat the caller and
+	 require the use of long branch instructions. On average the long
+	 branches cost more than eliminating the function call overhead saves,
+	 so we use extremely restrictive automatic inlining heuristics.  In ARM
+	 mode the results are fairly neutral, probably due to better constant
+	 pool placement. */
+      maybe_set_param_value (PARAM_MAX_INLINE_INSNS_SINGLE, 1,
+			     global_options.x_param_values,
+			     global_options_set.x_param_values);
+      maybe_set_param_value (PARAM_MAX_INLINE_INSNS_AUTO, 1,
+			     global_options.x_param_values,
+			     global_options_set.x_param_values);
+    }
+
   /* Register global variables with the garbage collector.  */
   arm_add_gc_roots ();
 }
@@ -2171,6 +2540,14 @@ arm_allocate_stack_slots_for_args (void)
   return !IS_NAKED (arm_current_func_type ());
 }
 
+static bool
+arm_warn_func_result (void)
+{
+  /* Naked functions are implemented entirely in assembly, including the
+     return sequence, so suppress warnings about this.  */
+  return !IS_NAKED (arm_current_func_type ());
+}
+
 
 /* Output assembler code for a block containing the constant parts
    of a trampoline, leaving space for the variable parts.
@@ -2250,6 +2627,18 @@ arm_trampoline_adjust_address (rtx addr)
   return addr;
 }
 
+/* Return true if we should try to use a simple_return insn, i.e. perform
+   shrink-wrapping if possible.  This is the case if we need to emit a
+   prologue, which we can test by looking at the offsets.  */
+bool
+use_simple_return_p (void)
+{
+  arm_stack_offsets *offsets;
+
+  offsets = arm_get_frame_offsets ();
+  return offsets->outgoing_args != 0;
+}
+
 /* Return 1 if it is possible to return using a single instruction.
    If SIBLING is non-null, this is a test for a return before a sibling
    call.  SIBLING is the call insn, so we can examine its register usage.  */
@@ -4447,6 +4836,10 @@ aapcs_allocate_return_reg (enum machine_mode mode, const_tree type,
 rtx
 aapcs_libcall_value (enum machine_mode mode)
 {
+  if (BYTES_BIG_ENDIAN && ALL_FIXED_POINT_MODE_P (mode)
+      && GET_MODE_SIZE (mode) <= 4)
+    mode = SImode;
+
   return aapcs_allocate_return_reg (mode, NULL_TREE, NULL_TREE);
 }
 
@@ -5950,7 +6343,7 @@ thumb1_index_register_rtx_p (rtx x, int strict_p)
    addresses based on the frame pointer or arg pointer until the
    reload pass starts.  This is so that eliminating such addresses
    into stack based ones won't produce impossible code.  */
-static int
+int
 thumb1_legitimate_address_p (enum machine_mode mode, rtx x, int strict_p)
 {
   /* ??? Not clear if this is right.  Experiment.  */
@@ -6137,6 +6530,7 @@ arm_call_tls_get_addr (rtx x, rtx reg, rtx *valuep, int reloc)
 {
   rtx insns, label, labelno, sum;
 
+  gcc_assert (reloc != TLS_DESCSEQ);
   start_sequence ();
 
   labelno = GEN_INT (pic_labelno++);
@@ -6151,20 +6545,42 @@ arm_call_tls_get_addr (rtx x, rtx reg, rtx *valuep, int reloc)
 
   if (TARGET_ARM)
     emit_insn (gen_pic_add_dot_plus_eight (reg, reg, labelno));
-  else if (TARGET_THUMB2)
-    emit_insn (gen_pic_add_dot_plus_four (reg, reg, labelno));
-  else /* TARGET_THUMB1 */
+  else
     emit_insn (gen_pic_add_dot_plus_four (reg, reg, labelno));
-
-  *valuep = emit_library_call_value (get_tls_get_addr (), NULL_RTX, LCT_PURE, /* LCT_CONST?  */
+  
+  *valuep = emit_library_call_value (get_tls_get_addr (), NULL_RTX,
+				     LCT_PURE, /* LCT_CONST?  */
 				     Pmode, 1, reg, Pmode);
-
+  
   insns = get_insns ();
   end_sequence ();
 
   return insns;
 }
 
+static rtx
+arm_tls_descseq_addr (rtx x, rtx reg)
+{
+  rtx labelno = GEN_INT (pic_labelno++);
+  rtx label = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, labelno), UNSPEC_PIC_LABEL);
+  rtx sum = gen_rtx_UNSPEC (Pmode,
+			    gen_rtvec (4, x, GEN_INT (TLS_DESCSEQ),
+				       gen_rtx_CONST (VOIDmode, label),
+				       GEN_INT (!TARGET_ARM)),
+			    UNSPEC_TLS);
+  rtx reg0 = load_tls_operand (sum, gen_rtx_REG (SImode, 0));
+  
+  emit_insn (gen_tlscall (x, labelno));
+  if (!reg)
+    reg = gen_reg_rtx (SImode);
+  else
+    gcc_assert (REGNO (reg) != 0);
+
+  emit_move_insn (reg, reg0);
+
+  return reg;
+}
+
 rtx
 legitimize_tls_address (rtx x, rtx reg)
 {
@@ -6174,26 +6590,49 @@ legitimize_tls_address (rtx x, rtx reg)
   switch (model)
     {
     case TLS_MODEL_GLOBAL_DYNAMIC:
-      insns = arm_call_tls_get_addr (x, reg, &ret, TLS_GD32);
-      dest = gen_reg_rtx (Pmode);
-      emit_libcall_block (insns, dest, ret, x);
-      return dest;
+      if (TARGET_GNU2_TLS)
+	{
+	  reg = arm_tls_descseq_addr (x, reg);
 
-    case TLS_MODEL_LOCAL_DYNAMIC:
-      insns = arm_call_tls_get_addr (x, reg, &ret, TLS_LDM32);
+	  tp = arm_load_tp (NULL_RTX);
+	  
+	  return gen_rtx_PLUS (Pmode, tp, reg);
+	}
+      else
+	{
+	  insns = arm_call_tls_get_addr (x, reg, &ret, TLS_GD32);
+	  dest = gen_reg_rtx (Pmode);
+	  emit_libcall_block (insns, dest, ret, x);
+	  return dest;
+	}
 
-      /* Attach a unique REG_EQUIV, to allow the RTL optimizers to
-	 share the LDM result with other LD model accesses.  */
-      eqv = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, const1_rtx),
-			    UNSPEC_TLS);
-      dest = gen_reg_rtx (Pmode);
-      emit_libcall_block (insns, dest, ret, eqv);
+    case TLS_MODEL_LOCAL_DYNAMIC:
+      if (TARGET_GNU2_TLS)
+	{
+	  reg = arm_tls_descseq_addr (x, reg);
 
-      /* Load the addend.  */
-      addend = gen_rtx_UNSPEC (Pmode, gen_rtvec (2, x, GEN_INT (TLS_LDO32)),
-			       UNSPEC_TLS);
-      addend = force_reg (SImode, gen_rtx_CONST (SImode, addend));
-      return gen_rtx_PLUS (Pmode, dest, addend);
+	  tp = arm_load_tp (NULL_RTX);
+	  
+	  return gen_rtx_PLUS (Pmode, tp, reg);
+	}
+      else
+	{
+	  insns = arm_call_tls_get_addr (x, reg, &ret, TLS_LDM32);
+	  
+	  /* Attach a unique REG_EQUIV, to allow the RTL optimizers to
+	     share the LDM result with other LD model accesses.  */
+	  eqv = gen_rtx_UNSPEC (Pmode, gen_rtvec (1, const1_rtx),
+				UNSPEC_TLS);
+	  dest = gen_reg_rtx (Pmode);
+	  emit_libcall_block (insns, dest, ret, eqv);
+	  
+	  /* Load the addend.  */
+	  addend = gen_rtx_UNSPEC (Pmode, gen_rtvec (2, x,
+						     GEN_INT (TLS_LDO32)),
+				   UNSPEC_TLS);
+	  addend = force_reg (SImode, gen_rtx_CONST (SImode, addend));
+	  return gen_rtx_PLUS (Pmode, dest, addend);
+	}
 
     case TLS_MODEL_INITIAL_EXEC:
       labelno = GEN_INT (pic_labelno++);
@@ -6441,10 +6880,17 @@ arm_legitimize_reload_address (rtx *p,
       HOST_WIDE_INT low, high;
 
       if (mode == DImode || (mode == DFmode && TARGET_SOFT_FLOAT))
-	low = ((val & 0xf) ^ 0x8) - 0x8;
+	{
+	  if (TARGET_NEON && (val & 0x3) != 0)
+	    return false;
+	  low = ((val & 0xf) ^ 0x8) - 0x8;
+	}
       else if (TARGET_MAVERICK && TARGET_HARD_FLOAT)
 	/* Need to be careful, -256 is not a valid offset.  */
 	low = val >= 0 ? (val & 0xff) : -((-val) & 0xff);
+      else if (TARGET_REALLY_IWMMXT && mode == SImode)
+	/* Need to be careful, -1024 is not a valid offset.  */
+	low = val >= 0 ? (val & 0x3ff) : -((-val) & 0x3ff);
       else if (mode == SImode
 	       || (mode == SFmode && TARGET_SOFT_FLOAT)
 	       || ((mode == HImode || mode == QImode) && ! arm_arch4))
@@ -7267,6 +7713,17 @@ arm_rtx_costs_1 (rtx x, enum rtx_code outer, int* total, bool speed)
 	*total = COSTS_N_INSNS (4);
       return true;
 
+    case CONST_VECTOR:
+      if (TARGET_NEON
+	  && TARGET_HARD_FLOAT
+	  && outer == SET
+	  && (VALID_NEON_DREG_MODE (mode) || VALID_NEON_QREG_MODE (mode))
+	  && neon_immediate_valid_for_move (x, mode, NULL, NULL))
+	*total = COSTS_N_INSNS (1);
+      else
+	*total = COSTS_N_INSNS (4);
+      return true;
+
     case UNSPEC:
       /* We cost this as high as our memory costs to allow this to
 	 be hoisted from loops.  */
@@ -7564,72 +8021,446 @@ arm_size_rtx_costs (rtx x, enum rtx_code code, enum rtx_code outer_code,
       return false;
 
     case IF_THEN_ELSE:
-      *total = 0;
+      *total = 0;
+      return false;
+
+    case COMPARE:
+      if (cc_register (XEXP (x, 0), VOIDmode))
+	* total = 0;
+      else
+	*total = COSTS_N_INSNS (1);
+      return false;
+
+    case ABS:
+      if (TARGET_HARD_FLOAT && GET_MODE_CLASS (mode) == MODE_FLOAT
+	  && (mode == SFmode || !TARGET_VFP_SINGLE))
+	*total = COSTS_N_INSNS (1);
+      else
+	*total = COSTS_N_INSNS (1 + ARM_NUM_REGS (mode));
+      return false;
+
+    case SIGN_EXTEND:
+    case ZERO_EXTEND:
+      return arm_rtx_costs_1 (x, outer_code, total, 0);
+
+    case CONST_INT:
+      if (const_ok_for_arm (INTVAL (x)))
+	/* A multiplication by a constant requires another instruction
+	   to load the constant to a register.  */
+	*total = COSTS_N_INSNS ((outer_code == SET || outer_code == MULT)
+				? 1 : 0);
+      else if (const_ok_for_arm (~INTVAL (x)))
+	*total = COSTS_N_INSNS (outer_code == AND ? 0 : 1);
+      else if (const_ok_for_arm (-INTVAL (x)))
+	{
+	  if (outer_code == COMPARE || outer_code == PLUS
+	      || outer_code == MINUS)
+	    *total = 0;
+	  else
+	    *total = COSTS_N_INSNS (1);
+	}
+      else
+	*total = COSTS_N_INSNS (2);
+      return true;
+
+    case CONST:
+    case LABEL_REF:
+    case SYMBOL_REF:
+      *total = COSTS_N_INSNS (2);
+      return true;
+
+    case CONST_DOUBLE:
+      *total = COSTS_N_INSNS (4);
+      return true;
+
+    case CONST_VECTOR:
+      if (TARGET_NEON
+	  && TARGET_HARD_FLOAT
+	  && outer_code == SET
+	  && (VALID_NEON_DREG_MODE (mode) || VALID_NEON_QREG_MODE (mode))
+	  && neon_immediate_valid_for_move (x, mode, NULL, NULL))
+	*total = COSTS_N_INSNS (1);
+      else
+	*total = COSTS_N_INSNS (4);
+      return true;
+
+    case HIGH:
+    case LO_SUM:
+      /* We prefer constant pool entries to MOVW/MOVT pairs, so bump the
+	 cost of these slightly.  */
+      *total = COSTS_N_INSNS (1) + 1;
+      return true;
+
+    default:
+      if (mode != VOIDmode)
+	*total = COSTS_N_INSNS (ARM_NUM_REGS (mode));
+      else
+	*total = COSTS_N_INSNS (4); /* How knows?  */
+      return false;
+    }
+}
+
+static bool
+thumb2_size_rtx_costs (rtx x, enum rtx_code code, enum rtx_code outer_code,
+		     int *total)
+{
+  /* Attempt to give a lower cost to RTXs which can optimistically be
+     represented as short insns, assuming that the right conditions will hold
+     later (e.g. low registers will be chosen if a short insn requires them).
+
+     Note that we don't make wide insns cost twice as much as narrow insns,
+     because we can't prove that a particular RTX will actually use a narrow
+     insn, because not enough information is available (e.g., we don't know
+     which hard registers pseudos will be assigned).  Consider these to be
+     "expected" sizes/weightings.
+
+     (COSTS_NARROW_INSNS has the same weight as COSTS_N_INSNS.)  */
+
+#define COSTS_NARROW_INSNS(N) ((N) * 4)
+#define COSTS_WIDE_INSNS(N) ((N) * 6)
+#define THUMB2_LIBCALL_COST COSTS_WIDE_INSNS (2)
+  enum machine_mode mode = GET_MODE (x);
+
+  switch (code)
+    {
+    case MEM:
+      if (REG_P (XEXP (x, 0)))
+	{
+	  /* Hopefully this will use a narrow ldm/stm insn.  */
+	  *total = COSTS_NARROW_INSNS (1);
+	  return true;
+	}
+      else if ((GET_CODE (XEXP (x, 0)) == SYMBOL_REF
+		&& CONSTANT_POOL_ADDRESS_P (XEXP (x, 0)))
+	       || reg_mentioned_p (virtual_stack_vars_rtx, XEXP (x, 0))
+	       || reg_mentioned_p (stack_pointer_rtx, XEXP (x, 0)))
+	{
+	  *total = COSTS_NARROW_INSNS (ARM_NUM_REGS (mode));
+	  return true;
+	}
+      else if (GET_CODE (XEXP (x, 0)) == PLUS)
+	{
+	  rtx plus = XEXP (x, 0);
+
+	  if (GET_CODE (XEXP (plus, 1)) == CONST_INT)
+	    {
+	      HOST_WIDE_INT cst = INTVAL (XEXP (plus, 1));
+
+	      if (cst >= 0 && cst < 256)
+		*total = COSTS_NARROW_INSNS (ARM_NUM_REGS (mode));
+	      else
+		*total = COSTS_WIDE_INSNS (ARM_NUM_REGS (mode));
+
+	      *total += rtx_cost (XEXP (plus, 0), code, false);
+
+	      return true;
+	    }
+	}
+
+      *total = COSTS_NARROW_INSNS (ARM_NUM_REGS (mode));
+      return false;
+
+    case DIV:
+    case MOD:
+    case UDIV:
+    case UMOD:
+      if (TARGET_IDIV)
+	*total = COSTS_WIDE_INSNS (1);
+      else
+	*total = THUMB2_LIBCALL_COST;
+      return false;
+
+    case ROTATE:
+      if (mode == SImode && REG_P (XEXP (x, 1)))
+	{
+	  *total = COSTS_WIDE_INSNS (1) + COSTS_NARROW_INSNS (1)
+		   + rtx_cost (XEXP (x, 0), code, false);
+	  return true;
+	}
+      /* Fall through */
+
+    case ASHIFT:
+    case LSHIFTRT:
+    case ASHIFTRT:
+      if (mode == DImode && GET_CODE (XEXP (x, 1)) == CONST_INT)
+	{
+	  *total = COSTS_WIDE_INSNS (3) + rtx_cost (XEXP (x, 0), code, false);
+	  return true;
+	}
+      else if (mode == SImode)
+	{
+	  *total = COSTS_NARROW_INSNS (1);
+	  return false;
+	}
+
+      /* Needs a libcall.  */
+      *total = THUMB2_LIBCALL_COST;
+      return false;
+
+    case ROTATERT:
+      if (mode == DImode && GET_CODE (XEXP (x, 1)) == CONST_INT)
+	{
+	  *total = COSTS_WIDE_INSNS (3) + rtx_cost (XEXP (x, 0), code, false);
+	  return true;
+	}
+      else if (mode == SImode)
+	{
+	  if (GET_CODE (XEXP (x, 1)) == CONST_INT)
+	    *total = COSTS_WIDE_INSNS (1) + rtx_cost (XEXP (x, 0), code, false);
+	  else
+	    *total = COSTS_NARROW_INSNS (1)
+		     + rtx_cost (XEXP (x, 0), code, false);
+	  return true;
+	}
+
+      /* Needs a libcall.  */
+      *total = THUMB2_LIBCALL_COST;
+      return false;
+
+    case MINUS:
+      if (TARGET_HARD_FLOAT && GET_MODE_CLASS (mode) == MODE_FLOAT
+	  && (mode == SFmode || !TARGET_VFP_SINGLE))
+	{
+	  *total = COSTS_WIDE_INSNS (1);
+	  return false;
+	}
+
+      if (mode == SImode)
+	{
+	  enum rtx_code subcode0 = GET_CODE (XEXP (x, 0));
+	  enum rtx_code subcode1 = GET_CODE (XEXP (x, 1));
+
+	  if (subcode0 == ROTATE || subcode0 == ROTATERT || subcode0 == ASHIFT
+	      || subcode0 == LSHIFTRT || subcode0 == ASHIFTRT
+	      || subcode1 == ROTATE || subcode1 == ROTATERT
+	      || subcode1 == ASHIFT || subcode1 == LSHIFTRT
+	      || subcode1 == ASHIFTRT)
+	    {
+	      /* It's just the cost of the two operands.  */
+	      *total = 0;
+	      return false;
+	    }
+
+	  if (subcode1 == CONST_INT)
+	    {
+	      HOST_WIDE_INT cst = INTVAL (XEXP (x, 1));
+
+	      if (cst >= 0 && cst < 256)
+		*total = COSTS_NARROW_INSNS (1);
+	      else
+		*total = COSTS_WIDE_INSNS (1);
+
+	      *total += rtx_cost (XEXP (x, 0), code, false);
+
+	      return true;
+	    }
+
+	  *total = COSTS_NARROW_INSNS (1);
+	  return false;
+	}
+
+      *total = COSTS_WIDE_INSNS (ARM_NUM_REGS (mode));
+      return false;
+
+    case PLUS:
+      if (TARGET_HARD_FLOAT && GET_MODE_CLASS (mode) == MODE_FLOAT
+	  && (mode == SFmode || !TARGET_VFP_SINGLE))
+	{
+	  *total = COSTS_WIDE_INSNS (1);
+	  return false;
+	}
+
+      /* Fall through */
+    case AND: case XOR: case IOR:
+      if (mode == SImode)
+	{
+	  enum rtx_code subcode = GET_CODE (XEXP (x, 0));
+
+	  if (subcode == ROTATE || subcode == ROTATERT || subcode == ASHIFT
+	      || subcode == LSHIFTRT || subcode == ASHIFTRT
+	      || (code == AND && subcode == NOT))
+	    {
+	      /* It's just the cost of the two operands.  */
+	      *total = 0;
+	      return false;
+	    }
+
+	  if (code == PLUS && GET_CODE (XEXP (x, 1)) == CONST_INT)
+	    {
+	      HOST_WIDE_INT cst = INTVAL (XEXP (x, 1));
+
+	      if ((reg_mentioned_p (virtual_stack_vars_rtx, XEXP (x, 0))
+		   || reg_mentioned_p (stack_pointer_rtx, XEXP (x, 0)))
+		  && cst > -512 && cst < 1024)
+		/* Only approximately correct, depending on destination
+		   register.  */
+		*total = COSTS_NARROW_INSNS (1);
+	      else if (cst > -256 && cst < 256)
+		*total = COSTS_NARROW_INSNS (1);
+	      else
+		*total = COSTS_WIDE_INSNS (1);
+
+	      *total += rtx_cost (XEXP (x, 0), code, false);
+
+	      return true;
+	    }
+
+	  if (subcode == MULT
+	      && power_of_two_operand (XEXP (XEXP (x, 0), 1), mode))
+	    {
+	      *total = COSTS_WIDE_INSNS (1)
+		       + rtx_cost (XEXP (x, 1), code, false);
+	      return true;
+	    }
+	}
+
+      *total = COSTS_NARROW_INSNS (ARM_NUM_REGS (mode));
+      return false;
+
+    case MULT:
+      if (mode == SImode && GET_CODE (XEXP (x, 1)) != CONST_INT)
+	{
+	  /* Might be using muls.  */
+	  *total = COSTS_NARROW_INSNS (1);
+	  return false;
+	}
+      *total = COSTS_WIDE_INSNS (ARM_NUM_REGS (mode));
+      return false;
+
+    case NEG:
+      if (TARGET_HARD_FLOAT && GET_MODE_CLASS (mode) == MODE_FLOAT
+	  && (mode == SFmode || !TARGET_VFP_SINGLE))
+	{
+	  *total = COSTS_WIDE_INSNS (1);
+	  return false;
+	}
+
+      /* Fall through */
+    case NOT:
+      if (mode == SImode)
+	{
+	  *total = COSTS_NARROW_INSNS (1);
+	  return false;
+	}
+      *total = COSTS_WIDE_INSNS (ARM_NUM_REGS (mode));
+      return false;
+
+    case IF_THEN_ELSE:
+      *total = COSTS_NARROW_INSNS (1);
       return false;
 
     case COMPARE:
       if (cc_register (XEXP (x, 0), VOIDmode))
-	* total = 0;
+	*total = 0;
       else
-	*total = COSTS_N_INSNS (1);
+	*total = COSTS_NARROW_INSNS (1);
       return false;
 
     case ABS:
       if (TARGET_HARD_FLOAT && GET_MODE_CLASS (mode) == MODE_FLOAT
 	  && (mode == SFmode || !TARGET_VFP_SINGLE))
-	*total = COSTS_N_INSNS (1);
+	*total = COSTS_WIDE_INSNS (1);
       else
-	*total = COSTS_N_INSNS (1 + ARM_NUM_REGS (mode));
+	*total = COSTS_NARROW_INSNS (ARM_NUM_REGS (mode)) * 2;
       return false;
 
     case SIGN_EXTEND:
+      if (GET_MODE_SIZE (mode) <= 4)
+	*total = GET_CODE (XEXP (x, 0)) == MEM ? 0 : COSTS_NARROW_INSNS (1);
+      else
+	*total = COSTS_NARROW_INSNS (1)
+		 + COSTS_WIDE_INSNS (ARM_NUM_REGS (mode));
+      return false;
+
     case ZERO_EXTEND:
-      return arm_rtx_costs_1 (x, outer_code, total, 0);
+      if (GET_MODE_SIZE (mode) > 4)
+	*total = COSTS_WIDE_INSNS (ARM_NUM_REGS (mode) - 1);
+      else if (GET_CODE (XEXP (x, 0)) == MEM)
+	*total = 0;
+      else
+	*total = COSTS_NARROW_INSNS (1);
+      return false;
 
     case CONST_INT:
-      if (const_ok_for_arm (INTVAL (x)))
-	/* A multiplication by a constant requires another instruction
-	   to load the constant to a register.  */
-	*total = COSTS_N_INSNS ((outer_code == SET || outer_code == MULT)
-				? 1 : 0);
-      else if (const_ok_for_arm (~INTVAL (x)))
-	*total = COSTS_N_INSNS (outer_code == AND ? 0 : 1);
-      else if (const_ok_for_arm (-INTVAL (x)))
-	{
-	  if (outer_code == COMPARE || outer_code == PLUS
-	      || outer_code == MINUS)
+      {
+	HOST_WIDE_INT cst = INTVAL (x);
+
+	switch (outer_code)
+	  {
+	  case PLUS:
+	    if (cst > -256 && cst < 256)
+	      *total = 0;
+	    else
+	      /* See note about optabs below.  */
+	      *total = COSTS_N_INSNS (1);
+	    return true;
+
+	  case MINUS:
+	  case COMPARE:
+	    if (cst >= 0 && cst < 256)
+	      *total = 0;
+	    else
+	      /* See note about optabs below.  */
+	      *total = COSTS_N_INSNS (1);
+	    return true;
+
+	  case ASHIFT:
+	  case ASHIFTRT:
+	  case LSHIFTRT:
 	    *total = 0;
-	  else
+	    return true;
+
+	  default:
+	    /* Constants are compared explicitly against COSTS_N_INSNS (1) in
+	       optabs.c, creating an alternative, larger code sequence for more
+	       expensive constants).  So, it doesn't pay to make some constants
+	       cost more than this.  */
 	    *total = COSTS_N_INSNS (1);
-	}
-      else
-	*total = COSTS_N_INSNS (2);
-      return true;
+	  }
+	return true;
+      }
 
     case CONST:
     case LABEL_REF:
     case SYMBOL_REF:
-      *total = COSTS_N_INSNS (2);
+      *total = COSTS_WIDE_INSNS (2);
       return true;
 
     case CONST_DOUBLE:
-      *total = COSTS_N_INSNS (4);
+      *total = COSTS_WIDE_INSNS (4);
+      return true;
+
+    case CONST_VECTOR:
+      if (TARGET_NEON
+	  && TARGET_HARD_FLOAT
+	  && outer_code == SET
+	  && (VALID_NEON_DREG_MODE (mode) || VALID_NEON_QREG_MODE (mode))
+	  && neon_immediate_valid_for_move (x, mode, NULL, NULL))
+	*total = COSTS_WIDE_INSNS (1);
+      else
+	*total = COSTS_WIDE_INSNS (4);
       return true;
 
     case HIGH:
     case LO_SUM:
       /* We prefer constant pool entries to MOVW/MOVT pairs, so bump the
-	 cost of these slightly.  */
-      *total = COSTS_N_INSNS (1) + 1;
+       cost of these slightly.  */
+      *total = COSTS_WIDE_INSNS (1) + 1;
       return true;
 
     default:
       if (mode != VOIDmode)
-	*total = COSTS_N_INSNS (ARM_NUM_REGS (mode));
+	*total = COSTS_WIDE_INSNS (ARM_NUM_REGS (mode));
       else
-	*total = COSTS_N_INSNS (4); /* How knows?  */
+	/* A guess (inherited from arm_size_rtx_costs).  */
+	*total = COSTS_WIDE_INSNS (4);
       return false;
     }
+
+  return true;
+#undef THUMB2_LIBCALL_COST
+#undef COSTS_WIDE_INSNS
+#undef COSTS_NARROW_INSNS
 }
 
 /* RTX costs when optimizing for size.  */
@@ -7638,8 +8469,14 @@ arm_rtx_costs (rtx x, int code, int outer_code, int *total,
 	       bool speed)
 {
   if (!speed)
-    return arm_size_rtx_costs (x, (enum rtx_code) code,
-			       (enum rtx_code) outer_code, total);
+    {
+      if (TARGET_THUMB2)
+	return thumb2_size_rtx_costs (x, (enum rtx_code) code,
+				      (enum rtx_code) outer_code, total);
+      else
+	return arm_size_rtx_costs (x, (enum rtx_code) code,
+				   (enum rtx_code) outer_code, total);
+    }
   else
     return current_tune->rtx_costs (x, (enum rtx_code) code,
 				    (enum rtx_code) outer_code,
@@ -8203,6 +9040,21 @@ arm_adjust_cost (rtx insn, rtx link, rtx dep, int cost)
   return cost;
 }
 
+static int
+arm_default_branch_cost (bool speed_p, bool predictable_p ATTRIBUTE_UNUSED)
+{
+  if (TARGET_32BIT)
+    return (TARGET_THUMB2 && !speed_p) ? 1 : 4;
+  else
+    return (optimize > 0) ? 2 : 0;
+}
+
+static int
+arm_cortex_a5_branch_cost (bool speed_p, bool predictable_p)
+{
+  return speed_p ? 0 : arm_default_branch_cost (speed_p, predictable_p);
+}
+
 static int fp_consts_inited = 0;
 
 /* Only zero is valid for VFP.  Other values are also valid for FPA.  */
@@ -8408,11 +9260,14 @@ vfp3_const_double_rtx (rtx x)
    vmov  i64    17    aaaaaaaa bbbbbbbb cccccccc dddddddd
                       eeeeeeee ffffffff gggggggg hhhhhhhh
    vmov  f32    18    aBbbbbbc defgh000 00000000 00000000
+   vmov  f32    19    00000000 00000000 00000000 00000000
 
    For case 18, B = !b. Representable values are exactly those accepted by
    vfp3_const_double_index, but are output as floating-point numbers rather
    than indices.
 
+   For case 19, we will change it to vmov.i32 when assembling.
+
    Variants 0-5 (inclusive) may also be used as immediates for the second
    operand of VORR/VBIC instructions.
 
@@ -8455,7 +9310,7 @@ neon_valid_immediate (rtx op, enum machine_mode mode, int inverse,
       rtx el0 = CONST_VECTOR_ELT (op, 0);
       REAL_VALUE_TYPE r0;
 
-      if (!vfp3_const_double_rtx (el0))
+      if (!vfp3_const_double_rtx (el0) && el0 != CONST0_RTX (GET_MODE (el0)))
         return -1;
 
       REAL_VALUE_FROM_CONST_DOUBLE (r0, el0);
@@ -8477,7 +9332,10 @@ neon_valid_immediate (rtx op, enum machine_mode mode, int inverse,
       if (elementwidth)
         *elementwidth = 0;
 
-      return 18;
+      if (el0 == CONST0_RTX (GET_MODE (el0)))
+	return 19;
+      else
+	return 18;
     }
 
   /* Splat vector constant out into a byte vector.  */
@@ -9154,6 +10012,11 @@ neon_struct_mem_operand (rtx op)
   if (GET_CODE (ind) == REG)
     return arm_address_register_rtx_p (ind, 0);
 
+  /* vldm/vstm allows POST_INC (ia) and PRE_DEC (db).  */
+  if (GET_CODE (ind) == POST_INC
+      || GET_CODE (ind) == PRE_DEC)
+    return arm_address_register_rtx_p (XEXP (ind, 0), 0);
+
   return FALSE;
 }
 
@@ -9202,8 +10065,9 @@ arm_return_in_msb (const_tree valtype)
 {
   return (TARGET_AAPCS_BASED
           && BYTES_BIG_ENDIAN
-          && (AGGREGATE_TYPE_P (valtype)
-              || TREE_CODE (valtype) == COMPLEX_TYPE));
+	  && (AGGREGATE_TYPE_P (valtype)
+	      || TREE_CODE (valtype) == COMPLEX_TYPE
+	      || FIXED_POINT_TYPE_P (valtype)));
 }
 
 /* Returns TRUE if INSN is an "LDR REG, ADDR" instruction.
@@ -9478,6 +10342,11 @@ arm_note_pic_base (rtx *x, void *date ATTRIBUTE_UNUSED)
 static bool
 arm_cannot_copy_insn_p (rtx insn)
 {
+  /* The tls call insn cannot be copied, as it is paired with a data
+     word.  */
+  if (recog_memoized (insn) == CODE_FOR_tlscall)
+    return true;
+  
   return for_each_rtx (&PATTERN (insn), arm_note_pic_base, NULL);
 }
 
@@ -10380,6 +11249,335 @@ gen_const_stm_seq (rtx *operands, int nops)
   return true;
 }
 
+/* Copy a block of memory using plain ldr/str/ldrh/strh instructions, to permit
+   unaligned copies on processors which support unaligned semantics for those
+   instructions.  INTERLEAVE_FACTOR can be used to attempt to hide load latency
+   (using more registers) by doing e.g. load/load/store/store for a factor of 2.
+   An interleave factor of 1 (the minimum) will perform no interleaving. 
+   Load/store multiple are used for aligned addresses where possible.  */
+
+static void
+arm_block_move_unaligned_straight (rtx dstbase, rtx srcbase,
+				   HOST_WIDE_INT length,
+				   unsigned int interleave_factor)
+{
+  rtx *regs = XALLOCAVEC (rtx, interleave_factor);
+  int *regnos = XALLOCAVEC (int, interleave_factor);
+  HOST_WIDE_INT block_size_bytes = interleave_factor * UNITS_PER_WORD;
+  HOST_WIDE_INT i, j;
+  HOST_WIDE_INT remaining = length, words;
+  rtx halfword_tmp = NULL, byte_tmp = NULL;
+  rtx dst, src;
+  bool src_aligned = MEM_ALIGN (srcbase) >= BITS_PER_WORD;
+  bool dst_aligned = MEM_ALIGN (dstbase) >= BITS_PER_WORD;
+  HOST_WIDE_INT srcoffset, dstoffset;
+  HOST_WIDE_INT src_autoinc, dst_autoinc;
+  rtx mem, addr;
+  
+  gcc_assert (1 <= interleave_factor && interleave_factor <= 4);
+  
+  /* Use hard registers if we have aligned source or destination so we can use
+     load/store multiple with contiguous registers.  */
+  if (dst_aligned || src_aligned)
+    for (i = 0; i < interleave_factor; i++)
+      regs[i] = gen_rtx_REG (SImode, i);
+  else
+    for (i = 0; i < interleave_factor; i++)
+      regs[i] = gen_reg_rtx (SImode);
+
+  dst = copy_addr_to_reg (XEXP (dstbase, 0));
+  src = copy_addr_to_reg (XEXP (srcbase, 0));
+
+  srcoffset = dstoffset = 0;
+  
+  /* Calls to arm_gen_load_multiple and arm_gen_store_multiple update SRC/DST.
+     For copying the last bytes we want to subtract this offset again.  */
+  src_autoinc = dst_autoinc = 0;
+
+  for (i = 0; i < interleave_factor; i++)
+    regnos[i] = i;
+
+  /* Copy BLOCK_SIZE_BYTES chunks.  */
+
+  for (i = 0; i + block_size_bytes <= length; i += block_size_bytes)
+    {
+      /* Load words.  */
+      if (src_aligned && interleave_factor > 1)
+        {
+	  emit_insn (arm_gen_load_multiple (regnos, interleave_factor, src,
+					    TRUE, srcbase, &srcoffset));
+	  src_autoinc += UNITS_PER_WORD * interleave_factor;
+	}
+      else
+        {
+	  for (j = 0; j < interleave_factor; j++)
+	    {
+	      addr = plus_constant (src, srcoffset + j * UNITS_PER_WORD
+					 - src_autoinc);
+	      mem = adjust_automodify_address (srcbase, SImode, addr,
+					       srcoffset + j * UNITS_PER_WORD);
+	      emit_insn (gen_unaligned_loadsi (regs[j], mem));
+	    }
+	  srcoffset += block_size_bytes;
+	}
+
+      /* Store words.  */
+      if (dst_aligned && interleave_factor > 1)
+        {
+          emit_insn (arm_gen_store_multiple (regnos, interleave_factor, dst,
+					     TRUE, dstbase, &dstoffset));
+	  dst_autoinc += UNITS_PER_WORD * interleave_factor;
+	}
+      else
+        {
+	  for (j = 0; j < interleave_factor; j++)
+	    {
+	      addr = plus_constant (dst, dstoffset + j * UNITS_PER_WORD
+					 - dst_autoinc);
+	      mem = adjust_automodify_address (dstbase, SImode, addr,
+					       dstoffset + j * UNITS_PER_WORD);
+	      emit_insn (gen_unaligned_storesi (mem, regs[j]));
+	    }
+	  dstoffset += block_size_bytes;
+	}
+
+      remaining -= block_size_bytes;
+    }
+  
+  /* Copy any whole words left (note these aren't interleaved with any
+     subsequent halfword/byte load/stores in the interests of simplicity).  */
+  
+  words = remaining / UNITS_PER_WORD;
+
+  gcc_assert (words < interleave_factor);
+  
+  if (src_aligned && words > 1)
+    {
+      emit_insn (arm_gen_load_multiple (regnos, words, src, TRUE, srcbase,
+					&srcoffset));
+      src_autoinc += UNITS_PER_WORD * words;
+    }
+  else
+    {
+      for (j = 0; j < words; j++)
+        {
+	  addr = plus_constant (src,
+				srcoffset + j * UNITS_PER_WORD - src_autoinc);
+	  mem = adjust_automodify_address (srcbase, SImode, addr,
+					   srcoffset + j * UNITS_PER_WORD);
+	  emit_insn (gen_unaligned_loadsi (regs[j], mem));
+	}
+      srcoffset += words * UNITS_PER_WORD;
+    }
+
+  if (dst_aligned && words > 1)
+    {
+      emit_insn (arm_gen_store_multiple (regnos, words, dst, TRUE, dstbase,
+					 &dstoffset));
+      dst_autoinc += words * UNITS_PER_WORD;
+    }
+  else
+    {
+      for (j = 0; j < words; j++)
+        {
+	  addr = plus_constant (dst,
+				dstoffset + j * UNITS_PER_WORD - dst_autoinc);
+	  mem = adjust_automodify_address (dstbase, SImode, addr,
+					   dstoffset + j * UNITS_PER_WORD);
+	  emit_insn (gen_unaligned_storesi (mem, regs[j]));
+	}
+      dstoffset += words * UNITS_PER_WORD;
+    }
+
+  remaining -= words * UNITS_PER_WORD;
+  
+  gcc_assert (remaining < 4);
+  
+  /* Copy a halfword if necessary.  */
+  
+  if (remaining >= 2)
+    {
+      halfword_tmp = gen_reg_rtx (SImode);
+
+      addr = plus_constant (src, srcoffset - src_autoinc);
+      mem = adjust_automodify_address (srcbase, HImode, addr, srcoffset);
+      emit_insn (gen_unaligned_loadhiu (halfword_tmp, mem));
+
+      /* Either write out immediately, or delay until we've loaded the last
+	 byte, depending on interleave factor.  */
+      if (interleave_factor == 1)
+        {
+	  addr = plus_constant (dst, dstoffset - dst_autoinc);
+	  mem = adjust_automodify_address (dstbase, HImode, addr, dstoffset);
+	  emit_insn (gen_unaligned_storehi (mem,
+		       gen_lowpart (HImode, halfword_tmp)));
+	  halfword_tmp = NULL;
+	  dstoffset += 2;
+	}
+
+      remaining -= 2;
+      srcoffset += 2;
+    }
+  
+  gcc_assert (remaining < 2);
+  
+  /* Copy last byte.  */
+  
+  if ((remaining & 1) != 0)
+    {
+      byte_tmp = gen_reg_rtx (SImode);
+
+      addr = plus_constant (src, srcoffset - src_autoinc);
+      mem = adjust_automodify_address (srcbase, QImode, addr, srcoffset);
+      emit_move_insn (gen_lowpart (QImode, byte_tmp), mem);
+
+      if (interleave_factor == 1)
+        {
+	  addr = plus_constant (dst, dstoffset - dst_autoinc);
+	  mem = adjust_automodify_address (dstbase, QImode, addr, dstoffset);
+	  emit_move_insn (mem, gen_lowpart (QImode, byte_tmp));
+	  byte_tmp = NULL;
+	  dstoffset++;
+	}
+
+      remaining--;
+      srcoffset++;
+    }
+  
+  /* Store last halfword if we haven't done so already.  */
+  
+  if (halfword_tmp)
+    {
+      addr = plus_constant (dst, dstoffset - dst_autoinc);
+      mem = adjust_automodify_address (dstbase, HImode, addr, dstoffset);
+      emit_insn (gen_unaligned_storehi (mem,
+		   gen_lowpart (HImode, halfword_tmp)));
+      dstoffset += 2;
+    }
+
+  /* Likewise for last byte.  */
+
+  if (byte_tmp)
+    {
+      addr = plus_constant (dst, dstoffset - dst_autoinc);
+      mem = adjust_automodify_address (dstbase, QImode, addr, dstoffset);
+      emit_move_insn (mem, gen_lowpart (QImode, byte_tmp));
+      dstoffset++;
+    }
+  
+  gcc_assert (remaining == 0 && srcoffset == dstoffset);
+}
+
+/* From mips_adjust_block_mem:
+
+   Helper function for doing a loop-based block operation on memory
+   reference MEM.  Each iteration of the loop will operate on LENGTH
+   bytes of MEM.
+
+   Create a new base register for use within the loop and point it to
+   the start of MEM.  Create a new memory reference that uses this
+   register.  Store them in *LOOP_REG and *LOOP_MEM respectively.  */
+
+static void
+arm_adjust_block_mem (rtx mem, HOST_WIDE_INT length, rtx *loop_reg,
+		      rtx *loop_mem)
+{
+  *loop_reg = copy_addr_to_reg (XEXP (mem, 0));
+  
+  /* Although the new mem does not refer to a known location,
+     it does keep up to LENGTH bytes of alignment.  */
+  *loop_mem = change_address (mem, BLKmode, *loop_reg);
+  set_mem_align (*loop_mem, MIN (MEM_ALIGN (mem), length * BITS_PER_UNIT));
+}
+
+/* From mips_block_move_loop:
+
+   Move LENGTH bytes from SRC to DEST using a loop that moves BYTES_PER_ITER
+   bytes at a time.  LENGTH must be at least BYTES_PER_ITER.  Assume that
+   the memory regions do not overlap.  */
+
+static void
+arm_block_move_unaligned_loop (rtx dest, rtx src, HOST_WIDE_INT length,
+			       unsigned int interleave_factor,
+			       HOST_WIDE_INT bytes_per_iter)
+{
+  rtx label, src_reg, dest_reg, final_src, test;
+  HOST_WIDE_INT leftover;
+  
+  leftover = length % bytes_per_iter;
+  length -= leftover;
+  
+  /* Create registers and memory references for use within the loop.  */
+  arm_adjust_block_mem (src, bytes_per_iter, &src_reg, &src);
+  arm_adjust_block_mem (dest, bytes_per_iter, &dest_reg, &dest);
+  
+  /* Calculate the value that SRC_REG should have after the last iteration of
+     the loop.  */
+  final_src = expand_simple_binop (Pmode, PLUS, src_reg, GEN_INT (length),
+				   0, 0, OPTAB_WIDEN);
+
+  /* Emit the start of the loop.  */
+  label = gen_label_rtx ();
+  emit_label (label);
+  
+  /* Emit the loop body.  */
+  arm_block_move_unaligned_straight (dest, src, bytes_per_iter,
+				     interleave_factor);
+
+  /* Move on to the next block.  */
+  emit_move_insn (src_reg, plus_constant (src_reg, bytes_per_iter));
+  emit_move_insn (dest_reg, plus_constant (dest_reg, bytes_per_iter));
+  
+  /* Emit the loop condition.  */
+  test = gen_rtx_NE (VOIDmode, src_reg, final_src);
+  emit_jump_insn (gen_cbranchsi4 (test, src_reg, final_src, label));
+  
+  /* Mop up any left-over bytes.  */
+  if (leftover)
+    arm_block_move_unaligned_straight (dest, src, leftover, interleave_factor);
+}
+
+/* Emit a block move when either the source or destination is unaligned (not
+   aligned to a four-byte boundary).  This may need further tuning depending on
+   core type, optimize_size setting, etc.  */
+
+static int
+arm_movmemqi_unaligned (rtx *operands)
+{
+  HOST_WIDE_INT length = INTVAL (operands[2]);
+  
+  if (optimize_size)
+    {
+      bool src_aligned = MEM_ALIGN (operands[1]) >= BITS_PER_WORD;
+      bool dst_aligned = MEM_ALIGN (operands[0]) >= BITS_PER_WORD;
+      /* Inlined memcpy using ldr/str/ldrh/strh can be quite big: try to limit
+         size of code if optimizing for size.  We'll use ldm/stm if src_aligned
+	 or dst_aligned though: allow more interleaving in those cases since the
+	 resulting code can be smaller.  */
+      unsigned int interleave_factor = (src_aligned || dst_aligned) ? 2 : 1;
+      HOST_WIDE_INT bytes_per_iter = (src_aligned || dst_aligned) ? 8 : 4;
+      
+      if (length > 12)
+	arm_block_move_unaligned_loop (operands[0], operands[1], length,
+				       interleave_factor, bytes_per_iter);
+      else
+	arm_block_move_unaligned_straight (operands[0], operands[1], length,
+					   interleave_factor);
+    }
+  else
+    {
+      /* Note that the loop created by arm_block_move_unaligned_loop may be
+         subject to loop unrolling, which makes tuning this condition a little
+	 redundant.  */
+      if (length > 32)
+	arm_block_move_unaligned_loop (operands[0], operands[1], length, 4, 16);
+      else
+	arm_block_move_unaligned_straight (operands[0], operands[1], length, 4);
+    }
+  
+  return 1;
+}
+
 int
 arm_gen_movmemqi (rtx *operands)
 {
@@ -10392,8 +11590,13 @@ arm_gen_movmemqi (rtx *operands)
 
   if (GET_CODE (operands[2]) != CONST_INT
       || GET_CODE (operands[3]) != CONST_INT
-      || INTVAL (operands[2]) > 64
-      || INTVAL (operands[3]) & 3)
+      || INTVAL (operands[2]) > 64)
+    return 0;
+
+  if (unaligned_access && (INTVAL (operands[3]) & 3) != 0)
+    return arm_movmemqi_unaligned (operands);
+
+  if (INTVAL (operands[3]) & 3)
     return 0;
 
   dstbase = operands[0];
@@ -11204,7 +12407,7 @@ arm_must_pass_in_stack (enum machine_mode mode, const_tree type)
    aggregate types are placed in the lowest memory address.  */
 
 bool
-arm_pad_arg_upward (enum machine_mode mode, const_tree type)
+arm_pad_arg_upward (enum machine_mode mode ATTRIBUTE_UNUSED, const_tree type)
 {
   if (!TARGET_AAPCS_BASED)
     return DEFAULT_FUNCTION_ARG_PADDING(mode, type) == upward;
@@ -11217,21 +12420,33 @@ arm_pad_arg_upward (enum machine_mode mode, const_tree type)
 
 
 /* Similarly, for use by BLOCK_REG_PADDING (MODE, TYPE, FIRST).
-   For non-AAPCS, return !BYTES_BIG_ENDIAN if the least significant
-   byte of the register has useful data, and return the opposite if the
-   most significant byte does.
-   For AAPCS, small aggregates and small complex types are always padded
-   upwards.  */
+   Return !BYTES_BIG_ENDIAN if the least significant byte of the
+   register has useful data, and return the opposite if the most
+   significant byte does.  */
 
 bool
-arm_pad_reg_upward (enum machine_mode mode ATTRIBUTE_UNUSED,
+arm_pad_reg_upward (enum machine_mode mode,
                     tree type, int first ATTRIBUTE_UNUSED)
 {
-  if (TARGET_AAPCS_BASED
-      && BYTES_BIG_ENDIAN
-      && (AGGREGATE_TYPE_P (type) || TREE_CODE (type) == COMPLEX_TYPE)
-      && int_size_in_bytes (type) <= 4)
-    return true;
+  if (TARGET_AAPCS_BASED && BYTES_BIG_ENDIAN)
+    {
+      /* For AAPCS, small aggregates, small fixed-point types,
+	 and small complex types are always padded upwards.  */
+      if (type)
+	{
+	  if ((AGGREGATE_TYPE_P (type)
+	       || TREE_CODE (type) == COMPLEX_TYPE
+	       || FIXED_POINT_TYPE_P (type))
+	      && int_size_in_bytes (type) <= 4)
+	    return true;
+	}
+      else
+	{
+	  if ((COMPLEX_MODE_P (mode) || ALL_FIXED_POINT_MODE_P (mode))
+	      && GET_MODE_SIZE (mode) <= 4)
+	    return true;
+	}
+    }
 
   /* Otherwise, use default padding.  */
   return !BYTES_BIG_ENDIAN;
@@ -11430,6 +12645,7 @@ is_jump_table (rtx insn)
 
   if (GET_CODE (insn) == JUMP_INSN
       && JUMP_LABEL (insn) != NULL
+      && !ANY_RETURN_P (JUMP_LABEL (insn))
       && ((table = next_real_insn (JUMP_LABEL (insn)))
 	  == next_real_insn (insn))
       && table != NULL
@@ -12029,7 +13245,10 @@ create_fix_barrier (Mfix *fix, HOST_WIDE_INT max_address)
       gcc_assert (GET_CODE (from) != BARRIER);
 
       /* Count the length of this insn.  */
-      count += get_attr_length (from);
+      if (LABEL_P (from) && (align_jumps > 0 || align_loops > 0))
+        count += MAX (align_jumps, align_loops);
+      else
+        count += get_attr_length (from);
 
       /* If there is a jump table, add its length.  */
       tmp = is_jump_table (from);
@@ -12449,6 +13668,8 @@ arm_reorg (void)
 	      insn = table;
 	    }
 	}
+      else if (LABEL_P (insn) && (align_jumps > 0 || align_loops > 0))
+	address += MAX (align_jumps, align_loops);
     }
 
   fix = minipool_fix_head;
@@ -14266,7 +15487,7 @@ arm_get_vfp_saved_size (void)
 /* Generate a function exit sequence.  If REALLY_RETURN is false, then do
    everything bar the final return instruction.  */
 const char *
-output_return_instruction (rtx operand, int really_return, int reverse)
+output_return_instruction (rtx operand, bool really_return, bool reverse, bool simple)
 {
   char conditional[10];
   char instr[100];
@@ -14304,10 +15525,15 @@ output_return_instruction (rtx operand, int really_return, int reverse)
 
   sprintf (conditional, "%%?%%%c0", reverse ? 'D' : 'd');
 
-  cfun->machine->return_used_this_function = 1;
+  if (simple)
+    live_regs_mask = 0;
+  else
+    {
+      cfun->machine->return_used_this_function = 1;
 
-  offsets = arm_get_frame_offsets ();
-  live_regs_mask = offsets->saved_regs_mask;
+      offsets = arm_get_frame_offsets ();
+      live_regs_mask = offsets->saved_regs_mask;
+    }
 
   if (live_regs_mask)
     {
@@ -15464,7 +16690,10 @@ arm_get_frame_offsets (void)
   offsets->soft_frame = offsets->saved_regs + CALLER_INTERWORKING_SLOT_SIZE;
   /* A leaf function does not need any stack alignment if it has nothing
      on the stack.  */
-  if (leaf && frame_size == 0)
+  if (leaf && frame_size == 0
+      /* However if it calls alloca(), we have a dynamically allocated
+	 block of BIGGEST_ALIGNMENT on stack, so still do stack alignment.  */
+      && ! cfun->calls_alloca)
     {
       offsets->outgoing_args = offsets->soft_frame;
       offsets->locals_base = offsets->soft_frame;
@@ -17071,10 +18300,10 @@ arm_elf_asm_destructor (rtx symbol, int priority)
    decremented/zeroed by arm_asm_output_opcode as the insns are output.  */
 
 /* Returns the index of the ARM condition code string in
-   `arm_condition_codes'.  COMPARISON should be an rtx like
-   `(eq (...) (...))'.  */
-static enum arm_cond_code
-get_arm_condition_code (rtx comparison)
+   `arm_condition_codes', or ARM_NV if the comparison is invalid.
+   COMPARISON should be an rtx like `(eq (...) (...))'.  */
+enum arm_cond_code
+maybe_get_arm_condition_code (rtx comparison)
 {
   enum machine_mode mode = GET_MODE (XEXP (comparison, 0));
   enum arm_cond_code code;
@@ -17098,11 +18327,11 @@ get_arm_condition_code (rtx comparison)
     case CC_DLTUmode: code = ARM_CC;
 
     dominance:
-      gcc_assert (comp_code == EQ || comp_code == NE);
-
       if (comp_code == EQ)
 	return ARM_INVERSE_CONDITION_CODE (code);
-      return code;
+      if (comp_code == NE)
+	return code;
+      return ARM_NV;
 
     case CC_NOOVmode:
       switch (comp_code)
@@ -17111,7 +18340,7 @@ get_arm_condition_code (rtx comparison)
 	case EQ: return ARM_EQ;
 	case GE: return ARM_PL;
 	case LT: return ARM_MI;
-	default: gcc_unreachable ();
+	default: return ARM_NV;
 	}
 
     case CC_Zmode:
@@ -17119,7 +18348,7 @@ get_arm_condition_code (rtx comparison)
 	{
 	case NE: return ARM_NE;
 	case EQ: return ARM_EQ;
-	default: gcc_unreachable ();
+	default: return ARM_NV;
 	}
 
     case CC_Nmode:
@@ -17127,7 +18356,7 @@ get_arm_condition_code (rtx comparison)
 	{
 	case NE: return ARM_MI;
 	case EQ: return ARM_PL;
-	default: gcc_unreachable ();
+	default: return ARM_NV;
 	}
 
     case CCFPEmode:
@@ -17152,7 +18381,7 @@ get_arm_condition_code (rtx comparison)
 	  /* UNEQ and LTGT do not have a representation.  */
 	case UNEQ: /* Fall through.  */
 	case LTGT: /* Fall through.  */
-	default: gcc_unreachable ();
+	default: return ARM_NV;
 	}
 
     case CC_SWPmode:
@@ -17168,7 +18397,7 @@ get_arm_condition_code (rtx comparison)
 	case GTU: return ARM_CC;
 	case LEU: return ARM_CS;
 	case LTU: return ARM_HI;
-	default: gcc_unreachable ();
+	default: return ARM_NV;
 	}
 
     case CC_Cmode:
@@ -17176,7 +18405,7 @@ get_arm_condition_code (rtx comparison)
 	{
 	case LTU: return ARM_CS;
 	case GEU: return ARM_CC;
-	default: gcc_unreachable ();
+	default: return ARM_NV;
 	}
 
     case CC_CZmode:
@@ -17188,7 +18417,7 @@ get_arm_condition_code (rtx comparison)
 	case GTU: return ARM_HI;
 	case LEU: return ARM_LS;
 	case LTU: return ARM_CC;
-	default: gcc_unreachable ();
+	default: return ARM_NV;
 	}
 
     case CC_NCVmode:
@@ -17198,7 +18427,7 @@ get_arm_condition_code (rtx comparison)
 	case LT: return ARM_LT;
 	case GEU: return ARM_CS;
 	case LTU: return ARM_CC;
-	default: gcc_unreachable ();
+	default: return ARM_NV;
 	}
 
     case CCmode:
@@ -17214,13 +18443,22 @@ get_arm_condition_code (rtx comparison)
 	case GTU: return ARM_HI;
 	case LEU: return ARM_LS;
 	case LTU: return ARM_CC;
-	default: gcc_unreachable ();
+	default: return ARM_NV;
 	}
 
     default: gcc_unreachable ();
     }
 }
 
+/* Like maybe_get_arm_condition_code, but never return ARM_NV.  */
+static enum arm_cond_code
+get_arm_condition_code (rtx comparison)
+{
+  enum arm_cond_code code = maybe_get_arm_condition_code (comparison);
+  gcc_assert (code != ARM_NV);
+  return code;
+}
+
 /* Tell arm_asm_output_opcode to output IT blocks for conditionally executed
    instructions.  */
 void
@@ -17312,6 +18550,7 @@ arm_final_prescan_insn (rtx insn)
 
   /* If we start with a return insn, we only succeed if we find another one.  */
   int seeking_return = 0;
+  enum rtx_code return_code = UNKNOWN;
 
   /* START_INSN will hold the insn from where we start looking.  This is the
      first insn after the following code_label if REVERSE is true.  */
@@ -17350,7 +18589,7 @@ arm_final_prescan_insn (rtx insn)
 	  else
 	    return;
 	}
-      else if (GET_CODE (body) == RETURN)
+      else if (ANY_RETURN_P (body))
         {
 	  start_insn = next_nonnote_insn (start_insn);
 	  if (GET_CODE (start_insn) == BARRIER)
@@ -17361,6 +18600,7 @@ arm_final_prescan_insn (rtx insn)
 	    {
 	      reverse = TRUE;
 	      seeking_return = 1;
+	      return_code = GET_CODE (body);
 	    }
 	  else
 	    return;
@@ -17401,11 +18641,15 @@ arm_final_prescan_insn (rtx insn)
 	  label = XEXP (XEXP (SET_SRC (body), 2), 0);
 	  then_not_else = FALSE;
 	}
-      else if (GET_CODE (XEXP (SET_SRC (body), 1)) == RETURN)
-	seeking_return = 1;
-      else if (GET_CODE (XEXP (SET_SRC (body), 2)) == RETURN)
+      else if (ANY_RETURN_P (XEXP (SET_SRC (body), 1)))
+	{
+	  seeking_return = 1;
+	  return_code = GET_CODE (XEXP (SET_SRC (body), 1));
+	}
+      else if (ANY_RETURN_P (XEXP (SET_SRC (body), 2)))
         {
 	  seeking_return = 1;
+	  return_code = GET_CODE (XEXP (SET_SRC (body), 2));
 	  then_not_else = FALSE;
         }
       else
@@ -17506,8 +18750,7 @@ arm_final_prescan_insn (rtx insn)
 		       && !use_return_insn (TRUE, NULL)
 		       && !optimize_size)
 		fail = TRUE;
-	      else if (GET_CODE (scanbody) == RETURN
-		       && seeking_return)
+	      else if (GET_CODE (scanbody) == return_code)
 	        {
 		  arm_ccfsm_state = 2;
 		  succeed = TRUE;
@@ -17674,7 +18917,7 @@ arm_hard_regno_mode_ok (unsigned int regno, enum machine_mode mode)
 	return mode == SImode;
 
       if (IS_IWMMXT_REGNUM (regno))
-	return VALID_IWMMXT_REG_MODE (mode);
+	return VALID_IWMMXT_REG_MODE (mode) && mode != SImode;
     }
   
   /* We allow almost any value to be stored in the general registers.
@@ -19222,6 +20465,8 @@ arm_scalar_mode_supported_p (enum machine_mode mode)
 {
   if (mode == HFmode)
     return (arm_fp16_format != ARM_FP16_FORMAT_NONE);
+  else if (ALL_FIXED_POINT_MODE_P (mode))
+    return true;
   else
     return default_scalar_mode_supported_p (mode);
 }
@@ -21564,6 +22809,10 @@ arm_file_start (void)
 	val = 6;
       asm_fprintf (asm_out_file, "\t.eabi_attribute 30, %d\n", val);
 
+      /* Tag_CPU_unaligned_access.  */
+      asm_fprintf (asm_out_file, "\t.eabi_attribute 34, %d\n",
+		   unaligned_access);
+
       /* Tag_ABI_FP_16bit_format.  */
       if (arm_fp16_format)
 	asm_fprintf (asm_out_file, "\t.eabi_attribute 38, %d\n",
@@ -22304,6 +23553,11 @@ arm_vector_mode_supported_p (enum machine_mode mode)
 	  || (mode == V8QImode)))
     return true;
 
+  if (TARGET_INT_SIMD && (mode == V4UQQmode || mode == V4QQmode
+      || mode == V2UHQmode || mode == V2HQmode || mode == V2UHAmode
+      || mode == V2HAmode))
+    return true;
+
   return false;
 }
 
@@ -22350,15 +23604,18 @@ arm_preferred_simd_mode (enum machine_mode mode)
 }
 
 /* Implement TARGET_CLASS_LIKELY_SPILLED_P.
- 
-   We need to define this for LO_REGS on thumb.  Otherwise we can end up
+
+   We need to define this for LO_REGS on Thumb-1.  Otherwise we can end up
    using r0-r4 for function arguments, r7 for the stack frame and don't
-   have enough left over to do doubleword arithmetic.  */
+   have enough left over to do doubleword arithmetic.  For Thumb-2 all the
+   potentially problematic instructions accept high registers so this is not
+   necessary.  Care needs to be taken to avoid adding new Thumb-2 patterns
+   that require many low registers.  */
 
 static bool
 arm_class_likely_spilled_p (reg_class_t rclass)
 {
-  if ((TARGET_THUMB && rclass == LO_REGS)
+  if ((TARGET_THUMB1 && rclass == LO_REGS)
       || rclass  == CC_REG)
     return true;
 
@@ -22837,6 +24094,9 @@ arm_emit_tls_decoration (FILE *fp, rtx x)
     case TLS_LE32:
       fputs ("(tpoff)", fp);
       break;
+    case TLS_DESCSEQ:
+      fputs ("(tlsdesc)", fp);
+      break;
     default:
       gcc_unreachable ();
     }
@@ -22846,9 +24106,11 @@ arm_emit_tls_decoration (FILE *fp, rtx x)
     case TLS_GD32:
     case TLS_LDM32:
     case TLS_IE32:
+    case TLS_DESCSEQ:
       fputs (" + (. - ", fp);
       output_addr_const (fp, XVECEXP (x, 0, 2));
-      fputs (" - ", fp);
+      /* For DESCSEQ the 3rd operand encodes thumbness, and is added */
+      fputs (reloc == TLS_DESCSEQ ? " + " : " - ", fp);
       output_addr_const (fp, XVECEXP (x, 0, 3));
       fputc (')', fp);
       break;
@@ -23012,6 +24274,7 @@ arm_issue_rate (void)
     {
     case cortexr4:
     case cortexr4f:
+    case cortexr5:
     case cortexa5:
     case cortexa8:
     case cortexa9:
@@ -23559,10 +24822,13 @@ arm_expand_sync (enum machine_mode mode,
 static bool
 arm_vector_alignment_reachable (const_tree type, bool is_packed)
 {
-  /* Vectors which aren't in packed structures will not be less aligned than
-     the natural alignment of their element type, so this is safe.  */
+  /* NOTE: returning true here will unconditionally peel loop iterations so
+     that aligned accesses can be used.  This is undesirable when misaligned
+     accesses are available, particularly for small loop iteration counts,
+     since the overhead for dispatching to multiple versions of the loop is
+     quite high.  */
   if (TARGET_NEON && !BYTES_BIG_ENDIAN)
-    return !is_packed;
+    return false;
 
   return default_builtin_vector_alignment_reachable (type, is_packed);
 }
diff --git a/gcc/config/arm/arm.h b/gcc/config/arm/arm.h
index 292b48f..b1492f4 100644
--- a/gcc/config/arm/arm.h
+++ b/gcc/config/arm/arm.h
@@ -103,6 +103,8 @@ extern char arm_arch_name[];
 	      builtin_define ("__ARM_PCS");		\
 	    builtin_define ("__ARM_EABI__");		\
 	  }						\
+	if (TARGET_IDIV)				\
+	  builtin_define ("__ARM_ARCH_EXT_IDIV__");	\
     } while (0)
 
 /* The various ARM cores.  */
@@ -233,6 +235,7 @@ extern void (*arm_lang_output_object_attributes_hook)(void);
 
 #define TARGET_HARD_TP			(target_thread_pointer == TP_CP15)
 #define TARGET_SOFT_TP			(target_thread_pointer == TP_SOFT)
+#define TARGET_GNU2_TLS			(target_tls_dialect == TLS_GNU2)
 
 /* Only 16-bit thumb code.  */
 #define TARGET_THUMB1			(TARGET_THUMB && !arm_arch_thumb2)
@@ -284,7 +287,8 @@ extern void (*arm_lang_output_object_attributes_hook)(void);
   (TARGET_32BIT && arm_arch6 && (arm_arch_notm || arm_arch7em))
 
 /* Should MOVW/MOVT be used in preference to a constant pool.  */
-#define TARGET_USE_MOVT (arm_arch_thumb2 && !optimize_size)
+#define TARGET_USE_MOVT \
+  (arm_arch_thumb2 && !optimize_size && !current_tune->prefer_constant_pool)
 
 /* We could use unified syntax for arm mode, but for now we just use it
    for Thumb-2.  */
@@ -306,6 +310,10 @@ extern void (*arm_lang_output_object_attributes_hook)(void);
 /* Nonzero if this chip supports ldrex{bhd} and strex{bhd}.  */
 #define TARGET_HAVE_LDREXBHD	((arm_arch6k && TARGET_ARM) || arm_arch7)
 
+/* Nonzero if integer division instructions supported.  */
+#define TARGET_IDIV		((TARGET_ARM && arm_arch_arm_hwdiv) \
+				 || (TARGET_THUMB2 && arm_arch_thumb_hwdiv))
+
 /* True iff the full BPABI is being used.  If TARGET_BPABI is true,
    then TARGET_AAPCS_BASED must be true -- but the converse does not
    hold.  TARGET_BPABI implies the use of the BPABI runtime library,
@@ -323,7 +331,8 @@ extern void (*arm_lang_output_object_attributes_hook)(void);
    --with-float is ignored if -mhard-float, -msoft-float or -mfloat-abi are
    specified.
    --with-fpu is ignored if -mfpu is specified.
-   --with-abi is ignored is -mabi is specified.  */
+   --with-abi is ignored is -mabi is specified.
+   --with-tls is ignored if -mtls-dialect is specified. */
 #define OPTION_DEFAULT_SPECS \
   {"arch", "%{!march=*:%{!mcpu=*:-march=%(VALUE)}}" }, \
   {"cpu", "%{!march=*:%{!mcpu=*:-mcpu=%(VALUE)}}" }, \
@@ -332,7 +341,8 @@ extern void (*arm_lang_output_object_attributes_hook)(void);
     "%{!msoft-float:%{!mhard-float:%{!mfloat-abi=*:-mfloat-abi=%(VALUE)}}}" }, \
   {"fpu", "%{!mfpu=*:-mfpu=%(VALUE)}"}, \
   {"abi", "%{!mabi=*:-mabi=%(VALUE)}"}, \
-  {"mode", "%{!marm:%{!mthumb:-m%(VALUE)}}"},
+  {"mode", "%{!marm:%{!mthumb:-m%(VALUE)}}"}, \
+  {"tls", "%{!mtls-dialect=*:-mtls-dialect=%(VALUE)}"},
 
 /* Which floating point model to use.  */
 enum arm_fp_model
@@ -418,7 +428,13 @@ enum arm_tp_type {
   TP_CP15
 };
 
+enum arm_tls_type {
+  TLS_GNU,
+  TLS_GNU2
+};
+
 extern enum arm_tp_type target_thread_pointer;
+extern enum arm_tls_type target_tls_dialect;
 
 /* Nonzero if this chip supports the ARM Architecture 3M extensions.  */
 extern int arm_arch3m;
@@ -490,8 +506,11 @@ extern int arm_cpp_interwork;
 /* Nonzero if chip supports Thumb 2.  */
 extern int arm_arch_thumb2;
 
-/* Nonzero if chip supports integer division instruction.  */
-extern int arm_arch_hwdiv;
+/* Nonzero if chip supports integer division instruction in ARM mode.  */
+extern int arm_arch_arm_hwdiv;
+
+/* Nonzero if chip supports integer division instruction in Thumb mode.  */
+extern int arm_arch_thumb_hwdiv;
 
 #ifndef TARGET_DEFAULT
 #define TARGET_DEFAULT  (MASK_APCS_FRAME)
@@ -653,6 +672,20 @@ extern int arm_structure_size_boundary;
 #define WCHAR_TYPE_SIZE BITS_PER_WORD
 #endif
 
+/* Sized for fixed-point types.  */
+
+#define SHORT_FRACT_TYPE_SIZE 8
+#define FRACT_TYPE_SIZE 16
+#define LONG_FRACT_TYPE_SIZE 32
+#define LONG_LONG_FRACT_TYPE_SIZE 64
+
+#define SHORT_ACCUM_TYPE_SIZE 16
+#define ACCUM_TYPE_SIZE 32
+#define LONG_ACCUM_TYPE_SIZE 64
+#define LONG_LONG_ACCUM_TYPE_SIZE 64
+
+#define MAX_FIXED_MODE_SIZE 64
+
 #ifndef SIZE_TYPE
 #define SIZE_TYPE (TARGET_AAPCS_BASED ? "unsigned int" : "long unsigned int")
 #endif
@@ -1002,7 +1035,7 @@ extern int arm_structure_size_boundary;
   (GET_MODE_CLASS (MODE1) == GET_MODE_CLASS (MODE2))
 
 #define VALID_IWMMXT_REG_MODE(MODE) \
- (arm_vector_mode_supported_p (MODE) || (MODE) == DImode)
+ (arm_vector_mode_supported_p (MODE) || (MODE) == DImode || (MODE) == SImode)
 
 /* Modes valid for Neon D registers.  */
 #define VALID_NEON_DREG_MODE(MODE) \
@@ -1188,7 +1221,7 @@ enum reg_class
    when addressing quantities in QI or HI mode; if we don't know the
    mode, then we must be conservative.  */
 #define MODE_BASE_REG_CLASS(MODE)					\
-    (TARGET_32BIT ? CORE_REGS :					\
+    (TARGET_ARM || (TARGET_THUMB2 && !optimize_size) ? CORE_REGS :	\
      (((MODE) == SImode) ? BASE_REGS : LO_REGS))
 
 /* For Thumb we can not support SP+reg addressing, so we return LO_REGS
@@ -1210,6 +1243,7 @@ enum reg_class
   (TARGET_32BIT ? (CLASS) :				\
    ((CLASS) == GENERAL_REGS || (CLASS) == HI_REGS	\
     || (CLASS) == NO_REGS || (CLASS) == STACK_REG	\
+    || (CLASS) == CORE_REGS				\
    ? LO_REGS : (CLASS)))
 
 /* Must leave BASE_REGS reloads alone */
@@ -2042,7 +2076,8 @@ typedef struct
 /* Try to generate sequences that don't involve branches, we can then use
    conditional instructions */
 #define BRANCH_COST(speed_p, predictable_p) \
-  (TARGET_32BIT ? 4 : (optimize > 0 ? 2 : 0))
+  (current_tune->branch_cost (speed_p, predictable_p))
+
 
 /* Position Independent Code.  */
 /* We decide which register to use based on the compilation options and
@@ -2258,6 +2293,8 @@ extern int making_const_table;
 #define RETURN_ADDR_RTX(COUNT, FRAME) \
   arm_return_addr (COUNT, FRAME)
 
+#define RETURN_ADDR_REGNUM LR_REGNUM
+
 /* Mask of the bits in the PC that contain the real return address
    when running in 26-bit mode.  */
 #define RETURN_ADDR_MASK26 (0x03fffffc)
diff --git a/gcc/config/arm/arm.md b/gcc/config/arm/arm.md
index 130053b..6477518 100644
--- a/gcc/config/arm/arm.md
+++ b/gcc/config/arm/arm.md
@@ -31,6 +31,7 @@
 ;; Register numbers
 (define_constants
   [(R0_REGNUM        0)		; First CORE register
+   (R1_REGNUM	     1)		; Second CORE register
    (IP_REGNUM	    12)		; Scratch register
    (SP_REGNUM	    13)		; Stack pointer
    (LR_REGNUM       14)		; Return address register
@@ -104,7 +105,11 @@
    (UNSPEC_SYMBOL_OFFSET 27) ; The offset of the start of the symbol from
                              ; another symbolic address.
    (UNSPEC_MEMORY_BARRIER 28) ; Represent a memory barrier.
-   (UNSPEC_PIC_UNIFIED 29)  ; Create a common pic addressing form.
+   (UNSPEC_UNALIGNED_LOAD 29) ; Used to represent ldr/ldrh instructions that
+			      ; access unaligned locations, on architectures
+			      ; which support that.
+   (UNSPEC_UNALIGNED_STORE 30) ; Same for str/strh.
+   (UNSPEC_PIC_UNIFIED 31)  ; Create a common pic addressing form.
   ]
 )
 
@@ -137,6 +142,8 @@
    (VUNSPEC_WCMP_EQ  12) ; Used by the iWMMXt WCMPEQ instructions
    (VUNSPEC_WCMP_GTU 13) ; Used by the iWMMXt WCMPGTU instructions
    (VUNSPEC_WCMP_GT  14) ; Used by the iwMMXT WCMPGT instructions
+   (VUNSPEC_ALIGN16  15) ; Used to force 16-byte alignment.
+   (VUNSPEC_ALIGN32  16) ; Used to force 32-byte alignment.
    (VUNSPEC_EH_RETURN 20); Use to override the return address for exception
 			 ; handling.
    (VUNSPEC_SYNC_COMPARE_AND_SWAP 21)	; Represent an atomic compare swap.
@@ -150,6 +157,9 @@
 ;;---------------------------------------------------------------------------
 ;; Attributes
 
+;; Processor type.  This is created automatically from arm-cores.def.
+(include "arm-tune.md")
+
 ; IS_THUMB is set to 'yes' when we are generating Thumb code, and 'no' when
 ; generating ARM code.  This is used to control the length of some insn
 ; patterns that share the same RTL in both ARM and Thumb code.
@@ -193,7 +203,7 @@
 ; for ARM or Thumb-2 with arm_arch6, and nov6 for ARM without
 ; arm_arch6.  This attribute is used to compute attribute "enabled",
 ; use type "any" to enable an alternative in all cases.
-(define_attr "arch" "any,a,t,32,t1,t2,v6,nov6"
+(define_attr "arch" "any,a,t,32,t1,t2,v6,nov6,onlya8,nota8,vfp9,notvfp9"
   (const_string "any"))
 
 (define_attr "arch_enabled" "no,yes"
@@ -226,6 +236,24 @@
 
 	 (and (eq_attr "arch" "nov6")
 	      (ne (symbol_ref "(TARGET_32BIT && !arm_arch6)") (const_int 0)))
+	 (const_string "yes")
+
+	 (and (eq_attr "arch" "onlya8")
+	      (eq_attr "tune" "cortexa8"))
+	 (const_string "yes")
+
+	 (and (eq_attr "arch" "nota8")
+	      (not (eq_attr "tune" "cortexa8")))
+	 (const_string "yes")
+
+	 (and (eq_attr "arch" "vfp9")
+	      (ne (symbol_ref "(TARGET_32BIT && !(arm_arch6 || TARGET_VFP3))")
+	          (const_int 0)))
+	 (const_string "yes")
+
+	 (and (eq_attr "arch" "notvfp9")
+	      (ne (symbol_ref "(TARGET_32BIT && (arm_arch6 || TARGET_VFP3))")
+	          (const_int 0)))
 	 (const_string "yes")]
 	(const_string "no")))
 
@@ -338,8 +366,6 @@
 (define_attr "ldsched" "no,yes" (const (symbol_ref "arm_ld_sched")))
 
 ;; Classification of NEON instructions for scheduling purposes.
-;; Do not set this attribute and the "type" attribute together in
-;; any one instruction pattern.
 (define_attr "neon_type"
    "neon_int_1,\
    neon_int_2,\
@@ -486,12 +512,9 @@
 ;;---------------------------------------------------------------------------
 ;; Pipeline descriptions
 
-;; Processor type.  This is created automatically from arm-cores.def.
-(include "arm-tune.md")
-
 (define_attr "tune_cortexr4" "yes,no"
   (const (if_then_else
-	  (eq_attr "tune" "cortexr4,cortexr4f")
+	  (eq_attr "tune" "cortexr4,cortexr4f,cortexr5")
 	  (const_string "yes")
 	  (const_string "no"))))
 
@@ -2385,10 +2408,10 @@
 ;;; this insv pattern, so this pattern needs to be reevalutated.
 
 (define_expand "insv"
-  [(set (zero_extract:SI (match_operand:SI 0 "s_register_operand" "")
-                         (match_operand:SI 1 "general_operand" "")
-                         (match_operand:SI 2 "general_operand" ""))
-        (match_operand:SI 3 "reg_or_int_operand" ""))]
+  [(set (zero_extract (match_operand 0 "nonimmediate_operand" "")
+                      (match_operand 1 "general_operand" "")
+                      (match_operand 2 "general_operand" ""))
+        (match_operand 3 "reg_or_int_operand" ""))]
   "TARGET_ARM || arm_arch_thumb2"
   "
   {
@@ -2399,35 +2422,70 @@
 
     if (arm_arch_thumb2)
       {
-	bool use_bfi = TRUE;
-
-	if (GET_CODE (operands[3]) == CONST_INT)
+        if (unaligned_access && MEM_P (operands[0])
+	    && s_register_operand (operands[3], GET_MODE (operands[3]))
+	    && (width == 16 || width == 32) && (start_bit % BITS_PER_UNIT) == 0)
 	  {
-	    HOST_WIDE_INT val = INTVAL (operands[3]) & mask;
+	    rtx base_addr;
 
-	    if (val == 0)
+	    if (BYTES_BIG_ENDIAN)
+	      start_bit = GET_MODE_BITSIZE (GET_MODE (operands[3])) - width
+			  - start_bit;
+
+	    if (width == 32)
 	      {
-		emit_insn (gen_insv_zero (operands[0], operands[1],
-					  operands[2]));
-		DONE;
+	        base_addr = adjust_address (operands[0], SImode,
+					    start_bit / BITS_PER_UNIT);
+		emit_insn (gen_unaligned_storesi (base_addr, operands[3]));
 	      }
+	    else
+	      {
+	        rtx tmp = gen_reg_rtx (HImode);
 
-	    /* See if the set can be done with a single orr instruction.  */
-	    if (val == mask && const_ok_for_arm (val << start_bit))
-	      use_bfi = FALSE;
+	        base_addr = adjust_address (operands[0], HImode,
+					    start_bit / BITS_PER_UNIT);
+		emit_move_insn (tmp, gen_lowpart (HImode, operands[3]));
+		emit_insn (gen_unaligned_storehi (base_addr, tmp));
+	      }
+	    DONE;
 	  }
-	  
-	if (use_bfi)
+	else if (s_register_operand (operands[0], GET_MODE (operands[0])))
 	  {
-	    if (GET_CODE (operands[3]) != REG)
-	      operands[3] = force_reg (SImode, operands[3]);
+	    bool use_bfi = TRUE;
 
-	    emit_insn (gen_insv_t2 (operands[0], operands[1], operands[2],
-				    operands[3]));
-	    DONE;
+	    if (GET_CODE (operands[3]) == CONST_INT)
+	      {
+		HOST_WIDE_INT val = INTVAL (operands[3]) & mask;
+
+		if (val == 0)
+		  {
+		    emit_insn (gen_insv_zero (operands[0], operands[1],
+					      operands[2]));
+		    DONE;
+		  }
+
+		/* See if the set can be done with a single orr instruction.  */
+		if (val == mask && const_ok_for_arm (val << start_bit))
+		  use_bfi = FALSE;
+	      }
+
+	    if (use_bfi)
+	      {
+		if (GET_CODE (operands[3]) != REG)
+		  operands[3] = force_reg (SImode, operands[3]);
+
+		emit_insn (gen_insv_t2 (operands[0], operands[1], operands[2],
+					operands[3]));
+		DONE;
+	      }
 	  }
+	else
+	  FAIL;
       }
 
+    if (!s_register_operand (operands[0], GET_MODE (operands[0])))
+      FAIL;
+
     target = copy_rtx (operands[0]);
     /* Avoid using a subreg as a subtarget, and avoid writing a paradoxical 
        subreg as the final target.  */
@@ -3619,12 +3677,10 @@
 ;; to reduce register pressure later on.
 
 (define_expand "extzv"
-  [(set (match_dup 4)
-	(ashift:SI (match_operand:SI   1 "register_operand" "")
-		   (match_operand:SI   2 "const_int_operand" "")))
-   (set (match_operand:SI              0 "register_operand" "")
-	(lshiftrt:SI (match_dup 4)
-		     (match_operand:SI 3 "const_int_operand" "")))]
+  [(set (match_operand 0 "s_register_operand" "")
+	(zero_extract (match_operand 1 "nonimmediate_operand" "")
+		      (match_operand 2 "const_int_operand" "")
+		      (match_operand 3 "const_int_operand" "")))]
   "TARGET_THUMB1 || arm_arch_thumb2"
   "
   {
@@ -3633,10 +3689,57 @@
     
     if (arm_arch_thumb2)
       {
-	emit_insn (gen_extzv_t2 (operands[0], operands[1], operands[2],
-				 operands[3]));
-	DONE;
+	HOST_WIDE_INT width = INTVAL (operands[2]);
+	HOST_WIDE_INT bitpos = INTVAL (operands[3]);
+
+	if (unaligned_access && MEM_P (operands[1])
+	    && (width == 16 || width == 32) && (bitpos % BITS_PER_UNIT) == 0)
+	  {
+	    rtx base_addr;
+
+	    if (BYTES_BIG_ENDIAN)
+	      bitpos = GET_MODE_BITSIZE (GET_MODE (operands[0])) - width
+		       - bitpos;
+
+	    if (width == 32)
+              {
+		base_addr = adjust_address (operands[1], SImode,
+					    bitpos / BITS_PER_UNIT);
+		emit_insn (gen_unaligned_loadsi (operands[0], base_addr));
+              }
+	    else
+              {
+		rtx dest = operands[0];
+		rtx tmp = gen_reg_rtx (SImode);
+
+		/* We may get a paradoxical subreg here.  Strip it off.  */
+		if (GET_CODE (dest) == SUBREG
+		    && GET_MODE (dest) == SImode
+		    && GET_MODE (SUBREG_REG (dest)) == HImode)
+		  dest = SUBREG_REG (dest);
+
+		if (GET_MODE_BITSIZE (GET_MODE (dest)) != width)
+		  FAIL;
+
+		base_addr = adjust_address (operands[1], HImode,
+					    bitpos / BITS_PER_UNIT);
+		emit_insn (gen_unaligned_loadhiu (tmp, base_addr));
+		emit_move_insn (gen_lowpart (SImode, dest), tmp);
+	      }
+	    DONE;
+	  }
+	else if (s_register_operand (operands[1], GET_MODE (operands[1])))
+	  {
+	    emit_insn (gen_extzv_t2 (operands[0], operands[1], operands[2],
+				     operands[3]));
+	    DONE;
+	  }
+	else
+	  FAIL;
       }
+    
+    if (!s_register_operand (operands[1], GET_MODE (operands[1])))
+      FAIL;
 
     operands[3] = GEN_INT (rshift);
     
@@ -3646,12 +3749,154 @@
         DONE;
       }
       
-    operands[2] = GEN_INT (lshift);
-    operands[4] = gen_reg_rtx (SImode);
+    emit_insn (gen_extzv_t1 (operands[0], operands[1], GEN_INT (lshift),
+			     operands[3], gen_reg_rtx (SImode)));
+    DONE;
   }"
 )
 
-(define_insn "extv"
+;; Helper for extzv, for the Thumb-1 register-shifts case.
+
+(define_expand "extzv_t1"
+  [(set (match_operand:SI 4 "s_register_operand" "")
+	(ashift:SI (match_operand:SI 1 "nonimmediate_operand" "")
+		   (match_operand:SI 2 "const_int_operand" "")))
+   (set (match_operand:SI 0 "s_register_operand" "")
+	(lshiftrt:SI (match_dup 4)
+		     (match_operand:SI 3 "const_int_operand" "")))]
+  "TARGET_THUMB1"
+  "")
+
+(define_expand "extv"
+  [(set (match_operand 0 "s_register_operand" "")
+	(sign_extract (match_operand 1 "nonimmediate_operand" "")
+		      (match_operand 2 "const_int_operand" "")
+		      (match_operand 3 "const_int_operand" "")))]
+  "arm_arch_thumb2"
+{
+  HOST_WIDE_INT width = INTVAL (operands[2]);
+  HOST_WIDE_INT bitpos = INTVAL (operands[3]);
+
+  if (unaligned_access && MEM_P (operands[1]) && (width == 16 || width == 32)
+      && (bitpos % BITS_PER_UNIT)  == 0)
+    {
+      rtx base_addr;
+      
+      if (BYTES_BIG_ENDIAN)
+	bitpos = GET_MODE_BITSIZE (GET_MODE (operands[0])) - width - bitpos;
+      
+      if (width == 32)
+        {
+	  base_addr = adjust_address (operands[1], SImode,
+				      bitpos / BITS_PER_UNIT);
+	  emit_insn (gen_unaligned_loadsi (operands[0], base_addr));
+        }
+      else
+        {
+	  rtx dest = operands[0];
+	  rtx tmp = gen_reg_rtx (SImode);
+	  
+	  /* We may get a paradoxical subreg here.  Strip it off.  */
+	  if (GET_CODE (dest) == SUBREG
+	      && GET_MODE (dest) == SImode
+	      && GET_MODE (SUBREG_REG (dest)) == HImode)
+	    dest = SUBREG_REG (dest);
+	  
+	  if (GET_MODE_BITSIZE (GET_MODE (dest)) != width)
+	    FAIL;
+	  
+	  base_addr = adjust_address (operands[1], HImode,
+				      bitpos / BITS_PER_UNIT);
+	  emit_insn (gen_unaligned_loadhis (tmp, base_addr));
+	  emit_move_insn (gen_lowpart (SImode, dest), tmp);
+	}
+
+      DONE;
+    }
+  else if (!s_register_operand (operands[1], GET_MODE (operands[1])))
+    FAIL;
+  else if (GET_MODE (operands[0]) == SImode
+	   && GET_MODE (operands[1]) == SImode)
+    {
+      emit_insn (gen_extv_regsi (operands[0], operands[1], operands[2],
+				 operands[3]));
+      DONE;
+    }
+
+  FAIL;
+})
+
+; Helper to expand register forms of extv with the proper modes.
+
+(define_expand "extv_regsi"
+  [(set (match_operand:SI 0 "s_register_operand" "")
+	(sign_extract:SI (match_operand:SI 1 "s_register_operand" "")
+			 (match_operand 2 "const_int_operand" "")
+			 (match_operand 3 "const_int_operand" "")))]
+  ""
+{
+})
+
+; ARMv6+ unaligned load/store instructions (used for packed structure accesses).
+
+(define_insn "unaligned_loadsi"
+  [(set (match_operand:SI 0 "s_register_operand" "=l,r")
+	(unspec:SI [(match_operand:SI 1 "memory_operand" "Uw,m")]
+		   UNSPEC_UNALIGNED_LOAD))]
+  "unaligned_access && TARGET_32BIT"
+  "ldr%?\t%0, %1\t@ unaligned"
+  [(set_attr "arch" "t2,any")
+   (set_attr "length" "2,4")
+   (set_attr "predicable" "yes")
+   (set_attr "type" "load1")])
+
+(define_insn "unaligned_loadhis"
+  [(set (match_operand:SI 0 "s_register_operand" "=l,r")
+	(sign_extend:SI
+	  (unspec:HI [(match_operand:HI 1 "memory_operand" "Uw,m")]
+		     UNSPEC_UNALIGNED_LOAD)))]
+  "unaligned_access && TARGET_32BIT"
+  "ldr%(sh%)\t%0, %1\t@ unaligned"
+  [(set_attr "arch" "t2,any")
+   (set_attr "length" "2,4")
+   (set_attr "predicable" "yes")
+   (set_attr "type" "load_byte")])
+
+(define_insn "unaligned_loadhiu"
+  [(set (match_operand:SI 0 "s_register_operand" "=l,r")
+	(zero_extend:SI
+	  (unspec:HI [(match_operand:HI 1 "memory_operand" "Uw,m")]
+		     UNSPEC_UNALIGNED_LOAD)))]
+  "unaligned_access && TARGET_32BIT"
+  "ldr%(h%)\t%0, %1\t@ unaligned"
+  [(set_attr "arch" "t2,any")
+   (set_attr "length" "2,4")
+   (set_attr "predicable" "yes")
+   (set_attr "type" "load_byte")])
+
+(define_insn "unaligned_storesi"
+  [(set (match_operand:SI 0 "memory_operand" "=Uw,m")
+	(unspec:SI [(match_operand:SI 1 "s_register_operand" "l,r")]
+		   UNSPEC_UNALIGNED_STORE))]
+  "unaligned_access && TARGET_32BIT"
+  "str%?\t%1, %0\t@ unaligned"
+  [(set_attr "arch" "t2,any")
+   (set_attr "length" "2,4")
+   (set_attr "predicable" "yes")
+   (set_attr "type" "store1")])
+
+(define_insn "unaligned_storehi"
+  [(set (match_operand:HI 0 "memory_operand" "=Uw,m")
+	(unspec:HI [(match_operand:HI 1 "s_register_operand" "l,r")]
+		   UNSPEC_UNALIGNED_STORE))]
+  "unaligned_access && TARGET_32BIT"
+  "str%(h%)\t%1, %0\t@ unaligned"
+  [(set_attr "arch" "t2,any")
+   (set_attr "length" "2,4")
+   (set_attr "predicable" "yes")
+   (set_attr "type" "store1")])
+
+(define_insn "*extv_reg"
   [(set (match_operand:SI 0 "s_register_operand" "=r")
 	(sign_extract:SI (match_operand:SI 1 "s_register_operand" "r")
                          (match_operand:SI 2 "const_int_operand" "M")
@@ -3673,6 +3918,28 @@
    (set_attr "predicable" "yes")]
 )
 
+
+;; Division instructions
+(define_insn "divsi3"
+  [(set (match_operand:SI	  0 "s_register_operand" "=r")
+	(div:SI (match_operand:SI 1 "s_register_operand"  "r")
+		(match_operand:SI 2 "s_register_operand"  "r")))]
+  "TARGET_IDIV"
+  "sdiv%?\t%0, %1, %2"
+  [(set_attr "predicable" "yes")
+   (set_attr "insn" "sdiv")]
+)
+
+(define_insn "udivsi3"
+  [(set (match_operand:SI	   0 "s_register_operand" "=r")
+	(udiv:SI (match_operand:SI 1 "s_register_operand"  "r")
+		 (match_operand:SI 2 "s_register_operand"  "r")))]
+  "TARGET_IDIV"
+  "udiv%?\t%0, %1, %2"
+  [(set_attr "predicable" "yes")
+   (set_attr "insn" "udiv")]
+)
+
 
 ;; Unary arithmetic insns
 
@@ -4045,8 +4312,8 @@
 
 (define_insn "zero_extend<mode>di2"
   [(set (match_operand:DI 0 "s_register_operand" "=r")
-        (zero_extend:DI (match_operand:QHSI 1 "<qhs_extenddi_op>"
-					    "<qhs_extenddi_cstr>")))]
+        (zero_extend:DI (match_operand:QHSI 1 "<qhs_zextenddi_op>"
+					    "<qhs_zextenddi_cstr>")))]
   "TARGET_32BIT <qhs_zextenddi_cond>"
   "#"
   [(set_attr "length" "8")
@@ -4200,7 +4467,9 @@
    #
    ldr%(h%)\\t%0, %1"
   [(set_attr "type" "alu_shift,load_byte")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "pool_range" "*,256")
+   (set_attr "neg_pool_range" "*,244")]
 )
 
 (define_insn "*arm_zero_extendhisi2_v6"
@@ -4211,7 +4480,9 @@
    uxth%?\\t%0, %1
    ldr%(h%)\\t%0, %1"
   [(set_attr "type" "alu_shift,load_byte")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "pool_range" "*,256")
+   (set_attr "neg_pool_range" "*,244")]
 )
 
 (define_insn "*arm_zero_extendhisi2addsi"
@@ -4293,7 +4564,9 @@
    ldr%(b%)\\t%0, %1\\t%@ zero_extendqisi2"
   [(set_attr "length" "8,4")
    (set_attr "type" "alu_shift,load_byte")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "pool_range" "*,256")
+   (set_attr "neg_pool_range" "*,244")]
 )
 
 (define_insn "*arm_zero_extendqisi2_v6"
@@ -4304,7 +4577,9 @@
    uxtb%(%)\\t%0, %1
    ldr%(b%)\\t%0, %1\\t%@ zero_extendqisi2"
   [(set_attr "type" "alu_shift,load_byte")
-   (set_attr "predicable" "yes")]
+   (set_attr "predicable" "yes")
+   (set_attr "pool_range" "*,256")
+   (set_attr "neg_pool_range" "*,244")]
 )
 
 (define_insn "*arm_zero_extendqisi2addsi"
@@ -5049,14 +5324,6 @@
 			       optimize && can_create_pseudo_p ());
           DONE;
         }
-
-      if (TARGET_USE_MOVT && !target_word_relocations
-	  && GET_CODE (operands[1]) == SYMBOL_REF
-	  && !flag_pic && !arm_tls_referenced_p (operands[1]))
-	{
-	  arm_emit_movpair (operands[0], operands[1]);
-	  DONE;
-	}
     }
   else /* TARGET_THUMB1...  */
     {
@@ -5165,6 +5432,19 @@
   "
 )
 
+(define_split
+  [(set (match_operand:SI 0 "arm_general_register_operand" "")
+	(match_operand:SI 1 "general_operand" ""))]
+  "TARGET_32BIT
+   && TARGET_USE_MOVT && GET_CODE (operands[1]) == SYMBOL_REF
+   && !flag_pic && !target_word_relocations
+   && !arm_tls_referenced_p (operands[1])"
+  [(clobber (const_int 0))]
+{
+  arm_emit_movpair (operands[0], operands[1]);
+  DONE;
+})
+
 (define_insn "*thumb1_movsi_insn"
   [(set (match_operand:SI 0 "nonimmediate_operand" "=l,l,l,l,l,>,l, m,*l*h*k")
 	(match_operand:SI 1 "general_operand"      "l, I,J,K,>,l,mi,l,*l*h*k"))]
@@ -5877,7 +6157,7 @@
 (define_expand "reload_inhi"
   [(parallel [(match_operand:HI 0 "s_register_operand" "=r")
 	      (match_operand:HI 1 "arm_reload_memory_operand" "o")
-	      (match_operand:DI 2 "s_register_operand" "=&r")])]
+	      (match_operand:DI 2 "s_register_operand" "=&l")])]
   "TARGET_EITHER"
   "
   if (TARGET_ARM)
@@ -6468,7 +6748,7 @@
 
 (define_expand "cbranchsi4"
   [(set (pc) (if_then_else
-	      (match_operator 0 "arm_comparison_operator"
+	      (match_operator 0 "expandable_comparison_operator"
 	       [(match_operand:SI 1 "s_register_operand" "")
 	        (match_operand:SI 2 "nonmemory_operand" "")])
 	      (label_ref (match_operand 3 "" ""))
@@ -6519,7 +6799,7 @@
 
 (define_expand "cbranchsf4"
   [(set (pc) (if_then_else
-	      (match_operator 0 "arm_comparison_operator"
+	      (match_operator 0 "expandable_comparison_operator"
 	       [(match_operand:SF 1 "s_register_operand" "")
 	        (match_operand:SF 2 "arm_float_compare_operand" "")])
 	      (label_ref (match_operand 3 "" ""))
@@ -6531,7 +6811,7 @@
 
 (define_expand "cbranchdf4"
   [(set (pc) (if_then_else
-	      (match_operator 0 "arm_comparison_operator"
+	      (match_operator 0 "expandable_comparison_operator"
 	       [(match_operand:DF 1 "s_register_operand" "")
 	        (match_operand:DF 2 "arm_float_compare_operand" "")])
 	      (label_ref (match_operand 3 "" ""))
@@ -6543,7 +6823,7 @@
 
 (define_expand "cbranchdi4"
   [(set (pc) (if_then_else
-	      (match_operator 0 "arm_comparison_operator"
+	      (match_operator 0 "expandable_comparison_operator"
 	       [(match_operand:DI 1 "cmpdi_operand" "")
 	        (match_operand:DI 2 "cmpdi_operand" "")])
 	      (label_ref (match_operand 3 "" ""))
@@ -7380,7 +7660,7 @@
 
 (define_expand "cstoresi4"
   [(set (match_operand:SI 0 "s_register_operand" "")
-	(match_operator:SI 1 "arm_comparison_operator"
+	(match_operator:SI 1 "expandable_comparison_operator"
 	 [(match_operand:SI 2 "s_register_operand" "")
 	  (match_operand:SI 3 "reg_or_int_operand" "")]))]
   "TARGET_32BIT || TARGET_THUMB1"
@@ -7516,7 +7796,7 @@
 
 (define_expand "cstoresf4"
   [(set (match_operand:SI 0 "s_register_operand" "")
-	(match_operator:SI 1 "arm_comparison_operator"
+	(match_operator:SI 1 "expandable_comparison_operator"
 	 [(match_operand:SF 2 "s_register_operand" "")
 	  (match_operand:SF 3 "arm_float_compare_operand" "")]))]
   "TARGET_32BIT && TARGET_HARD_FLOAT"
@@ -7526,7 +7806,7 @@
 
 (define_expand "cstoredf4"
   [(set (match_operand:SI 0 "s_register_operand" "")
-	(match_operator:SI 1 "arm_comparison_operator"
+	(match_operator:SI 1 "expandable_comparison_operator"
 	 [(match_operand:DF 2 "s_register_operand" "")
 	  (match_operand:DF 3 "arm_float_compare_operand" "")]))]
   "TARGET_32BIT && TARGET_HARD_FLOAT && !TARGET_VFP_SINGLE"
@@ -7536,7 +7816,7 @@
 
 (define_expand "cstoredi4"
   [(set (match_operand:SI 0 "s_register_operand" "")
-	(match_operator:SI 1 "arm_comparison_operator"
+	(match_operator:SI 1 "expandable_comparison_operator"
 	 [(match_operand:DI 2 "cmpdi_operand" "")
 	  (match_operand:DI 3 "cmpdi_operand" "")]))]
   "TARGET_32BIT"
@@ -7656,7 +7936,7 @@
 
 (define_expand "movsicc"
   [(set (match_operand:SI 0 "s_register_operand" "")
-	(if_then_else:SI (match_operand 1 "arm_comparison_operator" "")
+	(if_then_else:SI (match_operand 1 "expandable_comparison_operator" "")
 			 (match_operand:SI 2 "arm_not_operand" "")
 			 (match_operand:SI 3 "arm_not_operand" "")))]
   "TARGET_32BIT"
@@ -7676,7 +7956,7 @@
 
 (define_expand "movsfcc"
   [(set (match_operand:SF 0 "s_register_operand" "")
-	(if_then_else:SF (match_operand 1 "arm_comparison_operator" "")
+	(if_then_else:SF (match_operand 1 "expandable_comparison_operator" "")
 			 (match_operand:SF 2 "s_register_operand" "")
 			 (match_operand:SF 3 "nonmemory_operand" "")))]
   "TARGET_32BIT && TARGET_HARD_FLOAT"
@@ -7702,7 +7982,7 @@
 
 (define_expand "movdfcc"
   [(set (match_operand:DF 0 "s_register_operand" "")
-	(if_then_else:DF (match_operand 1 "arm_comparison_operator" "")
+	(if_then_else:DF (match_operand 1 "expandable_comparison_operator" "")
 			 (match_operand:DF 2 "s_register_operand" "")
 			 (match_operand:DF 3 "arm_float_add_operand" "")))]
   "TARGET_32BIT && TARGET_HARD_FLOAT && (TARGET_FPA || TARGET_VFP_DOUBLE)"
@@ -8139,66 +8419,65 @@
   [(set_attr "type" "call")]
 )
 
-(define_expand "return"
-  [(return)]
-  "TARGET_32BIT && USE_RETURN_INSN (FALSE)"
+(define_expand "<return_str>return"
+  [(returns)]
+  "TARGET_32BIT<return_cond>"
   "")
 
-;; Often the return insn will be the same as loading from memory, so set attr
-(define_insn "*arm_return"
-  [(return)]
-  "TARGET_ARM && USE_RETURN_INSN (FALSE)"
-  "*
-  {
-    if (arm_ccfsm_state == 2)
-      {
-        arm_ccfsm_state += 2;
-        return \"\";
-      }
-    return output_return_instruction (const_true_rtx, TRUE, FALSE);
-  }"
+(define_insn "*arm_<return_str>return"
+  [(returns)]
+  "TARGET_ARM<return_cond>"
+{
+  if (arm_ccfsm_state == 2)
+    {
+      arm_ccfsm_state += 2;
+      return "";
+    }
+  return output_return_instruction (const_true_rtx, true, false,
+				    <return_simple_p>);
+}
   [(set_attr "type" "load1")
    (set_attr "length" "12")
    (set_attr "predicable" "yes")]
 )
 
-(define_insn "*cond_return"
+(define_insn "*cond_<return_str>return"
   [(set (pc)
         (if_then_else (match_operator 0 "arm_comparison_operator"
 		       [(match_operand 1 "cc_register" "") (const_int 0)])
-                      (return)
+                      (returns)
                       (pc)))]
-  "TARGET_ARM && USE_RETURN_INSN (TRUE)"
-  "*
-  {
-    if (arm_ccfsm_state == 2)
-      {
-        arm_ccfsm_state += 2;
-        return \"\";
-      }
-    return output_return_instruction (operands[0], TRUE, FALSE);
-  }"
+  "TARGET_ARM<return_cond>"
+{
+  if (arm_ccfsm_state == 2)
+    {
+      arm_ccfsm_state += 2;
+      return "";
+    }
+  return output_return_instruction (operands[0], true, false,
+				    <return_simple_p>);
+}
   [(set_attr "conds" "use")
    (set_attr "length" "12")
    (set_attr "type" "load1")]
 )
 
-(define_insn "*cond_return_inverted"
+(define_insn "*cond_<return_str>return_inverted"
   [(set (pc)
         (if_then_else (match_operator 0 "arm_comparison_operator"
 		       [(match_operand 1 "cc_register" "") (const_int 0)])
                       (pc)
-		      (return)))]
-  "TARGET_ARM && USE_RETURN_INSN (TRUE)"
-  "*
-  {
-    if (arm_ccfsm_state == 2)
-      {
-        arm_ccfsm_state += 2;
-        return \"\";
-      }
-    return output_return_instruction (operands[0], TRUE, TRUE);
-  }"
+		      (returns)))]
+  "TARGET_ARM<return_cond>"
+{
+  if (arm_ccfsm_state == 2)
+    {
+      arm_ccfsm_state += 2;
+      return "";
+    }
+  return output_return_instruction (operands[0], true, true,
+				    <return_simple_p>);
+}
   [(set_attr "conds" "use")
    (set_attr "length" "12")
    (set_attr "type" "load1")]
@@ -8377,7 +8656,8 @@
 	rtx reg = gen_reg_rtx (SImode);
 
 	emit_insn (gen_addsi3 (reg, operands[0],
-			       GEN_INT (-INTVAL (operands[1]))));
+			       gen_int_mode (-INTVAL (operands[1]),
+			       		     SImode)));
 	operands[0] = reg;
       }
 
@@ -10001,8 +10281,7 @@
       DONE;
     }
   emit_jump_insn (gen_rtx_UNSPEC_VOLATILE (VOIDmode,
-	gen_rtvec (1,
-		gen_rtx_RETURN (VOIDmode)),
+	gen_rtvec (1, ret_rtx),
 	VUNSPEC_EPILOGUE));
   DONE;
   "
@@ -10019,7 +10298,7 @@
   "TARGET_32BIT"
   "*
   if (use_return_insn (FALSE, next_nonnote_insn (insn)))
-    return output_return_instruction (const_true_rtx, FALSE, FALSE);
+    return output_return_instruction (const_true_rtx, false, false, false);
   return arm_output_epilogue (next_nonnote_insn (insn));
   "
 ;; Length is absolute worst case
@@ -10338,6 +10617,24 @@
   "
 )
 
+(define_insn "align_16"
+  [(unspec_volatile [(const_int 0)] VUNSPEC_ALIGN16)]
+  "TARGET_EITHER"
+  "*
+  assemble_align (128);
+  return \"\";
+  "
+)
+
+(define_insn "align_32"
+  [(unspec_volatile [(const_int 0)] VUNSPEC_ALIGN32)]
+  "TARGET_EITHER"
+  "*
+  assemble_align (256);
+  return \"\";
+  "
+)
+
 (define_insn "consttable_end"
   [(unspec_volatile [(const_int 0)] VUNSPEC_POOL_END)]
   "TARGET_EITHER"
@@ -10611,6 +10908,24 @@
   [(set_attr "conds" "clob")]
 )
 
+;; tls descriptor call
+(define_insn "tlscall"
+  [(set (reg:SI R0_REGNUM) (unspec:SI [(reg:SI R0_REGNUM)
+                               (match_operand:SI 0 "" "X")
+			       (match_operand 1 "" "")] UNSPEC_TLS))
+   (clobber (reg:SI R1_REGNUM))
+   (clobber (reg:SI LR_REGNUM))
+   (clobber (reg:SI CC_REGNUM))]
+  "TARGET_GNU2_TLS"
+  {
+    targetm.asm_out.internal_label (asm_out_file, "LPIC",
+				    INTVAL (operands[1]));
+    return "bl\\t+%c0(tlscall)";
+  }
+  [(set_attr "conds" "clob")
+   (set_attr "length" "4")]
+)
+
 ;; We only care about the lower 16 bits of the constant 
 ;; being inserted into the upper 16 bits of the register.
 (define_insn "*arm_movtas_ze" 
@@ -10744,3 +11059,5 @@
 (include "neon.md")
 ;; Synchronization Primitives
 (include "sync.md")
+;; Fixed-point patterns
+(include "arm-fixed.md")
diff --git a/gcc/config/arm/arm.opt b/gcc/config/arm/arm.opt
index a39bb3a..3ddeef7 100644
--- a/gcc/config/arm/arm.opt
+++ b/gcc/config/arm/arm.opt
@@ -137,6 +137,10 @@ mthumb-interwork
 Target Report Mask(INTERWORK)
 Support calls between Thumb and ARM instruction sets
 
+mtls-dialect=
+Target RejectNegative Joined Var(target_tls_dialect_switch)
+Specify thread local storage scheme
+
 mtp=
 Target RejectNegative Joined Var(target_thread_switch)
 Specify how to access the thread pointer
@@ -169,3 +173,7 @@ mfix-cortex-m3-ldrd
 Target Report Var(fix_cm3_ldrd) Init(2)
 Avoid overlapping destination and address registers on LDRD instructions
 that may trigger Cortex-M3 errata.
+
+munaligned-access
+Target Report Var(unaligned_access) Init(2)
+Enable unaligned word and halfword accesses to packed data.
diff --git a/gcc/config/arm/constraints.md b/gcc/config/arm/constraints.md
index 4e220e5..4e9c07a 100644
--- a/gcc/config/arm/constraints.md
+++ b/gcc/config/arm/constraints.md
@@ -29,13 +29,14 @@
 ;; in Thumb-1 state: I, J, K, L, M, N, O
 
 ;; The following multi-letter normal constraints have been used:
-;; in ARM/Thumb-2 state: Da, Db, Dc, Dn, Dl, DL, Dv, Dy, Di, Dz
+;; in ARM/Thumb-2 state: D0, Da, Db, Dc, Dn, Dl, DL, Dv, Dy, Di, Dz
 ;; in Thumb-1 state: Pa, Pb, Pc, Pd
 ;; in Thumb-2 state: Ps, Pt, Pu, Pv, Pw, Px
 
 ;; The following memory constraints have been used:
 ;; in ARM/Thumb-2 state: Q, Ut, Uv, Uy, Un, Um, Us
 ;; in ARM state: Uq
+;; in Thumb state: Uw
 
 
 (define_register_constraint "f" "TARGET_ARM ? FPA_REGS : NO_REGS"
@@ -205,6 +206,13 @@
  (and (match_code "const_vector")
       (match_test "TARGET_NEON && op == CONST0_RTX (mode)")))
 
+(define_constraint "D0"
+ "@internal
+  In ARM/Thumb-2 state a 0.0 floating point constant which can
+  be loaded with a Neon vmov immediate instruction."
+ (and (match_code "const_double")
+      (match_test "TARGET_NEON && op == CONST0_RTX (mode)")))
+
 (define_constraint "Da"
  "@internal
   In ARM/Thumb-2 state a const_int, const_double or const_vector that can
@@ -327,6 +335,19 @@
  (and (match_code "mem")
       (match_test "REG_P (XEXP (op, 0))")))
 
+; The 16-bit post-increment LDR/STR accepted by thumb1_legitimate_address_p
+; are actually LDM/STM instructions, so cannot be used to access unaligned
+; data.
+(define_memory_constraint "Uw"
+ "@internal
+  In Thumb state an address that is valid in 16bit encoding, and that can be
+  used for unaligned accesses."
+ (and (match_code "mem")
+      (match_test "TARGET_THUMB
+		   && thumb1_legitimate_address_p (GET_MODE (op), XEXP (op, 0),
+						   0)
+		   && GET_CODE (XEXP (op, 0)) != POST_INC")))
+
 ;; We used to have constraint letters for S and R in ARM state, but
 ;; all uses of these now appear to have been removed.
 
diff --git a/gcc/config/arm/elf.h b/gcc/config/arm/elf.h
index 8840088..e06e49a 100644
--- a/gcc/config/arm/elf.h
+++ b/gcc/config/arm/elf.h
@@ -152,7 +152,7 @@
 
 /* Horrible hack: We want to prevent some libgcc routines being included
    for some multilibs.  */
-#ifndef __ARM_ARCH_6M__
+#if !(defined(__ARM_ARCH_6M__) || defined(__ARM_ARCH_6SM__))
 #undef L_fixdfsi
 #undef L_fixunsdfsi
 #undef L_truncdfsf2
diff --git a/gcc/config/arm/fp16.c b/gcc/config/arm/fp16.c
index 936caeb..a49401d 100644
--- a/gcc/config/arm/fp16.c
+++ b/gcc/config/arm/fp16.c
@@ -22,10 +22,10 @@
    see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
    <http://www.gnu.org/licenses/>.  */
 
-static inline unsigned short
+static inline unsigned int
 __gnu_f2h_internal(unsigned int a, int ieee)
 {
-  unsigned short sign = (a >> 16) & 0x8000;
+  unsigned int sign = (a >> 16) & 0x8000;
   int aexp = (a >> 23) & 0xff;
   unsigned int mantissa = a & 0x007fffff;
   unsigned int mask;
@@ -95,10 +95,10 @@ __gnu_f2h_internal(unsigned int a, int ieee)
   return sign | (((aexp + 14) << 10) + (mantissa >> 13));
 }
 
-unsigned int
-__gnu_h2f_internal(unsigned short a, int ieee)
+static inline unsigned int
+__gnu_h2f_internal(unsigned int a, int ieee)
 {
-  unsigned int sign = (unsigned int)(a & 0x8000) << 16;
+  unsigned int sign = (a & 0x00008000) << 16;
   int aexp = (a >> 10) & 0x1f;
   unsigned int mantissa = a & 0x3ff;
 
@@ -120,26 +120,33 @@ __gnu_h2f_internal(unsigned short a, int ieee)
   return sign | (((aexp + 0x70) << 23) + (mantissa << 13));
 }
 
-unsigned short
+#define ALIAS(src, dst) \
+  typeof (src) dst __attribute__ ((alias (#src)));
+
+unsigned int
 __gnu_f2h_ieee(unsigned int a)
 {
   return __gnu_f2h_internal(a, 1);
 }
+ALIAS (__gnu_f2h_ieee, __aeabi_f2h)
 
 unsigned int
-__gnu_h2f_ieee(unsigned short a)
+__gnu_h2f_ieee(unsigned int a)
 {
   return __gnu_h2f_internal(a, 1);
 }
+ALIAS (__gnu_h2f_ieee, __aeabi_h2f)
 
-unsigned short
+unsigned int
 __gnu_f2h_alternative(unsigned int x)
 {
   return __gnu_f2h_internal(x, 0);
 }
+ALIAS (__gnu_f2h_alternative, __aeabi_f2h_alt)
 
 unsigned int
-__gnu_h2f_alternative(unsigned short a)
+__gnu_h2f_alternative(unsigned int a)
 {
   return __gnu_h2f_internal(a, 0);
 }
+ALIAS (__gnu_h2f_alternative, __aeabi_h2f_alt)
diff --git a/gcc/config/arm/iterators.md b/gcc/config/arm/iterators.md
index 887c962b..0684824 100644
--- a/gcc/config/arm/iterators.md
+++ b/gcc/config/arm/iterators.md
@@ -140,7 +140,18 @@
 
 ;; Modes with 8-bit, 16-bit and 32-bit elements.
 (define_mode_iterator VU [V16QI V8HI V4SI])
- 
+
+;; Iterators used for fixed-point support.
+(define_mode_iterator FIXED [QQ HQ SQ UQQ UHQ USQ HA SA UHA USA])
+
+(define_mode_iterator ADDSUB [V4QQ V2HQ V2HA])
+
+(define_mode_iterator UQADDSUB [V4UQQ V2UHQ UQQ UHQ V2UHA UHA])
+
+(define_mode_iterator QADDSUB [V4QQ V2HQ QQ HQ V2HA HA SQ SA])
+
+(define_mode_iterator QMUL [HQ HA])
+
 ;;----------------------------------------------------------------------------
 ;; Code iterators
 ;;----------------------------------------------------------------------------
@@ -381,10 +392,20 @@
 (define_mode_attr qhs_zextenddi_cond [(SI "") (HI "&& arm_arch6") (QI "")])
 (define_mode_attr qhs_sextenddi_cond [(SI "") (HI "&& arm_arch6")
 				      (QI "&& arm_arch6")])
-(define_mode_attr qhs_extenddi_op [(SI "s_register_operand")
+(define_mode_attr qhs_zextenddi_op [(SI "s_register_operand")
 				   (HI "nonimmediate_operand")
 				   (QI "nonimmediate_operand")])
-(define_mode_attr qhs_extenddi_cstr [(SI "r") (HI "rm") (QI "rm")])
+(define_mode_attr qhs_extenddi_op [(SI "s_register_operand")
+				   (HI "nonimmediate_operand")
+				   (QI "arm_reg_or_extendqisi_mem_op")])
+(define_mode_attr qhs_extenddi_cstr [(SI "r") (HI "rm") (QI "rUq")])
+(define_mode_attr qhs_zextenddi_cstr [(SI "r") (HI "rm") (QI "rm")])
+
+;; Mode attributes used for fixed-point support.
+(define_mode_attr qaddsub_suf [(V4UQQ "8") (V2UHQ "16") (UQQ "8") (UHQ "16")
+			       (V2UHA "16") (UHA "16")
+			       (V4QQ "8") (V2HQ "16") (QQ "8") (HQ "16")
+			       (V2HA "16") (HA "16") (SQ "") (SA "")])
 
 ;;----------------------------------------------------------------------------
 ;; Code attributes
@@ -403,3 +424,11 @@
 
 ;; Assembler mnemonics for signedness of widening operations.
 (define_code_attr US [(sign_extend "s") (zero_extend "u")])
+
+;; Both kinds of return insn.
+(define_code_iterator returns [return simple_return])
+(define_code_attr return_str [(return "") (simple_return "simple_")])
+(define_code_attr return_simple_p [(return "false") (simple_return "true")])
+(define_code_attr return_cond [(return " && USE_RETURN_INSN (FALSE)")
+			       (simple_return " && use_simple_return_p ()")])
+
diff --git a/gcc/config/arm/lib1funcs.asm b/gcc/config/arm/lib1funcs.asm
index 2e76c01..c1830385 100644
--- a/gcc/config/arm/lib1funcs.asm
+++ b/gcc/config/arm/lib1funcs.asm
@@ -99,7 +99,7 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
 #if defined(__ARM_ARCH_6__) || defined(__ARM_ARCH_6J__) \
 	|| defined(__ARM_ARCH_6K__) || defined(__ARM_ARCH_6Z__) \
 	|| defined(__ARM_ARCH_6ZK__) || defined(__ARM_ARCH_6T2__) \
-	|| defined(__ARM_ARCH_6M__)
+	|| defined(__ARM_ARCH_6M__) || defined(__ARM_ARCH_6SM__)
 # define __ARM_ARCH__ 6
 #endif
 
@@ -113,6 +113,10 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
 #error Unable to determine architecture.
 #endif
 
+#if defined(__ARM_ARCH_6M__) || defined(__ARM_ARCH_6SM__)
+#define __thumb1_only
+#endif
+
 /* There are times when we might prefer Thumb1 code even if ARM code is
    permitted, for example, the code might be smaller, or there might be
    interworking problems with switching to ARM state if interworking is
@@ -121,7 +125,7 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
      && !defined(__thumb2__)		\
      && (!defined(__THUMB_INTERWORK__)	\
 	 || defined (__OPTIMIZE_SIZE__)	\
-	 || defined(__ARM_ARCH_6M__)))
+	 || defined(__thumb1_only)))
 # define __prefer_thumb__
 #endif
 
@@ -302,7 +306,7 @@ LSYM(Lend_fde):
 
 #ifdef __ARM_EABI__
 .macro THUMB_LDIV0 name signed
-#if defined(__ARM_ARCH_6M__)
+#ifdef __thumb1_only
 	.ifc \signed, unsigned
 	cmp	r0, #0
 	beq	1f
@@ -461,7 +465,7 @@ _L__\name:
 
 #else /* !(__INTERWORKING_STUBS__ || __thumb2__) */
 
-#ifdef __ARM_ARCH_6M__
+#ifdef __thumb1_only
 #define EQUIV .thumb_set
 #else
 .macro	ARM_FUNC_START name
@@ -489,7 +493,7 @@ SYM (__\name):
 #endif
 .endm
 
-#ifndef __ARM_ARCH_6M__
+#ifndef __thumb1_only
 .macro	ARM_FUNC_ALIAS new old
 	.globl	SYM (__\new)
 	EQUIV	SYM (__\new), SYM (__\old)
@@ -1312,9 +1316,6 @@ LSYM(Lover12):
 #define ah	r1
 #endif
 
-/* Prevent __aeabi double-word shifts from being produced on SymbianOS.  */
-#ifndef __symbian__
-
 #ifdef L_lshrdi3
 
 	FUNC_START lshrdi3
@@ -1416,16 +1417,14 @@ LSYM(Lover12):
 
 #endif
 
-#endif /* __symbian__ */
-
-#if ((__ARM_ARCH__ > 5) && !defined(__ARM_ARCH_6M__)) \
+#if ((__ARM_ARCH__ > 5) && !defined(__thumb1_only)) \
     || defined(__ARM_ARCH_5E__) || defined(__ARM_ARCH_5TE__) \
     || defined(__ARM_ARCH_5TEJ__)
 #define HAVE_ARM_CLZ 1
 #endif
 
 #ifdef L_clzsi2
-#if defined(__ARM_ARCH_6M__)
+#ifdef __thumb1_only
 FUNC_START clzsi2
 	mov	r1, #28
 	mov	r3, #1
@@ -1486,7 +1485,7 @@ ARM_FUNC_START clzsi2
 #ifdef L_clzdi2
 #if !defined(HAVE_ARM_CLZ)
 
-# if defined(__ARM_ARCH_6M__)
+# ifdef __thumb1_only
 FUNC_START clzdi2
 	push	{r4, lr}
 # else
@@ -1511,7 +1510,7 @@ ARM_FUNC_START clzdi2
 	bl	__clzsi2
 # endif
 2:
-# if defined(__ARM_ARCH_6M__)
+# ifdef __thumb1_only
 	pop	{r4, pc}
 # else
 	RETLDM	r4
@@ -1584,7 +1583,7 @@ ARM_FUNC_START clzdi2
 
 /* Don't bother with the old interworking routines for Thumb-2.  */
 /* ??? Maybe only omit these on "m" variants.  */
-#if !defined(__thumb2__) && !defined(__ARM_ARCH_6M__)
+#if !defined(__thumb2__) && !defined(__thumb1_only)
 
 #if defined L_interwork_call_via_rX
 
@@ -1818,12 +1817,10 @@ LSYM(Lchange_\register):
 
 #endif /* Arch supports thumb.  */
 
-#ifndef __symbian__
-#ifndef __ARM_ARCH_6M__
+#ifndef __thumb1_only
 #include "ieee754-df.S"
 #include "ieee754-sf.S"
 #include "bpabi.S"
-#else /* __ARM_ARCH_6M__ */
+#else /* __thumb1_only */
 #include "bpabi-v6m.S"
-#endif /* __ARM_ARCH_6M__ */
-#endif /* !__symbian__ */
+#endif /* __thumb1_only */
diff --git a/gcc/config/arm/libunwind.S b/gcc/config/arm/libunwind.S
index 48eb592..25fea25 100644
--- a/gcc/config/arm/libunwind.S
+++ b/gcc/config/arm/libunwind.S
@@ -38,8 +38,6 @@
 	.eabi_attribute 25, 1
 #endif /* __ARM_EABI__ */
 
-#ifndef __symbian__
-
 #include "lib1funcs.asm"
 
 .macro UNPREFIX name
@@ -59,7 +57,7 @@
 #endif
 #endif
 
-#ifdef __ARM_ARCH_6M__
+#if defined(__ARM_ARCH_6M__) || defined(__ARM_ARCH_6SM__)
 
 /* r0 points to a 16-word block.  Upload these values to the actual core
    state.  */
@@ -359,5 +357,3 @@ UNWIND_WRAPPER _Unwind_Resume 1
 UNWIND_WRAPPER _Unwind_Resume_or_Rethrow 1
 UNWIND_WRAPPER _Unwind_ForcedUnwind 3
 UNWIND_WRAPPER _Unwind_Backtrace 2
-
-#endif  /* ndef __symbian__ */
diff --git a/gcc/config/arm/neon-testgen.ml b/gcc/config/arm/neon-testgen.ml
index 63fbbbf..08acbeb 100644
--- a/gcc/config/arm/neon-testgen.ml
+++ b/gcc/config/arm/neon-testgen.ml
@@ -257,7 +257,7 @@ let test_intrinsic dir opcode features shape name munge elt_ty =
      intrinsic expands to.  Watch out for any writeback character and
      comments after the instruction.  *)
   let regexps = List.map (fun regexp -> insn_regexp ^ "\\[ \t\\]+" ^ regexp ^
-			  "!?\\(\\[ \t\\]+@\\[a-zA-Z0-9 \\]+\\)?\\n")
+			  "!?\\(\\[ \t\\]+@.*\\)?\\n")
                          (analyze_all_shapes features shape analyze_shape)
   in
     (* Emit file and function prologues.  *)
diff --git a/gcc/config/arm/sfp-machine.h b/gcc/config/arm/sfp-machine.h
index a89d05a..f2d7a37 100644
--- a/gcc/config/arm/sfp-machine.h
+++ b/gcc/config/arm/sfp-machine.h
@@ -99,7 +99,7 @@ typedef int __gcc_CMPtype __attribute__ ((mode (__libgcc_cmp_return__)));
 #define __fixdfdi	__aeabi_d2lz
 #define __fixunsdfdi	__aeabi_d2ulz
 #define __floatdidf	__aeabi_l2d
-#define __extendhfsf2	__gnu_h2f_ieee
-#define __truncsfhf2	__gnu_f2h_ieee
+#define __extendhfsf2	__aeabi_h2f
+#define __truncsfhf2	__aeabi_f2h
 
 #endif /* __ARM_EABI__ */
diff --git a/gcc/config/arm/symbian.h b/gcc/config/arm/symbian.h
index ff233a8..defd3d2 100644
--- a/gcc/config/arm/symbian.h
+++ b/gcc/config/arm/symbian.h
@@ -71,11 +71,6 @@
 #define SUBTARGET_ASM_FLOAT_SPEC \
   "%{!mfpu=*:-mfpu=vfp} %{!mcpu=*:%{!march=*:-march=armv5t}}"
   
-/* SymbianOS provides the BPABI routines in a separate library.
-   Therefore, we do not need to define any of them in libgcc.  */
-#undef RENAME_LIBRARY
-#define RENAME_LIBRARY(GCC_NAME, AEABI_NAME) /* empty */
-
 /* Define the __symbian__ macro.  */
 #undef TARGET_OS_CPP_BUILTINS
 #define TARGET_OS_CPP_BUILTINS()				\
diff --git a/gcc/config/arm/t-arm b/gcc/config/arm/t-arm
index 33d7e19..a75dee3 100644
--- a/gcc/config/arm/t-arm
+++ b/gcc/config/arm/t-arm
@@ -37,7 +37,8 @@ MD_INCLUDES= 	$(srcdir)/config/arm/arm-tune.md \
 		$(srcdir)/config/arm/iwmmxt.md \
 		$(srcdir)/config/arm/vfp.md \
 		$(srcdir)/config/arm/neon.md \
-		$(srcdir)/config/arm/thumb2.md
+		$(srcdir)/config/arm/thumb2.md \
+		$(srcdir)/config/arm/arm-fixed.md
 
 LIB1ASMSRC = arm/lib1funcs.asm
 LIB1ASMFUNCS = _thumb1_case_sqi _thumb1_case_uqi _thumb1_case_shi \
diff --git a/gcc/config/arm/t-arm-softfp b/gcc/config/arm/t-arm-softfp
index f9cace9..dac4300 100644
--- a/gcc/config/arm/t-arm-softfp
+++ b/gcc/config/arm/t-arm-softfp
@@ -22,7 +22,7 @@ softfp_extensions := sfdf
 softfp_truncations := dfsf
 softfp_machine_header := arm/sfp-machine.h
 softfp_exclude_libgcc2 := y
-softfp_wrap_start := '\#ifdef __ARM_ARCH_6M__'
+softfp_wrap_start := '\#if defined(__ARM_ARCH_6M__) || defined(__ARM_ARCH_6SM__)'
 softfp_wrap_end := '\#endif'
 
 # softfp seems to be missing a whole bunch of prototypes.
diff --git a/gcc/config/arm/t-symbian b/gcc/config/arm/t-symbian
index 4a1476f..a6b8ccc 100644
--- a/gcc/config/arm/t-symbian
+++ b/gcc/config/arm/t-symbian
@@ -16,28 +16,6 @@
 # along with GCC; see the file COPYING3.  If not see
 # <http://www.gnu.org/licenses/>.
 
-LIB1ASMFUNCS += _bb_init_func _call_via_rX _interwork_call_via_rX _clzsi2 _clzdi2
-
-# These functions have __aeabi equivalents and will never be called by GCC.  
-# By putting them in LIB1ASMFUNCS, we avoid the standard libgcc2.c code being
-# used -- and we make sure that definitions are not available in lib1funcs.asm,
-# either, so they end up undefined.
-LIB1ASMFUNCS += \
-	_ashldi3 _ashrdi3 _divdi3 _floatdidf _udivmoddi4 _umoddi3 \
-	_udivdi3 _lshrdi3 _moddi3 _muldi3 _negdi2 _cmpdi2 \
-	_fixdfdi _fixsfdi _fixunsdfdi _fixunssfdi _floatdisf \
-	_negdf2 _addsubdf3 _muldivdf3 _cmpdf2 _unorddf2 _fixdfsi _fixunsdfsi \
-	_truncdfsf2 _negsf2 _addsubsf3 _muldivsf3 _cmpsf2 _unordsf2 \
-	_fixsfsi _fixunssfsi
-
-# Include the gcc personality routine
-UNWIND_H = $(srcdir)/config/arm/unwind-arm.h
-LIB2ADDEH = $(srcdir)/unwind-c.c $(srcdir)/config/arm/pr-support.c
-LIB2ADDEHDEP = $(UNWIND_H)
-
-# Include half-float helpers.
-LIB2FUNCS_STATIC_EXTRA = $(srcdir)/config/arm/fp16.c
-
 # Create a multilib for processors with VFP floating-point, and a
 # multilib for those without -- using the soft-float ABI in both
 # cases.  Symbian OS object should be compiled with interworking
@@ -45,6 +23,9 @@ LIB2FUNCS_STATIC_EXTRA = $(srcdir)/config/arm/fp16.c
 MULTILIB_OPTIONS     = mfloat-abi=softfp
 MULTILIB_DIRNAMES    = softfp
 
+LIB2FUNCS_EXTRA += $(srcdir)/config/arm/eabi-memcpy.c \
+  $(srcdir)/config/arm/eabi-memset.c
+
 # There is no C library to link against on Symbian OS -- at least when 
 # building GCC.
 SHLIB_LC = 
diff --git a/gcc/config/arm/thumb2.md b/gcc/config/arm/thumb2.md
index 1b2fb2d..8a4598a 100644
--- a/gcc/config/arm/thumb2.md
+++ b/gcc/config/arm/thumb2.md
@@ -635,16 +635,15 @@
 
 ;; Note: this is not predicable, to avoid issues with linker-generated
 ;; interworking stubs.
-(define_insn "*thumb2_return"
-  [(return)]
-  "TARGET_THUMB2 && USE_RETURN_INSN (FALSE)"
-  "*
-  {
-    return output_return_instruction (const_true_rtx, TRUE, FALSE);
-  }"
+(define_insn "*thumb2_<return_str>return"
+  [(returns)]
+  "TARGET_THUMB2<return_cond>"
+{
+  return output_return_instruction (const_true_rtx, true, false,
+				    <return_simple_p>);
+}
   [(set_attr "type" "load1")
-   (set_attr "length" "12")]
-)
+   (set_attr "length" "12")])
 
 (define_insn_and_split "thumb2_eh_return"
   [(unspec_volatile [(match_operand:SI 0 "s_register_operand" "r")]
@@ -780,26 +779,6 @@
    (set_attr "length" "2")]
 )
 
-(define_insn "divsi3"
-  [(set (match_operand:SI	  0 "s_register_operand" "=r")
-	(div:SI (match_operand:SI 1 "s_register_operand"  "r")
-		(match_operand:SI 2 "s_register_operand"  "r")))]
-  "TARGET_THUMB2 && arm_arch_hwdiv"
-  "sdiv%?\t%0, %1, %2"
-  [(set_attr "predicable" "yes")
-   (set_attr "insn" "sdiv")]
-)
-
-(define_insn "udivsi3"
-  [(set (match_operand:SI	   0 "s_register_operand" "=r")
-	(udiv:SI (match_operand:SI 1 "s_register_operand"  "r")
-		 (match_operand:SI 2 "s_register_operand"  "r")))]
-  "TARGET_THUMB2 && arm_arch_hwdiv"
-  "udiv%?\t%0, %1, %2"
-  [(set_attr "predicable" "yes")
-   (set_attr "insn" "udiv")]
-)
-
 (define_insn "*thumb2_subsi_short"
   [(set (match_operand:SI 0 "low_register_operand" "=l")
 	(minus:SI (match_operand:SI 1 "low_register_operand" "l")
diff --git a/gcc/config/arm/uclinux-eabi.h b/gcc/config/arm/uclinux-eabi.h
index 4455288..8345b41 100644
--- a/gcc/config/arm/uclinux-eabi.h
+++ b/gcc/config/arm/uclinux-eabi.h
@@ -50,6 +50,10 @@
 #undef ARM_DEFAULT_ABI
 #define ARM_DEFAULT_ABI ARM_ABI_AAPCS_LINUX
 
+#undef LINK_GCC_C_SEQUENCE_SPEC
+#define LINK_GCC_C_SEQUENCE_SPEC \
+  "--start-group %G %L --end-group"
+
 /* Clear the instruction cache from `beg' to `end'.  This makes an
    inline system call to SYS_cacheflush.  */
 #undef CLEAR_INSN_CACHE
diff --git a/gcc/config/arm/unwind-arm.c b/gcc/config/arm/unwind-arm.c
index 2c6e004..90d258d 100644
--- a/gcc/config/arm/unwind-arm.c
+++ b/gcc/config/arm/unwind-arm.c
@@ -32,13 +32,18 @@ extern void abort (void);
 typedef unsigned char bool;
 
 typedef struct _ZSt9type_info type_info; /* This names C++ type_info type */
+enum __cxa_type_match_result
+  {
+    ctm_failed = 0,
+    ctm_succeeded = 1,
+    ctm_succeeded_with_ptr_to_base = 2
+  };
 
 void __attribute__((weak)) __cxa_call_unexpected(_Unwind_Control_Block *ucbp);
 bool __attribute__((weak)) __cxa_begin_cleanup(_Unwind_Control_Block *ucbp);
-bool __attribute__((weak)) __cxa_type_match(_Unwind_Control_Block *ucbp,
-					    const type_info *rttip,
-					    bool is_reference,
-					    void **matched_object);
+enum __cxa_type_match_result __attribute__((weak)) __cxa_type_match
+  (_Unwind_Control_Block *ucbp, const type_info *rttip,
+   bool is_reference, void **matched_object);
 
 _Unwind_Ptr __attribute__((weak))
 __gnu_Unwind_Find_exidx (_Unwind_Ptr, int *);
@@ -1107,6 +1112,7 @@ __gnu_unwind_pr_common (_Unwind_State state,
 		      _uw rtti;
 		      bool is_reference = (data[0] & uint32_highbit) != 0;
 		      void *matched;
+		      enum __cxa_type_match_result match_type;
 
 		      /* Check for no-throw areas.  */
 		      if (data[1] == (_uw) -2)
@@ -1118,17 +1124,31 @@ __gnu_unwind_pr_common (_Unwind_State state,
 			{
 			  /* Match a catch specification.  */
 			  rtti = _Unwind_decode_target2 ((_uw) &data[1]);
-			  if (!__cxa_type_match (ucbp, (type_info *) rtti,
-						 is_reference,
-						 &matched))
-			    matched = (void *)0;
+			  match_type = __cxa_type_match (ucbp,
+							 (type_info *) rtti,
+							 is_reference,
+							 &matched);
 			}
+		      else
+			match_type = ctm_succeeded;
 
-		      if (matched)
+		      if (match_type)
 			{
 			  ucbp->barrier_cache.sp =
 			    _Unwind_GetGR (context, R_SP);
-			  ucbp->barrier_cache.bitpattern[0] = (_uw) matched;
+			  // ctm_succeeded_with_ptr_to_base really
+			  // means _c_t_m indirected the pointer
+			  // object.  We have to reconstruct the
+			  // additional pointer layer by using a temporary.
+			  if (match_type == ctm_succeeded_with_ptr_to_base)
+			    {
+			      ucbp->barrier_cache.bitpattern[2]
+				= (_uw) matched;
+			      ucbp->barrier_cache.bitpattern[0]
+				= (_uw) &ucbp->barrier_cache.bitpattern[2];
+			    }
+			  else
+			    ucbp->barrier_cache.bitpattern[0] = (_uw) matched;
 			  ucbp->barrier_cache.bitpattern[1] = (_uw) data;
 			  return _URC_HANDLER_FOUND;
 			}
@@ -1196,8 +1216,6 @@ __gnu_unwind_pr_common (_Unwind_State state,
 		  ucbp->barrier_cache.bitpattern[4] = (_uw) &data[1];
 
 		  if (data[0] & uint32_highbit)
-		    phase2_call_unexpected_after_unwind = 1;
-		  else
 		    {
 		      data += rtti_count + 1;
 		      /* Setup for entry to the handler.  */
@@ -1207,6 +1225,8 @@ __gnu_unwind_pr_common (_Unwind_State state,
 		      _Unwind_SetGR (context, 0, (_uw) ucbp);
 		      return _URC_INSTALL_CONTEXT;
 		    }
+		  else
+		    phase2_call_unexpected_after_unwind = 1;
 		}
 	      if (data[0] & uint32_highbit)
 		data++;
diff --git a/gcc/config/arm/vfp.md b/gcc/config/arm/vfp.md
index 1ac2d0c..5b48204 100644
--- a/gcc/config/arm/vfp.md
+++ b/gcc/config/arm/vfp.md
@@ -83,6 +83,7 @@
   "
   [(set_attr "predicable" "yes")
    (set_attr "type" "*,*,*,*,load1,store1,r_2_f,f_2_r,fcpys,f_loads,f_stores")
+   (set_attr "neon_type" "*,*,*,*,*,*,neon_mcr,neon_mrc,neon_vmov,neon_ldr,neon_str")
    (set_attr "insn" "mov,mov,mvn,mov,*,*,*,*,*,*,*")
    (set_attr "pool_range"     "*,*,*,*,4096,*,*,*,*,1020,*")
    (set_attr "neg_pool_range" "*,*,*,*,4084,*,*,*,*,1008,*")]
@@ -125,6 +126,7 @@
   "
   [(set_attr "predicable" "yes")
    (set_attr "type" "*,*,*,*,load1,load1,store1,store1,r_2_f,f_2_r,fcpys,f_loads,f_stores")
+   (set_attr "neon_type" "*,*,*,*,*,*,*,*,neon_mcr,neon_mrc,neon_vmov,*,*")
    (set_attr "insn" "mov,mov,mvn,mov,*,*,*,*,*,*,*,*,*")
    (set_attr "pool_range"     "*,*,*,*,1020,4096,*,*,*,*,*,1020,*")
    (set_attr "neg_pool_range" "*,*,*,*,   0,   0,*,*,*,*,*,1008,*")]
@@ -134,9 +136,51 @@
 ;; DImode moves
 
 (define_insn "*arm_movdi_vfp"
-  [(set (match_operand:DI 0 "nonimmediate_di_operand" "=r, r,m,w,r,w,w, Uv")
+  [(set (match_operand:DI 0 "nonimmediate_di_operand" "=r, r, m,w,r,w,w, Uv")
 	(match_operand:DI 1 "di_operand"              "rIK,mi,r,r,w,w,Uvi,w"))]
-  "TARGET_ARM && TARGET_HARD_FLOAT && TARGET_VFP
+  "TARGET_ARM && TARGET_HARD_FLOAT && TARGET_VFP && arm_tune != cortexa8
+   && (   register_operand (operands[0], DImode)
+       || register_operand (operands[1], DImode))"
+  "*
+  switch (which_alternative)
+    {
+    case 0: 
+      return \"#\";
+    case 1:
+    case 2:
+      return output_move_double (operands);
+    case 3:
+      return \"fmdrr%?\\t%P0, %Q1, %R1\\t%@ int\";
+    case 4:
+      return \"fmrrd%?\\t%Q0, %R0, %P1\\t%@ int\";
+    case 5:
+      if (TARGET_VFP_SINGLE)
+	return \"fcpys%?\\t%0, %1\\t%@ int\;fcpys%?\\t%p0, %p1\\t%@ int\";
+      else
+	return \"fcpyd%?\\t%P0, %P1\\t%@ int\";
+    case 6: case 7:
+      return output_move_vfp (operands);
+    default:
+      gcc_unreachable ();
+    }
+  "
+  [(set_attr "type" "*,load2,store2,r_2_f,f_2_r,ffarithd,f_loadd,f_stored")
+   (set_attr "neon_type" "*,*,*,neon_mcr_2_mcrr,neon_mrrc,neon_vmov,*,*")
+   (set (attr "length") (cond [(eq_attr "alternative" "0,1,2") (const_int 8)
+			       (eq_attr "alternative" "5")
+				(if_then_else
+				 (eq (symbol_ref "TARGET_VFP_SINGLE") (const_int 1))
+				 (const_int 8)
+				 (const_int 4))]
+			      (const_int 4)))
+   (set_attr "pool_range"     "*,1020,*,*,*,*,1020,*")
+   (set_attr "neg_pool_range" "*,1008,*,*,*,*,1008,*")]
+)
+
+(define_insn "*arm_movdi_vfp_cortexa8"
+  [(set (match_operand:DI 0 "nonimmediate_di_operand" "=r, r,m,w,!r,w,w, Uv")
+	(match_operand:DI 1 "di_operand"              "rIK,mi,r,r,w,w,Uvi,w"))]
+  "TARGET_ARM && TARGET_HARD_FLOAT && TARGET_VFP && arm_tune == cortexa8
    && (   register_operand (operands[0], DImode)
        || register_operand (operands[1], DImode))"
   "*
@@ -163,6 +207,7 @@
     }
   "
   [(set_attr "type" "*,load2,store2,r_2_f,f_2_r,ffarithd,f_loadd,f_stored")
+   (set_attr "neon_type" "*,*,*,neon_mcr_2_mcrr,neon_mrrc,neon_vmov,*,*")
    (set (attr "length") (cond [(eq_attr "alternative" "0,1,2") (const_int 8)
 			       (eq_attr "alternative" "5")
 				(if_then_else
@@ -201,6 +246,7 @@
     }
   "
   [(set_attr "type" "*,load2,store2,r_2_f,f_2_r,ffarithd,f_loadd,f_stored")
+   (set_attr "neon_type" "*,*,*,neon_mcr_2_mcrr,neon_mrrc,neon_vmov,*,*")
    (set (attr "length") (cond [(eq_attr "alternative" "0,1,2") (const_int 8)
 			       (eq_attr "alternative" "5")
 				(if_then_else
@@ -355,6 +401,7 @@
   [(set_attr "predicable" "yes")
    (set_attr "type"
      "r_2_f,f_2_r,fconsts,f_loads,f_stores,load1,store1,fcpys,*")
+   (set_attr "neon_type" "neon_mcr,neon_mrc,*,*,*,*,*,neon_vmov,*")
    (set_attr "insn" "*,*,*,*,*,*,*,*,mov")
    (set_attr "pool_range" "*,*,*,1020,*,4096,*,*,*")
    (set_attr "neg_pool_range" "*,*,*,1008,*,4080,*,*,*")]
@@ -392,6 +439,7 @@
   [(set_attr "predicable" "yes")
    (set_attr "type"
      "r_2_f,f_2_r,fconsts,f_loads,f_stores,load1,store1,fcpys,*")
+   (set_attr "neon_type" "neon_mcr,neon_mrc,*,*,*,*,*,neon_vmov,*")
    (set_attr "insn" "*,*,*,*,*,*,*,*,mov")
    (set_attr "pool_range" "*,*,*,1020,*,4092,*,*,*")
    (set_attr "neg_pool_range" "*,*,*,1008,*,0,*,*,*")]
@@ -401,8 +449,8 @@
 ;; DFmode moves
 
 (define_insn "*movdf_vfp"
-  [(set (match_operand:DF 0 "nonimmediate_soft_df_operand" "=w,?r,w ,r, m,w  ,Uv,w,r")
-	(match_operand:DF 1 "soft_df_operand"		   " ?r,w,Dy,mF,r,UvF,w, w,r"))]
+  [(set (match_operand:DF 0 "nonimmediate_soft_df_operand" "=w,?r,w ,w, r, m,w  ,Uv,w,r")
+	(match_operand:DF 1 "soft_df_operand"		   " ?r,w,Dy,D0,mF,r,UvF,w, w,r"))]
   "TARGET_ARM && TARGET_HARD_FLOAT && TARGET_VFP
    && (   register_operand (operands[0], DFmode)
        || register_operand (operands[1], DFmode))"
@@ -417,16 +465,18 @@
       case 2:
 	gcc_assert (TARGET_VFP_DOUBLE);
         return \"fconstd%?\\t%P0, #%G1\";
-      case 3: case 4:
+      case 3:
+	return \"vmov.i32\\t%P0, #0\";
+      case 4: case 5:
 	return output_move_double (operands);
-      case 5: case 6:
+      case 6: case 7:
 	return output_move_vfp (operands);
-      case 7:
+      case 8:
 	if (TARGET_VFP_SINGLE)
 	  return \"fcpys%?\\t%0, %1\;fcpys%?\\t%p0, %p1\";
 	else
 	  return \"fcpyd%?\\t%P0, %P1\";
-      case 8:
+      case 9:
         return \"#\";
       default:
 	gcc_unreachable ();
@@ -434,9 +484,10 @@
     }
   "
   [(set_attr "type"
-     "r_2_f,f_2_r,fconstd,f_loadd,f_stored,load2,store2,ffarithd,*")
-   (set (attr "length") (cond [(eq_attr "alternative" "3,4,8") (const_int 8)
-			       (eq_attr "alternative" "7")
+     "r_2_f,f_2_r,fconstd,*,f_loadd,f_stored,load2,store2,ffarithd,*")
+   (set_attr "neon_type" "neon_mcr_2_mcrr,neon_mrrc,*,neon_vmov,*,*,*,*,neon_vmov,*")
+   (set (attr "length") (cond [(eq_attr "alternative" "4,5,9") (const_int 8)
+			       (eq_attr "alternative" "8")
 				(if_then_else
 				 (eq (symbol_ref "TARGET_VFP_SINGLE")
 				     (const_int 1))
@@ -444,14 +495,16 @@
 				 (const_int 4))]
 			      (const_int 4)))
    (set_attr "predicable" "yes")
-   (set_attr "pool_range" "*,*,*,1020,*,1020,*,*,*")
-   (set_attr "neg_pool_range" "*,*,*,1008,*,1008,*,*,*")]
+   (set_attr "pool_range" "*,*,*,*,1020,*,1020,*,*,*")
+   (set_attr "neg_pool_range" "*,*,*,*,1008,*,1008,*,*,*")]
 )
 
 (define_insn "*thumb2_movdf_vfp"
-  [(set (match_operand:DF 0 "nonimmediate_soft_df_operand" "=w,?r,w ,r, m,w  ,Uv,w,r")
-	(match_operand:DF 1 "soft_df_operand"		   " ?r,w,Dy,mF,r,UvF,w, w,r"))]
-  "TARGET_THUMB2 && TARGET_HARD_FLOAT && TARGET_VFP"
+  [(set (match_operand:DF 0 "nonimmediate_soft_df_operand" "=w,?r,w ,w,r, m,w  ,Uv,w,r")
+	(match_operand:DF 1 "soft_df_operand"		   " ?r,w,Dy,D0,mF,r,UvF,w, w,r"))]
+  "TARGET_THUMB2 && TARGET_HARD_FLOAT && TARGET_VFP
+   && (   register_operand (operands[0], DFmode)
+       || register_operand (operands[1], DFmode))"
   "*
   {
     switch (which_alternative)
@@ -463,11 +516,13 @@
       case 2:
 	gcc_assert (TARGET_VFP_DOUBLE);
 	return \"fconstd%?\\t%P0, #%G1\";
-      case 3: case 4: case 8:
+      case 3:
+	return \"vmov.i32\\t%P0, #0\";
+      case 4: case 5: case 9:
 	return output_move_double (operands);
-      case 5: case 6:
+      case 6: case 7:
 	return output_move_vfp (operands);
-      case 7:
+      case 8:
 	if (TARGET_VFP_SINGLE)
 	  return \"fcpys%?\\t%0, %1\;fcpys%?\\t%p0, %p1\";
 	else
@@ -478,17 +533,18 @@
     }
   "
   [(set_attr "type"
-     "r_2_f,f_2_r,fconstd,load2,store2,f_loadd,f_stored,ffarithd,*")
-   (set (attr "length") (cond [(eq_attr "alternative" "3,4,8") (const_int 8)
-			       (eq_attr "alternative" "7")
+     "r_2_f,f_2_r,fconstd,*,load2,store2,f_loadd,f_stored,ffarithd,*")
+   (set_attr "neon_type" "neon_mcr_2_mcrr,neon_mrrc,*,neon_vmov,*,*,*,*,neon_vmov,*")
+   (set (attr "length") (cond [(eq_attr "alternative" "4,5,9") (const_int 8)
+			       (eq_attr "alternative" "8")
 				(if_then_else
 				 (eq (symbol_ref "TARGET_VFP_SINGLE")
 				     (const_int 1))
 				 (const_int 8)
 				 (const_int 4))]
 			      (const_int 4)))
-   (set_attr "pool_range" "*,*,*,4096,*,1020,*,*,*")
-   (set_attr "neg_pool_range" "*,*,*,0,*,1008,*,*,*")]
+   (set_attr "pool_range" "*,*,*,*,4096,*,1020,*,*,*")
+   (set_attr "neg_pool_range" "*,*,*,*,0,*,1008,*,*,*")]
 )
 
 
@@ -514,7 +570,8 @@
    fmrs%D3\\t%0, %2\;fmrs%d3\\t%0, %1"
    [(set_attr "conds" "use")
     (set_attr "length" "4,4,8,4,4,8,4,4,8")
-    (set_attr "type" "fcpys,fcpys,fcpys,r_2_f,r_2_f,r_2_f,f_2_r,f_2_r,f_2_r")]
+    (set_attr "type" "fcpys,fcpys,fcpys,r_2_f,r_2_f,r_2_f,f_2_r,f_2_r,f_2_r")
+    (set_attr "neon_type" "neon_vmov,neon_vmov,neon_vmov,neon_mcr,neon_mcr,neon_mcr,neon_mrc,neon_mrc,neon_mrc")]
 )
 
 (define_insn "*thumb2_movsfcc_vfp"
@@ -537,7 +594,8 @@
    ite\\t%D3\;fmrs%D3\\t%0, %2\;fmrs%d3\\t%0, %1"
    [(set_attr "conds" "use")
     (set_attr "length" "6,6,10,6,6,10,6,6,10")
-    (set_attr "type" "fcpys,fcpys,fcpys,r_2_f,r_2_f,r_2_f,f_2_r,f_2_r,f_2_r")]
+    (set_attr "type" "fcpys,fcpys,fcpys,r_2_f,r_2_f,r_2_f,f_2_r,f_2_r,f_2_r")
+    (set_attr "neon_type" "neon_vmov,neon_vmov,neon_vmov,neon_mcr,neon_mcr,neon_mcr,neon_mrc,neon_mrc,neon_mrc")]
 )
 
 (define_insn "*movdfcc_vfp"
@@ -560,7 +618,8 @@
    fmrrd%D3\\t%Q0, %R0, %P2\;fmrrd%d3\\t%Q0, %R0, %P1"
    [(set_attr "conds" "use")
     (set_attr "length" "4,4,8,4,4,8,4,4,8")
-    (set_attr "type" "ffarithd,ffarithd,ffarithd,r_2_f,r_2_f,r_2_f,f_2_r,f_2_r,f_2_r")]
+    (set_attr "type" "ffarithd,ffarithd,ffarithd,r_2_f,r_2_f,r_2_f,f_2_r,f_2_r,f_2_r")
+    (set_attr "neon_type" "neon_vmov,neon_vmov,neon_vmov,neon_mcr_2_mcrr,neon_mcr_2_mcrr,neon_mcr_2_mcrr,neon_mrrc,neon_mrrc,neon_mrrc")]
 )
 
 (define_insn "*thumb2_movdfcc_vfp"
@@ -583,7 +642,8 @@
    ite\\t%D3\;fmrrd%D3\\t%Q0, %R0, %P2\;fmrrd%d3\\t%Q0, %R0, %P1"
    [(set_attr "conds" "use")
     (set_attr "length" "6,6,10,6,6,10,6,6,10")
-    (set_attr "type" "ffarithd,ffarithd,ffarithd,r_2_f,r_2_f,r_2_f,f_2_r,f_2_r,f_2_r")]
+    (set_attr "type" "ffarithd,ffarithd,ffarithd,r_2_f,r_2_f,r_2_f,f_2_r,f_2_r,f_2_r")
+    (set_attr "neon_type" "neon_vmov,neon_vmov,neon_vmov,neon_mcr_2_mcrr,neon_mcr_2_mcrr,neon_mcr_2_mcrr,neon_mrrc,neon_mrrc,neon_mrrc")]
 )
 
 
@@ -712,23 +772,25 @@
 ;; Division insns
 
 (define_insn "*divsf3_vfp"
-  [(set (match_operand:SF	  0 "s_register_operand" "+t")
-	(div:SF (match_operand:SF 1 "s_register_operand" "t")
-		(match_operand:SF 2 "s_register_operand" "t")))]
+  [(set (match_operand:SF	  0 "s_register_operand" "+t,&t")
+	(div:SF (match_operand:SF 1 "s_register_operand" "t,t")
+		(match_operand:SF 2 "s_register_operand" "t,t")))]
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "fdivs%?\\t%0, %1, %2"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "fdivs")]
+   (set_attr "type" "fdivs")
+   (set_attr "arch" "notvfp9,vfp9")]
 )
 
 (define_insn "*divdf3_vfp"
-  [(set (match_operand:DF	  0 "s_register_operand" "+w")
-	(div:DF (match_operand:DF 1 "s_register_operand" "w")
-		(match_operand:DF 2 "s_register_operand" "w")))]
+  [(set (match_operand:DF	  0 "s_register_operand" "+w,&w")
+	(div:DF (match_operand:DF 1 "s_register_operand" "w,w")
+		(match_operand:DF 2 "s_register_operand" "w,w")))]
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fdivd%?\\t%P0, %P1, %P2"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "fdivd")]
+   (set_attr "type" "fdivd")
+   (set_attr "arch" "notvfp9,vfp9")]
 )
 
 
@@ -991,21 +1053,23 @@
 ;; Sqrt insns.
 
 (define_insn "*sqrtsf2_vfp"
-  [(set (match_operand:SF	   0 "s_register_operand" "=t")
-	(sqrt:SF (match_operand:SF 1 "s_register_operand" "t")))]
+  [(set (match_operand:SF	   0 "s_register_operand" "=t,&t")
+	(sqrt:SF (match_operand:SF 1 "s_register_operand" "t,t")))]
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP"
   "fsqrts%?\\t%0, %1"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "fdivs")]
+   (set_attr "type" "fdivs")
+   (set_attr "arch" "notvfp9,vfp9")]
 )
 
 (define_insn "*sqrtdf2_vfp"
-  [(set (match_operand:DF	   0 "s_register_operand" "=w")
-	(sqrt:DF (match_operand:DF 1 "s_register_operand" "w")))]
+  [(set (match_operand:DF	   0 "s_register_operand" "=w,&w")
+	(sqrt:DF (match_operand:DF 1 "s_register_operand" "w,w")))]
   "TARGET_32BIT && TARGET_HARD_FLOAT && TARGET_VFP_DOUBLE"
   "fsqrtd%?\\t%P0, %P1"
   [(set_attr "predicable" "yes")
-   (set_attr "type" "fdivd")]
+   (set_attr "type" "fdivd")
+   (set_attr "arch" "notvfp9,vfp9")]
 )
 
 
diff --git a/gcc/config/bfin/bfin.c b/gcc/config/bfin/bfin.c
index 60cd09ef..2b41ef7 100644
--- a/gcc/config/bfin/bfin.c
+++ b/gcc/config/bfin/bfin.c
@@ -2332,7 +2332,7 @@ bfin_expand_call (rtx retval, rtx fnaddr, rtx callarg1, rtx cookie, int sibcall)
     XVECEXP (pat, 0, n++) = gen_rtx_USE (VOIDmode, picreg);
   XVECEXP (pat, 0, n++) = gen_rtx_USE (VOIDmode, cookie);
   if (sibcall)
-    XVECEXP (pat, 0, n++) = gen_rtx_RETURN (VOIDmode);
+    XVECEXP (pat, 0, n++) = ret_rtx;
   else
     XVECEXP (pat, 0, n++) = gen_rtx_CLOBBER (VOIDmode, retsreg);
   call = emit_call_insn (pat);
diff --git a/gcc/config/cris/cris.c b/gcc/config/cris/cris.c
index aec7cba..2f8f267 100644
--- a/gcc/config/cris/cris.c
+++ b/gcc/config/cris/cris.c
@@ -1788,7 +1788,7 @@ cris_expand_return (bool on_stack)
      we do that until they're fixed.  Currently, all return insns in a
      function must be the same (not really a limiting factor) so we need
      to check that it doesn't change half-way through.  */
-  emit_jump_insn (gen_rtx_RETURN (VOIDmode));
+  emit_jump_insn (ret_rtx);
 
   CRIS_ASSERT (cfun->machine->return_type != CRIS_RETINSN_RET || !on_stack);
   CRIS_ASSERT (cfun->machine->return_type != CRIS_RETINSN_JUMP || on_stack);
diff --git a/gcc/config/fixed-bit.h b/gcc/config/fixed-bit.h
index 562772d..4de3f40 100644
--- a/gcc/config/fixed-bit.h
+++ b/gcc/config/fixed-bit.h
@@ -25,6 +25,10 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
 #ifndef _FIXED_BIT_H
 #define _FIXED_BIT_H
 
+#ifdef LIBGCC2_GNU_PREFIX
+#define LIBGCC2_FIXEDBIT_GNU_PREFIX
+#endif
+
 /* To use this file we need to define one of the following:
    QQ_MODE, UQQ_MODE, HQ_MODE, UHQ_MODE, SQ_MODE, USQ_MODE, DQ_MODE, UDQ_MODE,
    TQ_MODE, UTQ_MODE, HA_MODE, UHA_MODE, SA_MODE, USA_MODE, DA_MODE, UDA_MODE,
@@ -445,35 +449,39 @@ typedef union
 #define IBITS		IBITS2(MODE_NAME)
 #define I_F_BITS	(FBITS + IBITS)
 
-#define FIXED_OP(OP,MODE,NUM)	OP ## MODE ## NUM
-
-#define FIXED_SATURATE1_TEMP(NAME)	FIXED_OP(__saturate1,NAME,)
-#define FIXED_SATURATE2_TEMP(NAME)	FIXED_OP(__saturate2,NAME,)
-#define FIXED_MULHELPER_TEMP(NAME)	FIXED_OP(__mulhelper,NAME,)
-#define FIXED_DIVHELPER_TEMP(NAME)	FIXED_OP(__divhelper,NAME,)
-#define FIXED_ASHLHELPER_TEMP(NAME)	FIXED_OP(__ashlhelper,NAME,)
-#define FIXED_ADD_TEMP(NAME)	FIXED_OP(__add,NAME,3)
-#define FIXED_SSADD_TEMP(NAME)	FIXED_OP(__ssadd,NAME,3)
-#define FIXED_USADD_TEMP(NAME)	FIXED_OP(__usadd,NAME,3)
-#define FIXED_SUB_TEMP(NAME)	FIXED_OP(__sub,NAME,3)
-#define FIXED_SSSUB_TEMP(NAME)	FIXED_OP(__sssub,NAME,3)
-#define FIXED_USSUB_TEMP(NAME)	FIXED_OP(__ussub,NAME,3)
-#define FIXED_MUL_TEMP(NAME)	FIXED_OP(__mul,NAME,3)
-#define FIXED_SSMUL_TEMP(NAME)	FIXED_OP(__ssmul,NAME,3)
-#define FIXED_USMUL_TEMP(NAME)	FIXED_OP(__usmul,NAME,3)
-#define FIXED_DIV_TEMP(NAME)	FIXED_OP(__div,NAME,3)
-#define FIXED_UDIV_TEMP(NAME)	FIXED_OP(__udiv,NAME,3)
-#define FIXED_SSDIV_TEMP(NAME)	FIXED_OP(__ssdiv,NAME,3)
-#define FIXED_USDIV_TEMP(NAME)	FIXED_OP(__usdiv,NAME,3)
-#define FIXED_NEG_TEMP(NAME)	FIXED_OP(__neg,NAME,2)
-#define FIXED_SSNEG_TEMP(NAME)	FIXED_OP(__ssneg,NAME,2)
-#define FIXED_USNEG_TEMP(NAME)	FIXED_OP(__usneg,NAME,2)
-#define FIXED_ASHL_TEMP(NAME)	FIXED_OP(__ashl,NAME,3)
-#define FIXED_ASHR_TEMP(NAME)	FIXED_OP(__ashr,NAME,3)
-#define FIXED_LSHR_TEMP(NAME)	FIXED_OP(__lshr,NAME,3)
-#define FIXED_SSASHL_TEMP(NAME)	FIXED_OP(__ssashl,NAME,3)
-#define FIXED_USASHL_TEMP(NAME)	FIXED_OP(__usashl,NAME,3)
-#define FIXED_CMP_TEMP(NAME)	FIXED_OP(__cmp,NAME,2)
+#ifdef LIBGCC2_FIXEDBIT_GNU_PREFIX
+#define FIXED_OP(OP,MODE,NUM)	__gnu_ ## OP ## MODE ## NUM
+#else
+#define FIXED_OP(OP,MODE,NUM)	__ ## OP ## MODE ## NUM
+#endif
+
+#define FIXED_SATURATE1_TEMP(NAME)	FIXED_OP(saturate1,NAME,)
+#define FIXED_SATURATE2_TEMP(NAME)	FIXED_OP(saturate2,NAME,)
+#define FIXED_MULHELPER_TEMP(NAME)	FIXED_OP(mulhelper,NAME,)
+#define FIXED_DIVHELPER_TEMP(NAME)	FIXED_OP(divhelper,NAME,)
+#define FIXED_ASHLHELPER_TEMP(NAME)	FIXED_OP(ashlhelper,NAME,)
+#define FIXED_ADD_TEMP(NAME)	FIXED_OP(add,NAME,3)
+#define FIXED_SSADD_TEMP(NAME)	FIXED_OP(ssadd,NAME,3)
+#define FIXED_USADD_TEMP(NAME)	FIXED_OP(usadd,NAME,3)
+#define FIXED_SUB_TEMP(NAME)	FIXED_OP(sub,NAME,3)
+#define FIXED_SSSUB_TEMP(NAME)	FIXED_OP(sssub,NAME,3)
+#define FIXED_USSUB_TEMP(NAME)	FIXED_OP(ussub,NAME,3)
+#define FIXED_MUL_TEMP(NAME)	FIXED_OP(mul,NAME,3)
+#define FIXED_SSMUL_TEMP(NAME)	FIXED_OP(ssmul,NAME,3)
+#define FIXED_USMUL_TEMP(NAME)	FIXED_OP(usmul,NAME,3)
+#define FIXED_DIV_TEMP(NAME)	FIXED_OP(div,NAME,3)
+#define FIXED_UDIV_TEMP(NAME)	FIXED_OP(udiv,NAME,3)
+#define FIXED_SSDIV_TEMP(NAME)	FIXED_OP(ssdiv,NAME,3)
+#define FIXED_USDIV_TEMP(NAME)	FIXED_OP(usdiv,NAME,3)
+#define FIXED_NEG_TEMP(NAME)	FIXED_OP(neg,NAME,2)
+#define FIXED_SSNEG_TEMP(NAME)	FIXED_OP(ssneg,NAME,2)
+#define FIXED_USNEG_TEMP(NAME)	FIXED_OP(usneg,NAME,2)
+#define FIXED_ASHL_TEMP(NAME)	FIXED_OP(ashl,NAME,3)
+#define FIXED_ASHR_TEMP(NAME)	FIXED_OP(ashr,NAME,3)
+#define FIXED_LSHR_TEMP(NAME)	FIXED_OP(lshr,NAME,3)
+#define FIXED_SSASHL_TEMP(NAME)	FIXED_OP(ssashl,NAME,3)
+#define FIXED_USASHL_TEMP(NAME)	FIXED_OP(usashl,NAME,3)
+#define FIXED_CMP_TEMP(NAME)	FIXED_OP(cmp,NAME,2)
 
 #if defined (MODE_NAME)
 #if defined (DINT_C_TYPE)
@@ -1146,14 +1154,19 @@ extern FIXED_C_TYPE FIXED_USASHL (FIXED_C_TYPE, word_type);
 #define TO_HAVE_PADDING_BITS	(TO_PADDING_BITS > 0)
 #endif /* TO_TYPE == 4  */
 
-#define FIXED_CONVERT_OP(OP,FROM,TO)	OP ## FROM ## TO
-#define FIXED_CONVERT_OP2(OP,FROM,TO)	OP ## FROM ## TO ## 2
-#define FRACT_TEMP(N1,N2)		FIXED_CONVERT_OP(__fract,N1,N2)
-#define FRACT2_TEMP(N1,N2)		FIXED_CONVERT_OP2(__fract,N1,N2)
-#define SATFRACT_TEMP(N1,N2)		FIXED_CONVERT_OP(__satfract,N1,N2)
-#define SATFRACT2_TEMP(N1,N2)		FIXED_CONVERT_OP2(__satfract,N1,N2)
-#define FRACTUNS_TEMP(N1,N2)		FIXED_CONVERT_OP(__fractuns,N1,N2)
-#define SATFRACTUNS_TEMP(N1,N2)		FIXED_CONVERT_OP(__satfractuns,N1,N2)
+#ifdef LIBGCC2_FIXEDBIT_GNU_PREFIX
+#define FIXED_CONVERT_OP(OP,FROM,TO)	__gnu_ ## OP ## FROM ## TO
+#define FIXED_CONVERT_OP2(OP,FROM,TO)	__gnu_ ## OP ## FROM ## TO ## 2
+#else
+#define FIXED_CONVERT_OP(OP,FROM,TO)	__ ## OP ## FROM ## TO
+#define FIXED_CONVERT_OP2(OP,FROM,TO)	__ ## OP ## FROM ## TO ## 2
+#endif
+#define FRACT_TEMP(N1,N2)		FIXED_CONVERT_OP(fract,N1,N2)
+#define FRACT2_TEMP(N1,N2)		FIXED_CONVERT_OP2(fract,N1,N2)
+#define SATFRACT_TEMP(N1,N2)		FIXED_CONVERT_OP(satfract,N1,N2)
+#define SATFRACT2_TEMP(N1,N2)		FIXED_CONVERT_OP2(satfract,N1,N2)
+#define FRACTUNS_TEMP(N1,N2)		FIXED_CONVERT_OP(fractuns,N1,N2)
+#define SATFRACTUNS_TEMP(N1,N2)		FIXED_CONVERT_OP(satfractuns,N1,N2)
 
 /* Define conversions from fixed-point to fixed-point.  */
 #if FROM_TYPE == 4 && TO_TYPE == 4
diff --git a/gcc/config/frv/t-linux b/gcc/config/frv/t-linux
index 13eb536..5b09451 100644
--- a/gcc/config/frv/t-linux
+++ b/gcc/config/frv/t-linux
@@ -29,5 +29,5 @@ EXTRA_MULTILIB_PARTS =
 CRTSTUFF_T_CFLAGS = -fPIC
 TARGET_LIBGCC2_CFLAGS = -fPIC
 
-SHLIB_MAPFILES = $(srcdir)/libgcc-std.ver \
+SHLIB_MAPFILES = $$(libgcc_objdir)/libgcc-std.ver \
 		 $(srcdir)/config/frv/libgcc-frv.ver
diff --git a/gcc/config/h8300/h8300.c b/gcc/config/h8300/h8300.c
index 55acfff..23463c3 100644
--- a/gcc/config/h8300/h8300.c
+++ b/gcc/config/h8300/h8300.c
@@ -702,7 +702,7 @@ h8300_push_pop (int regno, int nregs, bool pop_p, bool return_p)
   /* Add the return instruction.  */
   if (return_p)
     {
-      RTVEC_ELT (vec, i) = gen_rtx_RETURN (VOIDmode);
+      RTVEC_ELT (vec, i) = ret_rtx;
       i++;
     }
 
@@ -986,7 +986,7 @@ h8300_expand_epilogue (void)
     }
 
   if (!returned_p)
-    emit_jump_insn (gen_rtx_RETURN (VOIDmode));
+    emit_jump_insn (ret_rtx);
 }
 
 /* Return nonzero if the current function is an interrupt
diff --git a/gcc/config/i386/i386.c b/gcc/config/i386/i386.c
index c43d3ed..99c8c13 100644
--- a/gcc/config/i386/i386.c
+++ b/gcc/config/i386/i386.c
@@ -11286,13 +11286,13 @@ ix86_expand_epilogue (int style)
 
 	  pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,
 				     popc, -1, true);
-	  emit_jump_insn (gen_return_indirect_internal (ecx));
+	  emit_jump_insn (gen_simple_return_indirect_internal (ecx));
 	}
       else
-	emit_jump_insn (gen_return_pop_internal (popc));
+	emit_jump_insn (gen_simple_return_pop_internal (popc));
     }
   else
-    emit_jump_insn (gen_return_internal ());
+    emit_jump_insn (gen_simple_return_internal ());
 
   /* Restore the state back to the state from the prologue,
      so that it's correct for the next epilogue.  */
@@ -30255,7 +30255,7 @@ ix86_pad_returns (void)
       rtx prev;
       bool replace = false;
 
-      if (!JUMP_P (ret) || GET_CODE (PATTERN (ret)) != RETURN
+      if (!JUMP_P (ret) || !ANY_RETURN_P (PATTERN (ret))
 	  || optimize_bb_for_size_p (bb))
 	continue;
       for (prev = PREV_INSN (ret); prev; prev = PREV_INSN (prev))
@@ -30285,7 +30285,10 @@ ix86_pad_returns (void)
 	}
       if (replace)
 	{
-	  emit_jump_insn_before (gen_return_internal_long (), ret);
+	  if (PATTERN (ret) == ret_rtx)
+	    emit_jump_insn_before (gen_return_internal_long (), ret);
+	  else
+	    emit_jump_insn_before (gen_simple_return_internal_long (), ret);
 	  delete_insn (ret);
 	}
     }
@@ -30306,7 +30309,7 @@ ix86_count_insn_bb (basic_block bb)
     {
       /* Only happen in exit blocks.  */
       if (JUMP_P (insn)
-	  && GET_CODE (PATTERN (insn)) == RETURN)
+	  && ANY_RETURN_P (PATTERN (insn)))
 	break;
 
       if (NONDEBUG_INSN_P (insn)
@@ -30379,7 +30382,7 @@ ix86_pad_short_function (void)
   FOR_EACH_EDGE (e, ei, EXIT_BLOCK_PTR->preds)
     {
       rtx ret = BB_END (e->src);
-      if (JUMP_P (ret) && GET_CODE (PATTERN (ret)) == RETURN)
+      if (JUMP_P (ret) && ANY_RETURN_P (PATTERN (ret)))
 	{
 	  int insn_count = ix86_count_insn (e->src);
 
diff --git a/gcc/config/i386/i386.md b/gcc/config/i386/i386.md
index 3a27ca4..86938ae 100644
--- a/gcc/config/i386/i386.md
+++ b/gcc/config/i386/i386.md
@@ -11687,24 +11687,29 @@
   ""
   [(set_attr "length" "0")])
 
+(define_code_iterator returns [return simple_return])
+(define_code_attr return_str [(return "") (simple_return "simple_")])
+(define_code_attr return_cond [(return "ix86_can_use_return_insn_p ()")
+			       (simple_return "")])
+
 ;; Insn emitted into the body of a function to return from a function.
 ;; This is only done if the function's epilogue is known to be simple.
 ;; See comments for ix86_can_use_return_insn_p in i386.c.
 
-(define_expand "return"
-  [(return)]
-  "ix86_can_use_return_insn_p ()"
+(define_expand "<return_str>return"
+  [(returns)]
+  "<return_cond>"
 {
   if (crtl->args.pops_args)
     {
       rtx popc = GEN_INT (crtl->args.pops_args);
-      emit_jump_insn (gen_return_pop_internal (popc));
+      emit_jump_insn (gen_<return_str>return_pop_internal (popc));
       DONE;
     }
 })
 
-(define_insn "return_internal"
-  [(return)]
+(define_insn "<return_str>return_internal"
+  [(returns)]
   "reload_completed"
   "ret"
   [(set_attr "length" "1")
@@ -11715,8 +11720,8 @@
 ;; Used by x86_machine_dependent_reorg to avoid penalty on single byte RET
 ;; instruction Athlon and K8 have.
 
-(define_insn "return_internal_long"
-  [(return)
+(define_insn "<return_str>return_internal_long"
+  [(returns)
    (unspec [(const_int 0)] UNSPEC_REP)]
   "reload_completed"
   "rep\;ret"
@@ -11726,8 +11731,8 @@
    (set_attr "prefix_rep" "1")
    (set_attr "modrm" "0")])
 
-(define_insn "return_pop_internal"
-  [(return)
+(define_insn "<return_str>return_pop_internal"
+  [(returns)
    (use (match_operand:SI 0 "const_int_operand" ""))]
   "reload_completed"
   "ret\t%0"
@@ -11736,8 +11741,8 @@
    (set_attr "length_immediate" "2")
    (set_attr "modrm" "0")])
 
-(define_insn "return_indirect_internal"
-  [(return)
+(define_insn "<return_str>return_indirect_internal"
+  [(returns)
    (use (match_operand:SI 0 "register_operand" "r"))]
   "reload_completed"
   "jmp\t%A0"
diff --git a/gcc/config/i386/i386elf.h b/gcc/config/i386/i386elf.h
index 79d7b95..c3c58ce 100644
--- a/gcc/config/i386/i386elf.h
+++ b/gcc/config/i386/i386elf.h
@@ -20,10 +20,6 @@ You should have received a copy of the GNU General Public License
 along with GCC; see the file COPYING3.  If not see
 <http://www.gnu.org/licenses/>.  */
 
-/* Use stabs instead of DWARF debug format.  */
-#undef  PREFERRED_DEBUGGING_TYPE
-#define PREFERRED_DEBUGGING_TYPE DBX_DEBUG
-
 #define TARGET_VERSION fprintf (stderr, " (i386 bare ELF target)");
 
 /* The ELF ABI for the i386 says that records and unions are returned
@@ -36,12 +32,9 @@ along with GCC; see the file COPYING3.  If not see
 #undef CPP_SPEC
 #define CPP_SPEC ""
 
-#define ENDFILE_SPEC "crtend.o%s"
+#define ENDFILE_SPEC "crtend.o%s crtn.o%s"
 
-#define STARTFILE_SPEC "%{!shared: \
-			 %{!symbolic: \
-			  %{pg:gcrt0.o%s}%{!pg:%{p:mcrt0.o%s}%{!p:crt0.o%s}}}}\
-			crtbegin.o%s"
+#define STARTFILE_SPEC "crtbegin.o%s crti.o%s"
 
 #undef DBX_REGISTER_NUMBER
 #define DBX_REGISTER_NUMBER(n) \
diff --git a/gcc/config/i386/t-cygming b/gcc/config/i386/t-cygming
index ccae237..b41cb80 100644
--- a/gcc/config/i386/t-cygming
+++ b/gcc/config/i386/t-cygming
@@ -106,4 +106,4 @@ SHLIB_MKMAP = $(srcdir)/mkmap-flat.awk
 # We'd like to use SHLIB_SONAME here too, but shlib_base_name
 # does not get substituted before mkmap-flat.awk is run.
 SHLIB_MKMAP_OPTS = -v pe_dll=libgcc_s_$(EH_MODEL)-$(SHLIB_SOVERSION)$(SHLIB_EXT)
-SHLIB_MAPFILES = $(srcdir)/libgcc-std.ver
+SHLIB_MAPFILES = $$(libgcc_objdir)/libgcc-std.ver
diff --git a/gcc/config/i386/t-i386elf b/gcc/config/i386/t-i386elf
index 9560d90..e4c55cb 100644
--- a/gcc/config/i386/t-i386elf
+++ b/gcc/config/i386/t-i386elf
@@ -1,4 +1,17 @@
 # For svr4 we build crtbegin.o and crtend.o which serve to add begin and
 # end labels to the .ctors and .dtors section when we link using gcc.
 
-EXTRA_PARTS=crtbegin.o crtend.o
+EXTRA_PARTS=crtbegin.o crtend.o crti.o crtn.o
+
+#
+# Also build crti.o and ctrn.o to provide _init/_fini.
+# The x68 Solaris implementation works here.
+#
+
+$(T)crti.o: $(srcdir)/config/i386/sol2-ci.asm $(GCC_PASSES)
+	sed -e '/^!/d' <$(srcdir)/config/i386/sol2-ci.asm >crti.s
+	$(GCC_FOR_TARGET) -c -o $(T)crti.o crti.s
+$(T)crtn.o: $(srcdir)/config/i386/sol2-cn.asm $(GCC_PASSES)
+	sed -e '/^!/d' <$(srcdir)/config/i386/sol2-cn.asm >crtn.s
+	$(GCC_FOR_TARGET) -c -o $(T)crtn.o crtn.s
+
diff --git a/gcc/config/i386/t-linux b/gcc/config/i386/t-linux
index 76e3f64..f05cbd3 100644
--- a/gcc/config/i386/t-linux
+++ b/gcc/config/i386/t-linux
@@ -1,7 +1,7 @@
 # On 64bit we do not need any exports for glibc for 64-bit libgcc_s.
 # Need to support TImode for x86.  Override the settings from
 # t-slibgcc-elf-ver and t-linux
-SHLIB_MAPFILES = $(srcdir)/libgcc-std.ver \
+SHLIB_MAPFILES = $$(libgcc_objdir)/libgcc-std.ver \
 		 $(srcdir)/config/i386/libgcc-glibc.ver
 
 ifneq (,$(findstring -linux,$(target)))
diff --git a/gcc/config/m68k/m68k-devices.def b/gcc/config/m68k/m68k-devices.def
index 4838fb0..41e11c8 100644
--- a/gcc/config/m68k/m68k-devices.def
+++ b/gcc/config/m68k/m68k-devices.def
@@ -84,10 +84,16 @@ M68K_DEVICE ("cpu32", cpu32,    "cpu32", "cpu32", cpu32,    isa_cpu32, FL_MMU)
 /* For historical reasons, the 51 multilib is named 51qe.  */
 M68K_DEVICE ("51",    mcf51,    "51",    "51qe",  cfv1,     isa_c,     FL_CF_USP)
 M68K_DEVICE ("51ac",  mcf51ac,  "51",    "51qe",  cfv1,     isa_c,     FL_CF_USP)
+M68K_DEVICE ("51ag",  mcf51ag,  "51",    "51qe",  cfv1,     isa_c,     FL_CF_USP)
 M68K_DEVICE ("51cn",  mcf51cn,  "51",    "51qe",  cfv1,     isa_c,     FL_CF_USP)
 M68K_DEVICE ("51em",  mcf51em,  "51",    "51qe",  cfv1,     isa_c,     FL_CF_USP | FL_CF_MAC)
+M68K_DEVICE ("51je",  mcf51je,  "51",    "51qe",  cfv1,     isa_c,     FL_CF_USP | FL_CF_MAC)
+M68K_DEVICE ("51jf",  mcf51jf,  "51",    "51qe",  cfv1,     isa_c,     FL_CF_USP | FL_CF_EMAC)
+M68K_DEVICE ("51jg",  mcf51jg,  "51",    "51qe",  cfv1,     isa_c,     FL_CF_USP | FL_CF_EMAC)
 M68K_DEVICE ("51jm",  mcf51jm,  "51",    "51qe",  cfv1,     isa_c,     FL_CF_USP)
+M68K_DEVICE ("51mm",  mcf51mm,  "51",    "51qe",  cfv1,     isa_c,     FL_CF_USP | FL_CF_MAC)
 M68K_DEVICE ("51qe",  mcf51qe,  "51",    "51qe",  cfv1,     isa_c,     FL_CF_USP)
+M68K_DEVICE ("51qm",  mcf51qm,  "51",    "51qe",  cfv1,     isa_c,     FL_CF_USP | FL_CF_EMAC)
 
 /* ColdFire CFV2 processors.  */
 M68K_DEVICE ("5202",  mcf5202,  "5206",  "5206",  cfv2,     isa_a,     0)
diff --git a/gcc/config/m68k/m68k-protos.h b/gcc/config/m68k/m68k-protos.h
index ad02026..fb8f718 100644
--- a/gcc/config/m68k/m68k-protos.h
+++ b/gcc/config/m68k/m68k-protos.h
@@ -54,7 +54,7 @@ extern void print_operand (FILE *, rtx, int);
 extern bool m68k_output_addr_const_extra (FILE *, rtx);
 extern void notice_update_cc (rtx, rtx);
 extern bool m68k_legitimate_base_reg_p (rtx, bool);
-extern bool m68k_legitimate_index_reg_p (rtx, bool);
+extern bool m68k_legitimate_index_reg_p (enum machine_mode, rtx, bool);
 extern bool m68k_illegitimate_symbolic_constant_p (rtx);
 extern bool m68k_matches_q_p (rtx);
 extern bool m68k_matches_u_p (rtx);
diff --git a/gcc/config/m68k/m68k.c b/gcc/config/m68k/m68k.c
index e5bd011..1fe6e20 100644
--- a/gcc/config/m68k/m68k.c
+++ b/gcc/config/m68k/m68k.c
@@ -1397,7 +1397,7 @@ m68k_expand_epilogue (bool sibcall_p)
 			   EH_RETURN_STACKADJ_RTX));
 
   if (!sibcall_p)
-    emit_jump_insn (gen_rtx_RETURN (VOIDmode));
+    emit_jump_insn (ret_rtx);
 }
 
 /* Return true if X is a valid comparison operator for the dbcc 
@@ -1917,15 +1917,16 @@ m68k_legitimate_base_reg_p (rtx x, bool strict_p)
    whether we need strict checking.  */
 
 bool
-m68k_legitimate_index_reg_p (rtx x, bool strict_p)
+m68k_legitimate_index_reg_p (enum machine_mode mode, rtx x, bool strict_p)
 {
   if (!strict_p && GET_CODE (x) == SUBREG)
     x = SUBREG_REG (x);
 
   return (REG_P (x)
 	  && (strict_p
-	      ? REGNO_OK_FOR_INDEX_P (REGNO (x))
-	      : REGNO_OK_FOR_INDEX_NONSTRICT_P (REGNO (x))));
+	      ? REGNO_MODE_OK_FOR_INDEX_P (REGNO (x), mode)
+	      : (MODE_OK_FOR_INDEX_P (mode)
+		 && REGNO_OK_FOR_INDEX_NONSTRICT_P (REGNO (x)))));
 }
 
 /* Return true if X is a legitimate index expression for a (d8,An,Xn) or
@@ -1933,7 +1934,8 @@ m68k_legitimate_index_reg_p (rtx x, bool strict_p)
    ADDRESS if so.  STRICT_P says whether we need strict checking.  */
 
 static bool
-m68k_decompose_index (rtx x, bool strict_p, struct m68k_address *address)
+m68k_decompose_index (enum machine_mode mode, rtx x, bool strict_p,
+		      struct m68k_address *address)
 {
   int scale;
 
@@ -1957,7 +1959,7 @@ m68k_decompose_index (rtx x, bool strict_p, struct m68k_address *address)
       && GET_MODE (XEXP (x, 0)) == HImode)
     x = XEXP (x, 0);
 
-  if (m68k_legitimate_index_reg_p (x, strict_p))
+  if (m68k_legitimate_index_reg_p (mode, x, strict_p))
     {
       address->scale = scale;
       address->index = x;
@@ -2111,7 +2113,7 @@ m68k_decompose_address (enum machine_mode mode, rtx x,
      accesses to unplaced labels in other cases.  */
   if (GET_CODE (x) == PLUS
       && m68k_jump_table_ref_p (XEXP (x, 1))
-      && m68k_decompose_index (XEXP (x, 0), strict_p, address))
+      && m68k_decompose_index (mode, XEXP (x, 0), strict_p, address))
     {
       address->offset = XEXP (x, 1);
       return true;
@@ -2143,7 +2145,7 @@ m68k_decompose_address (enum machine_mode mode, rtx x,
 	 worse code.  */
       if (address->offset
 	  && symbolic_operand (address->offset, VOIDmode)
-	  && m68k_decompose_index (x, strict_p, address))
+	  && m68k_decompose_index (mode, x, strict_p, address))
 	return true;
     }
   else
@@ -2162,14 +2164,14 @@ m68k_decompose_address (enum machine_mode mode, rtx x,
   if (GET_CODE (x) == PLUS)
     {
       if (m68k_legitimate_base_reg_p (XEXP (x, 0), strict_p)
-	  && m68k_decompose_index (XEXP (x, 1), strict_p, address))
+	  && m68k_decompose_index (mode, XEXP (x, 1), strict_p, address))
 	{
 	  address->base = XEXP (x, 0);
 	  return true;
 	}
 
       if (m68k_legitimate_base_reg_p (XEXP (x, 1), strict_p)
-	  && m68k_decompose_index (XEXP (x, 0), strict_p, address))
+	  && m68k_decompose_index (mode, XEXP (x, 0), strict_p, address))
 	{
 	  address->base = XEXP (x, 1);
 	  return true;
@@ -4302,7 +4304,8 @@ notice_update_cc (rtx exp, rtx insn)
       && GET_MODE_CLASS (GET_MODE (XEXP (cc_status.value2, 0))) == MODE_FLOAT)
     {
       cc_status.flags = CC_IN_68881;
-      if (!FP_REG_P (XEXP (cc_status.value2, 0)))
+      if (!FP_REG_P (XEXP (cc_status.value2, 0))
+	  && FP_REG_P (XEXP (cc_status.value2, 1)))
 	cc_status.flags |= CC_REVERSED;
     }
 }
@@ -6175,7 +6178,14 @@ m68k_sched_variable_issue (FILE *sched_dump ATTRIBUTE_UNUSED,
 	  gcc_unreachable ();
 	}
 
-      gcc_assert (insn_size <= sched_ib.filled);
+      if (insn_size > sched_ib.filled)
+	/* Scheduling for register pressure does not always take DFA into
+	   account.  Workaround instruction buffer not being filled enough.  */
+	{
+	  gcc_assert (sched_pressure_p);
+	  insn_size = sched_ib.filled;
+	}
+
       --can_issue_more;
     }
   else if (GET_CODE (PATTERN (insn)) == ASM_INPUT
@@ -6341,6 +6351,8 @@ m68k_sched_md_init (FILE *sched_dump ATTRIBUTE_UNUSED,
     /* haifa-sched.c: schedule_block () calls advance_cycle () just before
        the first cycle.  Workaround that.  */
     sched_ib.filled = -2;
+  /* Undo attempts by sched-rgn to save states across block boundaries.  */
+  state_reset (curr_state);
 }
 
 /* Implementation of targetm.sched.dfa_pre_advance_cycle () hook.
diff --git a/gcc/config/rx/rx.c b/gcc/config/rx/rx.c
index 392e2ff..5cc9a7f 100644
--- a/gcc/config/rx/rx.c
+++ b/gcc/config/rx/rx.c
@@ -1566,7 +1566,7 @@ gen_rx_rtsd_vector (unsigned int adjust, unsigned int low, unsigned int high)
 				: plus_constant (stack_pointer_rtx,
 						 i * UNITS_PER_WORD)));
 
-  XVECEXP (vector, 0, count - 1) = gen_rtx_RETURN (VOIDmode);
+  XVECEXP (vector, 0, count - 1) = ret_rtx;
 
   return vector;
 }
diff --git a/gcc/config/s390/s390.c b/gcc/config/s390/s390.c
index 9b275b0..d9d3aa9 100644
--- a/gcc/config/s390/s390.c
+++ b/gcc/config/s390/s390.c
@@ -4856,7 +4856,7 @@ s390_expand_cs_hqi (enum machine_mode mode, rtx target, rtx mem, rtx cmp, rtx ne
   if (ac.aligned && MEM_P (cmp))
     {
       cmpv = force_reg (SImode, val);
-      store_bit_field (cmpv, GET_MODE_BITSIZE (mode), 0, SImode, cmp);
+      store_bit_field (cmpv, GET_MODE_BITSIZE (mode), 0, false, SImode, cmp);
     }
   else
     cmpv = force_reg (SImode, expand_simple_binop (SImode, IOR, cmp, val,
@@ -4864,7 +4864,8 @@ s390_expand_cs_hqi (enum machine_mode mode, rtx target, rtx mem, rtx cmp, rtx ne
   if (ac.aligned && MEM_P (new_rtx))
     {
       newv = force_reg (SImode, val);
-      store_bit_field (newv, GET_MODE_BITSIZE (mode), 0, SImode, new_rtx);
+      store_bit_field (newv, GET_MODE_BITSIZE (mode), 0, false, SImode,
+		       new_rtx);
     }
   else
     newv = force_reg (SImode, expand_simple_binop (SImode, IOR, new_rtx, val,
@@ -4941,7 +4942,8 @@ s390_expand_atomic (enum machine_mode mode, enum rtx_code code,
       /* FALLTHRU */
     case SET:
       if (ac.aligned && MEM_P (val))
-	store_bit_field (new_rtx, GET_MODE_BITSIZE (mode), 0, SImode, val);
+	store_bit_field (new_rtx, GET_MODE_BITSIZE (mode), 0, false, SImode,
+			 val);
       else
 	{
 	  new_rtx = expand_simple_binop (SImode, AND, new_rtx, ac.modemaski,
@@ -8479,7 +8481,7 @@ s390_emit_epilogue (bool sibcall)
 
       p = rtvec_alloc (2);
 
-      RTVEC_ELT (p, 0) = gen_rtx_RETURN (VOIDmode);
+      RTVEC_ELT (p, 0) = ret_rtx;
       RTVEC_ELT (p, 1) = gen_rtx_USE (VOIDmode, return_reg);
       emit_jump_insn (gen_rtx_PARALLEL (VOIDmode, p));
     }
@@ -8736,7 +8738,7 @@ s390_promote_function_mode (const_tree type, enum machine_mode mode,
   if (INTEGRAL_MODE_P (mode)
       && GET_MODE_SIZE (mode) < UNITS_PER_LONG)
     {
-      if (POINTER_TYPE_P (type))
+      if (type != NULL_TREE && POINTER_TYPE_P (type))
 	*punsignedp = POINTERS_EXTEND_UNSIGNED;
       return Pmode;
     }
diff --git a/gcc/config/sh/constraints.md b/gcc/config/sh/constraints.md
index 6b0e5d2..dbc4bbe 100644
--- a/gcc/config/sh/constraints.md
+++ b/gcc/config/sh/constraints.md
@@ -75,6 +75,9 @@
 (define_register_constraint "t" "T_REGS"
   "T register.")
 
+(define_register_constraint "u" "NON_SP_REGS"
+  "Non-stack-pointer register.")
+
 (define_register_constraint "w" "FP0_REGS"
   "Floating-point register 0.")
diff --git a/gcc/config/t-slibgcc-sld b/gcc/config/t-slibgcc-sld
index 3a343f5..013a0d7 100644
--- a/gcc/config/t-slibgcc-sld
+++ b/gcc/config/t-slibgcc-sld
@@ -47,4 +47,4 @@ SHLIB_INSTALL = \
 	$(LN_S) $(SHLIB_SONAME) \
 	  $$(DESTDIR)$$(slibdir)$(SHLIB_SLIBDIR_QUAL)/$(SHLIB_SOLINK)
 SHLIB_MKMAP = $(srcdir)/mkmap-symver.awk
-SHLIB_MAPFILES = $(srcdir)/libgcc-std.ver
+SHLIB_MAPFILES = $$(libgcc_objdir)/libgcc-std.ver
diff --git a/gcc/config/t-sysroot-suffix b/gcc/config/t-sysroot-suffix
index 08b4f94..5ebd9f0 100644
--- a/gcc/config/t-sysroot-suffix
+++ b/gcc/config/t-sysroot-suffix
@@ -3,5 +3,5 @@
 sysroot-suffix.h: $(srcdir)/config/print-sysroot-suffix.sh
 	$(SHELL) $(srcdir)/config/print-sysroot-suffix.sh \
 	  "$(MULTILIB_OSDIRNAMES)" "$(MULTILIB_OPTIONS)" \
-	  "$(MULTILIB_MATCHES)" > tmp-sysroot-suffix.h
+	  "$(MULTILIB_MATCHES)" "$(MULTILIB_ALIASES)" > tmp-sysroot-suffix.h
 	mv tmp-sysroot-suffix.h $@
diff --git a/gcc/config/v850/v850.c b/gcc/config/v850/v850.c
index d75f88c..bb32613 100644
--- a/gcc/config/v850/v850.c
+++ b/gcc/config/v850/v850.c
@@ -1886,7 +1886,7 @@ expand_epilogue (void)
 	  int offset;
 	  restore_all = gen_rtx_PARALLEL (VOIDmode,
 					  rtvec_alloc (num_restore + 2));
-	  XVECEXP (restore_all, 0, 0) = gen_rtx_RETURN (VOIDmode);
+	  XVECEXP (restore_all, 0, 0) = ret_rtx;
 	  XVECEXP (restore_all, 0, 1)
 	    = gen_rtx_SET (VOIDmode, stack_pointer_rtx,
 			    gen_rtx_PLUS (Pmode,
diff --git a/gcc/config/vx-common.h b/gcc/config/vx-common.h
index 6a6d109..f1aad2e 100644
--- a/gcc/config/vx-common.h
+++ b/gcc/config/vx-common.h
@@ -92,3 +92,6 @@ along with GCC; see the file COPYING3.  If not see
 /* We occasionally need to distinguish between the VxWorks variants.  */
 #define VXWORKS_KIND_NORMAL  1
 #define VXWORKS_KIND_AE      2
+
+/* Enable get_feature license checking.  */
+#define TARGET_FLEXLM
diff --git a/gcc/configure b/gcc/configure
index c8caff2..6542f2f 100755
--- a/gcc/configure
+++ b/gcc/configure
@@ -755,6 +755,7 @@ SET_MAKE
 REPORT_BUGS_TEXI
 REPORT_BUGS_TO
 PKGVERSION
+EGLIBC_CONFIGS
 CONFIGURE_SPECS
 CROSS_SYSTEM_HEADER_DIR
 TARGET_SYSTEM_ROOT_DEFINE
@@ -801,6 +802,7 @@ target_subdir
 host_subdir
 build_subdir
 build_libsubdir
+licensedir
 target_noncanonical
 target_os
 target_vendor
@@ -856,6 +858,9 @@ ac_subst_files='option_includes
 language_hooks'
 ac_user_opts='
 enable_option_checking
+with_csl_license_version
+with_license
+with_csl_license_feature
 with_build_libsubdir
 with_local_prefix
 with_gxx_include_dir
@@ -886,6 +891,7 @@ enable_shared
 with_build_sysroot
 with_sysroot
 with_specs
+with_eglibc_configs
 with_pkgversion
 with_bugurl
 enable_languages
@@ -915,6 +921,7 @@ with_system_zlib
 enable_maintainer_mode
 enable_version_specific_runtime_libs
 with_slibdir
+enable_poison_system_directories
 enable_plugin
 enable_libquadmath_support
 '
@@ -1627,6 +1634,8 @@ Optional Features:
   --enable-version-specific-runtime-libs
                           specify that runtime libraries should be
                           installed in a compiler-specific directory
+  --enable-poison-system-directories
+                          warn for use of native system header directories
   --enable-plugin         enable plugin support
   --disable-libquadmath-support
                           disable libquadmath support for Fortran
@@ -1634,6 +1643,11 @@ Optional Features:
 Optional Packages:
   --with-PACKAGE[=ARG]    use PACKAGE [ARG=yes]
   --without-PACKAGE       do not use PACKAGE (same as --with-PACKAGE=no)
+  --with-csl-license-version=VERSION
+                          Use VERSION to communicate with the license manager
+  --with-license          the path to the installed license component
+  --with-csl-license-feature=FEATURE
+                          Use FEATURE to communicate with the license manager
   --with-build-libsubdir=DIR  Directory where to find libraries for build system
   --with-local-prefix=DIR specifies directory to put local include
   --with-gxx-include-dir=DIR
@@ -1652,6 +1666,8 @@ Optional Packages:
                           use sysroot as the system root during the build
   --with-sysroot=DIR Search for usr/lib, usr/include, et al, within DIR.
   --with-specs=SPECS      add SPECS to driver command-line processing
+  --with-eglibc-configs=CONFIGS
+                          build multilibs for these EGLIBC configurations
   --with-pkgversion=PKG   Use PKG in the version string in place of "GCC"
   --with-bugurl=URL       Direct users to URL to report a bug
   --with-multilib-list    Select multilibs (SH only)
@@ -3163,6 +3179,67 @@ esac
 
 
 
+
+
+# Check whether --with-csl-license-version was given.
+if test "${with_csl_license_version+set}" = set; then :
+  withval=$with_csl_license_version; case "$withval" in
+      (yes) as_fn_error "license version not specified" "$LINENO" 5 ;;
+      (no)  CSL_LICENSE_VERSION="" ;;
+      (*)   CSL_LICENSE_VERSION="$withval" ;;
+     esac
+else
+  CSL_LICENSE_VERSION=""
+
+fi
+
+  if test x"$CSL_LICENSE_VERSION" != x; then
+
+cat >>confdefs.h <<_ACEOF
+#define CSL_LICENSE_VERSION "$CSL_LICENSE_VERSION"
+_ACEOF
+
+  fi
+
+
+
+# Check whether --with-license was given.
+if test "${with_license+set}" = set; then :
+  withval=$with_license; case "$withval" in
+     (yes) as_fn_error "license not specified" "$LINENO" 5 ;;
+     (no)  with_license= ;;
+     (*) ;;
+  esac
+else
+  with_license=
+fi
+
+  licensedir=$with_license
+
+
+
+
+# Check whether --with-csl-license-feature was given.
+if test "${with_csl_license_feature+set}" = set; then :
+  withval=$with_csl_license_feature; case "$withval" in
+      (yes) as_fn_error "license feature not specified" "$LINENO" 5 ;;
+      (no)  CSL_LICENSE_FEATURE="" ;;
+      (*)   CSL_LICENSE_FEATURE="$withval" ;;
+     esac
+else
+  CSL_LICENSE_FEATURE=""
+
+fi
+
+  if test x"$CSL_LICENSE_FEATURE" != x; then
+
+cat >>confdefs.h <<_ACEOF
+#define CSL_LICENSE_FEATURE "$CSL_LICENSE_FEATURE"
+_ACEOF
+
+  fi
+
+
 # Determine the target- and build-specific subdirectories
 
 # post-stage1 host modules use a different CC_FOR_BUILD so, in order to
@@ -3410,7 +3487,7 @@ fi
 if test "${with_demangler_in_ld+set}" = set; then :
   withval=$with_demangler_in_ld; demangler_in_ld="$with_demangler_in_ld"
 else
-  demangler_in_ld=no
+  demangler_in_ld=yes
 fi
 
 
@@ -6990,6 +7067,10 @@ if test "${enable_fixed_point+set}" = set; then :
 else
 
   case $target in
+    arm*)
+      enable_fixed_point=yes
+      ;;
+
     mips*-*-*)
       case $host in
 	mips*-sgi-irix*)
@@ -7152,6 +7233,17 @@ fi
 
 
 
+# Check whether --with-eglibc-configs was given.
+if test "${with_eglibc_configs+set}" = set; then :
+  withval=$with_eglibc_configs; EGLIBC_CONFIGS=$withval
+else
+  EGLIBC_CONFIGS=
+
+fi
+
+
+
+
 
 # Check whether --with-pkgversion was given.
 if test "${with_pkgversion+set}" = set; then :
@@ -12553,6 +12649,10 @@ solaris*)
   lt_cv_deplibs_check_method=pass_all
   ;;
 
+symbian*)
+  lt_cv_deplibs_check_method=pass_all
+  ;;
+
 sysv5* | sco3.2v5* | sco5v6* | unixware* | OpenUNIX* | sysv4*uw2*)
   lt_cv_deplibs_check_method=pass_all
   ;;
@@ -17014,6 +17129,14 @@ sunos4*)
   need_version=yes
   ;;
 
+symbian*)
+  version_type=windows
+  shrext_cmds=".dll"
+  need_version=no
+  need_lib_prefix=no
+  library_names_spec='${libname}.dll'
+  ;;
+
 sysv4 | sysv4.3*)
   version_type=linux
   library_names_spec='${libname}${release}${shared_ext}$versuffix ${libname}${release}${shared_ext}$major $libname${shared_ext}'
@@ -17527,7 +17650,7 @@ else
   lt_dlunknown=0; lt_dlno_uscore=1; lt_dlneed_uscore=2
   lt_status=$lt_dlunknown
   cat > conftest.$ac_ext <<_LT_EOF
-#line 17530 "configure"
+#line 17653 "configure"
 #include "confdefs.h"
 
 #if HAVE_DLFCN_H
@@ -17633,7 +17756,7 @@ else
   lt_dlunknown=0; lt_dlno_uscore=1; lt_dlneed_uscore=2
   lt_status=$lt_dlunknown
   cat > conftest.$ac_ext <<_LT_EOF
-#line 17636 "configure"
+#line 17759 "configure"
 #include "confdefs.h"
 
 #if HAVE_DLFCN_H
@@ -19475,6 +19598,10 @@ $as_echo_n "checking for $compiler option to produce PIC... " >&6; }
       # Interix 3.x gcc -fpic/-fPIC options generate broken code.
       # Instead, we relocate shared libraries at runtime.
       ;;
+    symbian*)
+      # symbian does not have PIC, the loader relocates non-pic shared objects
+      lt_prog_compiler_pic_CXX=
+      ;;
     sysv4*MP*)
       if test -d /usr/nec; then
 	lt_prog_compiler_pic_CXX=-Kconform_pic
@@ -20673,6 +20815,14 @@ sunos4*)
   need_version=yes
   ;;
 
+symbian*)
+  version_type=windows
+  shrext_cmds=".dll"
+  need_version=no
+  need_lib_prefix=no
+  library_names_spec='${libname}.dll'
+  ;;
+
 sysv4 | sysv4.3*)
   version_type=linux
   library_names_spec='${libname}${release}${shared_ext}$versuffix ${libname}${release}${shared_ext}$major $libname${shared_ext}'
@@ -25522,6 +25672,37 @@ fi
 { $as_echo "$as_me:${as_lineno-$LINENO}: result: $gcc_cv_ld_eh_gc_sections_bug" >&5
 $as_echo "$gcc_cv_ld_eh_gc_sections_bug" >&6; }
 
+{ $as_echo "$as_me:${as_lineno-$LINENO}: checking assembler for CFI .eh_frame_entry" >&5
+$as_echo_n "checking assembler for CFI .eh_frame_entry... " >&6; }
+if test "${gcc_cv_as_eh_frame_entry+set}" = set; then :
+  $as_echo_n "(cached) " >&6
+else
+  gcc_cv_as_eh_frame_entry=no
+  if test x$gcc_cv_as != x; then
+    echo '	.cfi_sections .eh_frame_entry ' > conftest.s
+    if { ac_try='$gcc_cv_as $gcc_cv_as_flags  -o conftest.o conftest.s >&5'
+  { { eval echo "\"\$as_me\":${as_lineno-$LINENO}: \"$ac_try\""; } >&5
+  (eval $ac_try) 2>&5
+  ac_status=$?
+  $as_echo "$as_me:${as_lineno-$LINENO}: \$? = $ac_status" >&5
+  test $ac_status = 0; }; }
+    then
+	gcc_cv_as_eh_frame_entry=yes
+    else
+      echo "configure: failed program was" >&5
+      cat conftest.s >&5
+    fi
+    rm -f conftest.o conftest.s
+  fi
+fi
+{ $as_echo "$as_me:${as_lineno-$LINENO}: result: $gcc_cv_as_eh_frame_entry" >&5
+$as_echo "$gcc_cv_as_eh_frame_entry" >&6; }
+if test $gcc_cv_as_eh_frame_entry = yes; then
+
+$as_echo "#define HAVE_GAS_EH_FRAME_ENTRY 1" >>confdefs.h
+
+fi
+
 # --------
 # UNSORTED
 # --------
@@ -25777,6 +25958,9 @@ else
   gcc_cv_libc_provides_ssp=no
     case "$target" in
        *-*-linux* | *-*-kfreebsd*-gnu | *-*-knetbsd*-gnu)
+      if test "x$enable_libssp" = "xno"; then
+	gcc_cv_libc_provides_ssp=yes
+      fi
       # glibc 2.4 and later provides __stack_chk_fail and
       # either __stack_chk_guard, or TLS access to stack guard canary.
       if test -f $target_header_dir/features.h \
@@ -26158,6 +26342,19 @@ fi
 
 
 
+# Check whether --enable-poison-system-directories was given.
+if test "${enable_poison_system_directories+set}" = set; then :
+  enableval=$enable_poison_system_directories;
+else
+  enable_poison_system_directories=no
+fi
+
+if test "x${enable_poison_system_directories}" = "xyes"; then
+
+$as_echo "#define ENABLE_POISON_SYSTEM_DIRECTORIES 1" >>confdefs.h
+
+fi
+
 # Substitute configuration variables
 
 
diff --git a/gcc/configure.ac b/gcc/configure.ac
index 48605c8..12d3ab7 100644
--- a/gcc/configure.ac
+++ b/gcc/configure.ac
@@ -39,6 +39,10 @@ AC_CANONICAL_TARGET
 # Determine the noncanonical target name, for directory use.
 ACX_NONCANONICAL_TARGET
 
+CSL_AC_LICENSE_VERSION
+CSL_AC_LICENSE
+CSL_AC_LICENSE_FEATURE
+
 # Determine the target- and build-specific subdirectories
 GCC_TOPLEV_SUBDIRS
 
@@ -221,7 +225,7 @@ fi
 AC_ARG_WITH(demangler-in-ld,
 [  --with-demangler-in-ld  try to use demangler in GNU ld.],
 demangler_in_ld="$with_demangler_in_ld",
-demangler_in_ld=no)
+demangler_in_ld=yes)
 
 # ----------------------
 # Find default assembler
@@ -655,6 +659,10 @@ AC_ARG_ENABLE(fixed-point,
 ],
 [
   case $target in
+    arm*)
+      enable_fixed_point=yes
+      ;;
+
     mips*-*-*)
       case $host in
 	mips*-sgi-irix*)
@@ -782,6 +790,14 @@ AC_ARG_WITH(specs,
 )
 AC_SUBST(CONFIGURE_SPECS)
 
+AC_ARG_WITH(eglibc-configs,
+  [AS_HELP_STRING([--with-eglibc-configs=CONFIGS],
+                  [build multilibs for these EGLIBC configurations])],
+  [EGLIBC_CONFIGS=$withval],
+  [EGLIBC_CONFIGS=]
+)
+AC_SUBST(EGLIBC_CONFIGS)
+
 ACX_PKGVERSION([GCC])
 ACX_BUGURL([http://gcc.gnu.org/bugs.html])
 
@@ -4162,6 +4178,13 @@ if test x$gcc_cv_ld_eh_gc_sections_bug = xyes; then
 fi
 AC_MSG_RESULT($gcc_cv_ld_eh_gc_sections_bug)
 
+gcc_GAS_CHECK_FEATURE([CFI .eh_frame_entry],
+ gcc_cv_as_eh_frame_entry,
+ ,,
+[	.cfi_sections .eh_frame_entry ],,
+[AC_DEFINE(HAVE_GAS_EH_FRAME_ENTRY, 1,
+[Define if your assembler supports generation of .eh_frame_entry from CFI directives.])])
+
 # --------
 # UNSORTED
 # --------
@@ -4365,6 +4388,9 @@ AC_CACHE_CHECK(__stack_chk_fail in target C library,
       [gcc_cv_libc_provides_ssp=no
     case "$target" in
        *-*-linux* | *-*-kfreebsd*-gnu | *-*-knetbsd*-gnu)
+      if test "x$enable_libssp" = "xno"; then
+	gcc_cv_libc_provides_ssp=yes
+      fi
       [# glibc 2.4 and later provides __stack_chk_fail and
       # either __stack_chk_guard, or TLS access to stack guard canary.
       if test -f $target_header_dir/features.h \
@@ -4716,6 +4742,16 @@ else
 fi)
 AC_SUBST(slibdir)
 
+AC_ARG_ENABLE([poison-system-directories],
+	      AS_HELP_STRING([--enable-poison-system-directories],
+			     [warn for use of native system header directories]),,
+	      [enable_poison_system_directories=no])
+if test "x${enable_poison_system_directories}" = "xyes"; then
+  AC_DEFINE([ENABLE_POISON_SYSTEM_DIRECTORIES],
+	    [1],
+	    [Define to warn for use of native system header directories])
+fi
+
 # Substitute configuration variables
 AC_SUBST(subdirs)
 AC_SUBST(srcdir)
diff --git a/gcc/cp/cp-lang.c b/gcc/cp/cp-lang.c
index e5c1c09..7593191 100644
--- a/gcc/cp/cp-lang.c
+++ b/gcc/cp/cp-lang.c
@@ -161,7 +161,10 @@ cp_eh_personality (void)
   if (!cp_eh_personality_decl)
     {
       const char *lang = (pragma_java_exceptions ? "gcj" : "gxx");
-      cp_eh_personality_decl = build_personality_function (lang);
+      cp_eh_personality_decl = build_personality_function (lang, false);
+      if (TARGET_COMPACT_EH)
+	 DECL_FUNCTION_PERSONALITY2 (current_function_decl) =
+           build_personality_function (lang, true);
     }
 
   return cp_eh_personality_decl;
diff --git a/gcc/cp/decl2.c b/gcc/cp/decl2.c
index 4c55f4f..cecc6e0 100644
--- a/gcc/cp/decl2.c
+++ b/gcc/cp/decl2.c
@@ -4172,6 +4172,15 @@ mark_used (tree decl)
 
   /* Set TREE_USED for the benefit of -Wunused.  */
   TREE_USED (decl) = 1;
+  if (current_function_decl != NULL_TREE
+      && (TREE_CODE (decl) == VAR_DECL
+	  || TREE_CODE (decl) == PARM_DECL
+	  || TREE_CODE (decl) == FUNCTION_DECL))
+    {
+      tree context = decl_function_context (decl);
+      if (context != NULL_TREE && context != current_function_decl)
+	DECL_NONLOCAL (decl) = 1;
+    }
   if (DECL_CLONED_FUNCTION_P (decl))
     TREE_USED (DECL_CLONED_FUNCTION (decl)) = 1;
 
diff --git a/gcc/crtstuff.c b/gcc/crtstuff.c
index b65f490..38542f3 100644
--- a/gcc/crtstuff.c
+++ b/gcc/crtstuff.c
@@ -152,6 +152,9 @@ extern void __register_frame_info (const void *, struct object *)
 extern void __register_frame_info_bases (const void *, struct object *,
 					 void *, void *)
 				  TARGET_ATTRIBUTE_WEAK;
+extern void __register_frame_info_header_bases (const void *, struct object *,
+					 void *, void *)
+				  TARGET_ATTRIBUTE_WEAK;
 extern void *__deregister_frame_info (const void *)
 				     TARGET_ATTRIBUTE_WEAK;
 extern void *__deregister_frame_info_bases (const void *)
@@ -226,6 +229,10 @@ STATIC func_ptr __DTOR_LIST__[1]
 STATIC EH_FRAME_SECTION_CONST char __EH_FRAME_BEGIN__[]
      __attribute__((section(EH_FRAME_SECTION_NAME), aligned(4)))
      = { };
+
+#ifdef MD_HAVE_COMPACT_EH
+extern char __GNU_EH_FRAME_HDR[] TARGET_ATTRIBUTE_WEAK;
+#endif /* MD_HAVE_COMPACT_EH */
 #endif /* USE_EH_FRAME_REGISTRY */
 
 #ifdef JCR_SECTION_NAME
@@ -327,6 +334,12 @@ __do_global_dtors_aux (void)
 
 #ifdef USE_EH_FRAME_REGISTRY
 #ifdef CRT_GET_RFIB_DATA
+#ifdef MD_HAVE_COMPACT_EH
+  if (__register_frame_info_header_bases && __GNU_EH_FRAME_HDR &&
+      __GNU_EH_FRAME_HDR[0] > 1)
+    __deregister_frame_info_bases (__GNU_EH_FRAME_HDR);
+  else
+#endif /* MD_HAVE_COMPACT_EH */
   /* If we used the new __register_frame_info_bases interface,
      make sure that we deregister from the same place.  */
   if (__deregister_frame_info_bases)
@@ -370,6 +383,13 @@ frame_dummy (void)
   void *tbase, *dbase;
   tbase = 0;
   CRT_GET_RFIB_DATA (dbase);
+#ifdef MD_HAVE_COMPACT_EH
+  if (__register_frame_info_header_bases && __GNU_EH_FRAME_HDR &&
+      __GNU_EH_FRAME_HDR[0] > 1)
+    __register_frame_info_header_bases (__GNU_EH_FRAME_HDR, &object,
+					tbase, dbase);
+  else
+#endif /* MD_HAVE_COMPACT_EH */
   if (__register_frame_info_bases)
     __register_frame_info_bases (__EH_FRAME_BEGIN__, &object, tbase, dbase);
 #else
diff --git a/gcc/cse.c b/gcc/cse.c
index 078977f..6f06807 100644
--- a/gcc/cse.c
+++ b/gcc/cse.c
@@ -3054,6 +3054,12 @@ find_comparison_args (enum rtx_code code, rtx *parg1, rtx *parg2,
 	  if (! exp_equiv_p (p->exp, p->exp, 1, false))
 	    continue;
 
+	  /* If it's the same comparison we're already looking at, skip it.  */
+	  if (COMPARISON_P (p->exp)
+	      && XEXP (p->exp, 0) == arg1
+	      && XEXP (p->exp, 1) == arg2)
+	    continue;
+
 	  if (GET_CODE (p->exp) == COMPARE
 	      /* Another possibility is that this machine has a compare insn
 		 that includes the comparison code.  In that case, ARG1 would
@@ -4231,7 +4237,8 @@ cse_insn (rtx insn)
 	{
 	  if (GET_CODE (XEXP (tem, 0)) == CLOBBER)
 	    invalidate (SET_DEST (XEXP (tem, 0)), VOIDmode);
-	  XEXP (tem, 0) = canon_reg (XEXP (tem, 0), insn);
+	  if (GET_CODE (XEXP (tem, 0)) != SET)
+	    XEXP (tem, 0) = canon_reg (XEXP (tem, 0), insn);
 	}
     }
 
@@ -6071,6 +6078,11 @@ cse_process_notes_1 (rtx x, rtx object, bool *changed)
       validate_change (object, &XEXP (x, i),
 		       cse_process_notes (XEXP (x, i), object, changed), 0);
 
+  /* Rebuild a PLUS expression in canonical form if the first operand
+     ends up as a constant.  */
+  if (code == PLUS && GET_CODE (XEXP (x, 0)) == CONST_INT)
+    return plus_constant (XEXP(x, 1), INTVAL (XEXP (x, 0)));
+
   return x;
 }
 
diff --git a/gcc/dbgcnt.def b/gcc/dbgcnt.def
index e3ac8ea..439f3e1 100644
--- a/gcc/dbgcnt.def
+++ b/gcc/dbgcnt.def
@@ -184,3 +184,4 @@ DEBUG_COUNTER (sms_sched_loop)
 DEBUG_COUNTER (store_motion)
 DEBUG_COUNTER (split_for_sched2)
 DEBUG_COUNTER (tail_call)
+DEBUG_COUNTER (ira_move)
diff --git a/gcc/defaults.h b/gcc/defaults.h
index 815ddd2..1a5e042 100644
--- a/gcc/defaults.h
+++ b/gcc/defaults.h
@@ -380,6 +380,10 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
 #endif
 #endif
 
+#ifndef TARGET_COMPACT_EH
+#define TARGET_COMPACT_EH 0
+#endif
+
 /* If we have named section and we support weak symbols, then use the
    .jcr section for recording java classes which need to be registered
    at program start-up time.  */
diff --git a/gcc/df-problems.c b/gcc/df-problems.c
index c3901b8..58a8c64 100644
--- a/gcc/df-problems.c
+++ b/gcc/df-problems.c
@@ -4037,7 +4037,10 @@ can_move_insns_across (rtx from, rtx to, rtx across_from, rtx across_to,
 	  if (bitmap_intersect_p (merge_set, test_use)
 	      || bitmap_intersect_p (merge_use, test_set))
 	    break;
-	  max_to = insn;
+#ifdef HAVE_cc0
+	  if (!sets_cc0_p (insn))
+#endif
+	    max_to = insn;
 	}
       next = NEXT_INSN (insn);
       if (insn == to)
@@ -4074,7 +4077,11 @@ can_move_insns_across (rtx from, rtx to, rtx across_from, rtx across_to,
     {
       if (NONDEBUG_INSN_P (insn))
 	{
-	  if (!bitmap_intersect_p (test_set, local_merge_live))
+	  if (!bitmap_intersect_p (test_set, local_merge_live)
+#ifdef HAVE_cc0
+	      && !sets_cc0_p (insn)
+#endif
+	      )
 	    {
 	      max_to = insn;
 	      break;
diff --git a/gcc/df-scan.c b/gcc/df-scan.c
index 42b4b14..f256278 100644
--- a/gcc/df-scan.c
+++ b/gcc/df-scan.c
@@ -3181,6 +3181,7 @@ df_uses_record (struct df_collection_rec *collection_rec,
       }
 
     case RETURN:
+    case SIMPLE_RETURN:
       break;
 
     case ASM_OPERANDS:
@@ -3323,6 +3324,7 @@ df_get_call_refs (struct df_collection_rec * collection_rec,
   unsigned int i;
   df_ref def;
   bitmap_head defs_generated;
+  HARD_REG_SET fn_reg_set_usage;
 
   bitmap_initialize (&defs_generated, &df_bitmap_obstack);
 
@@ -3374,9 +3376,14 @@ df_get_call_refs (struct df_collection_rec * collection_rec,
 			   NULL, bb, insn_info, DF_REF_REG_DEF, flags);
 	}
 
+  get_call_reg_set_usage (insn_info->insn, &fn_reg_set_usage,
+			  regs_invalidated_by_call);
   is_sibling_call = SIBLING_CALL_P (insn_info->insn);
   EXECUTE_IF_SET_IN_BITMAP (regs_invalidated_by_call_regset, 0, ui, bi)
     {
+      if (!TEST_HARD_REG_BIT (fn_reg_set_usage, ui))
+	 continue;
+
       if (!global_regs[ui]
 	  && (!bitmap_bit_p (&defs_generated, ui))
 	  && (!is_sibling_call
diff --git a/gcc/dojump.c b/gcc/dojump.c
index 801436b..7d899ca 100644
--- a/gcc/dojump.c
+++ b/gcc/dojump.c
@@ -36,6 +36,7 @@ along with GCC; see the file COPYING3.  If not see
 #include "ggc.h"
 #include "basic-block.h"
 #include "output.h"
+#include "tm_p.h"
 
 static bool prefer_and_bit_test (enum machine_mode, int);
 static void do_jump_by_parts_greater (tree, tree, int, rtx, rtx, int);
diff --git a/gcc/dwarf2asm.c b/gcc/dwarf2asm.c
index b480698..0efc57a 100644
--- a/gcc/dwarf2asm.c
+++ b/gcc/dwarf2asm.c
@@ -91,6 +91,22 @@ dw2_asm_output_data_raw (int size, unsigned HOST_WIDE_INT value)
     }
 }
 
+/* Output a comment.  */
+void
+dw2_asm_output_comment (const char *comment, ...)
+{
+  va_list ap;
+
+  va_start (ap, comment);
+
+  if (flag_debug_asm && comment)
+    {
+      fprintf (asm_out_file, "\t%s ", ASM_COMMENT_START);
+      vfprintf (asm_out_file, comment, ap);
+    }
+  fputc ('\n', asm_out_file);
+}
+
 /* Output an immediate constant in a given SIZE in bytes.  */
 
 void
@@ -761,6 +777,36 @@ dw2_asm_output_delta_uleb128 (const char *lab1 ATTRIBUTE_UNUSED,
   va_end (ap);
 }
 
+void
+dw2_asm_output_delta_setbit0_uleb128 (const char *lab1, const char *lab2,
+				      const char *comment, ...)
+{
+  va_list ap;
+
+  va_start (ap, comment);
+
+#ifdef HAVE_AS_LEB128
+  fputs ("\t.uleb128 ", asm_out_file);
+  fputc ('(', asm_out_file);
+  assemble_name (asm_out_file, lab1);
+  fputc ('-', asm_out_file);
+  assemble_name (asm_out_file, lab2);
+  fputc (')', asm_out_file);
+  fprintf (asm_out_file, "|1");
+#else
+  gcc_unreachable ();
+#endif
+
+  if (flag_debug_asm && comment)
+    {
+      fprintf (asm_out_file, "\t%s ", ASM_COMMENT_START);
+      vfprintf (asm_out_file, comment, ap);
+    }
+  fputc ('\n', asm_out_file);
+
+  va_end (ap);
+}
+
 #if 0
 
 void
diff --git a/gcc/dwarf2asm.h b/gcc/dwarf2asm.h
index 39515d0..6015510 100644
--- a/gcc/dwarf2asm.h
+++ b/gcc/dwarf2asm.h
@@ -23,6 +23,8 @@ extern void dw2_assemble_integer (int, rtx);
 
 extern void dw2_asm_output_data_raw (int, unsigned HOST_WIDE_INT);
 
+extern void dw2_asm_output_comment (const char *, ...);
+
 extern void dw2_asm_output_data (int, unsigned HOST_WIDE_INT,
 				 const char *, ...)
      ATTRIBUTE_NULL_PRINTF_3;
@@ -69,6 +71,10 @@ extern void dw2_asm_output_delta_uleb128 (const char *, const char *,
 					  const char *, ...)
      ATTRIBUTE_NULL_PRINTF_3;
 
+extern void dw2_asm_output_delta_setbit0_uleb128 (const char *, const char *,
+					          const char *, ...)
+     ATTRIBUTE_NULL_PRINTF_3;
+
 extern int size_of_uleb128 (unsigned HOST_WIDE_INT);
 extern int size_of_sleb128 (HOST_WIDE_INT);
 extern int size_of_encoded_value (int);
diff --git a/gcc/dwarf2out.c b/gcc/dwarf2out.c
index 025593c..50af338 100644
--- a/gcc/dwarf2out.c
+++ b/gcc/dwarf2out.c
@@ -159,6 +159,16 @@ dwarf2out_do_frame (void)
   return false;
 }
 
+static bool
+asm_cfi_special_encoding (int enc ATTRIBUTE_UNUSED)
+{
+#ifdef ASM_CFI_SPECIAL_ENCODING
+  return ASM_CFI_SPECIAL_ENCODING (enc);
+#else
+  return false;
+#endif
+}
+
 /* Decide whether to emit frame unwind via assembler directives.  */
 
 int
@@ -179,10 +189,12 @@ dwarf2out_do_cfi_asm (void)
   /* Make sure the personality encoding is one the assembler can support.
      In particular, aligned addresses can't be handled.  */
   enc = ASM_PREFERRED_EH_DATA_FORMAT (/*code=*/2,/*global=*/1);
-  if ((enc & 0x70) != 0 && (enc & 0x70) != DW_EH_PE_pcrel)
+  if ((enc & 0x70) != 0 && (enc & 0x70) != DW_EH_PE_pcrel
+      && ! asm_cfi_special_encoding (enc))
     return false;
   enc = ASM_PREFERRED_EH_DATA_FORMAT (/*code=*/0,/*global=*/0);
-  if ((enc & 0x70) != 0 && (enc & 0x70) != DW_EH_PE_pcrel)
+  if ((enc & 0x70) != 0 && (enc & 0x70) != DW_EH_PE_pcrel
+      && ! asm_cfi_special_encoding (enc))
     return false;
 
   /* If we can't get the assembler to emit only .debug_frame, and we don't need
@@ -1485,7 +1497,7 @@ compute_barrier_args_size_1 (rtx insn, HOST_WIDE_INT cur_args_size,
     {
       rtx dest = JUMP_LABEL (insn);
 
-      if (dest)
+      if (dest && !ANY_RETURN_P (dest))
 	{
 	  if (barrier_args_size [INSN_UID (dest)] < 0)
 	    {
@@ -2967,15 +2979,15 @@ dwarf2out_cfi_begin_epilogue (rtx insn)
 	  int idx;
 	  rtx seq = PATTERN (i);
 
+	  for (idx = 0; idx < XVECLEN (seq, 0); idx++)
+	    if (RTX_FRAME_RELATED_P (XVECEXP (seq, 0, idx)))
+	      saw_frp = true;
+
 	  if (returnjump_p (XVECEXP (seq, 0, 0)))
 	    break;
 	  if (CALL_P (XVECEXP (seq, 0, 0))
 	      && SIBLING_CALL_P (XVECEXP (seq, 0, 0)))
 	    break;
-
-	  for (idx = 0; idx < XVECLEN (seq, 0); idx++)
-	    if (RTX_FRAME_RELATED_P (XVECEXP (seq, 0, idx)))
-	      saw_frp = true;
 	}
 
       if (RTX_FRAME_RELATED_P (i))
@@ -3001,9 +3013,8 @@ dwarf2out_cfi_begin_epilogue (rtx insn)
   while (1)
     {
       rtx p = PREV_INSN (i);
-      if (!NOTE_P (p))
-	break;
-      if (NOTE_KIND (p) == NOTE_INSN_BASIC_BLOCK)
+      if (!LABEL_P (p)
+	  && (!NOTE_P (p) || NOTE_KIND (p) == NOTE_INSN_BASIC_BLOCK))
 	break;
       i = p;
     }
@@ -4002,7 +4013,7 @@ dwarf2out_do_cfi_startproc (bool second)
 	 handle indirect support ourselves, but PC-relative is done
 	 in the assembler.  Further, the assembler can't handle any
 	 of the weirder relocation types.  */
-      if (enc & DW_EH_PE_indirect)
+      if (enc & DW_EH_PE_indirect && !asm_cfi_special_encoding (enc))
 	ref = dw2_force_const_mem (ref, true);
 
       fprintf (asm_out_file, "\t.cfi_personality %#x,", enc);
@@ -4020,7 +4031,7 @@ dwarf2out_do_cfi_startproc (bool second)
       ref = gen_rtx_SYMBOL_REF (Pmode, lab);
       SYMBOL_REF_FLAGS (ref) = SYMBOL_FLAG_LOCAL;
 
-      if (enc & DW_EH_PE_indirect)
+      if (enc & DW_EH_PE_indirect && !asm_cfi_special_encoding (enc))
 	ref = dw2_force_const_mem (ref, true);
 
       fprintf (asm_out_file, "\t.cfi_lsda %#x,", enc);
diff --git a/gcc/emit-rtl.c b/gcc/emit-rtl.c
index 0864571..b62670a 100644
--- a/gcc/emit-rtl.c
+++ b/gcc/emit-rtl.c
@@ -547,6 +547,13 @@ immed_double_const (HOST_WIDE_INT i0, HOST_WIDE_INT i1, enum machine_mode mode)
       if (GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT)
 	return gen_int_mode (i0, mode);
 
+      /* For modes larger than 2 * HOST_BITS_PER_WIDE_INT, the integer may
+	 still be representable if it fits in one word. For other cases,
+	 assert fail below.  */
+      if (GET_MODE_BITSIZE (mode) > 2 * HOST_BITS_PER_WIDE_INT
+	  && ((i1 == 0 && i0 >= 0) || (i1 == ~0 && i0 < 0)))
+	return GEN_INT (i0);
+
       gcc_assert (GET_MODE_BITSIZE (mode) == 2 * HOST_BITS_PER_WIDE_INT);
     }
 
@@ -1680,6 +1687,11 @@ set_mem_attributes_minus_bitpos (rtx ref, tree t, int objectp,
 	  && !TREE_THIS_VOLATILE (base))
 	MEM_READONLY_P (ref) = 1;
 
+      /* Mark static const strings readonly as well.  */
+      if (base && TREE_CODE (base) == STRING_CST && TREE_READONLY (base)
+	  && TREE_STATIC (base))
+	MEM_READONLY_P (ref) = 1;
+
       /* If this expression uses it's parent's alias set, mark it such
 	 that we won't change it.  */
       if (component_uses_parent_alias_set (t))
@@ -2447,6 +2459,8 @@ verify_rtx_sharing (rtx orig, rtx insn)
     case CODE_LABEL:
     case PC:
     case CC0:
+    case RETURN:
+    case SIMPLE_RETURN:
     case SCRATCH:
       return;
       /* SCRATCH must be shared because they represent distinct values.  */
@@ -2547,6 +2561,8 @@ verify_rtl_sharing (void)
 	    for (i = 0; i < XVECLEN (sequence, 0); i++)
 	      {
 		q = XVECEXP (sequence, 0, i);
+		if (LABEL_P (q) || DELETED_NOTE_P (q))
+		  continue;
 		gcc_assert (INSN_P (q));
 		reset_used_flags (PATTERN (q));
 		reset_used_flags (REG_NOTES (q));
@@ -3251,14 +3267,17 @@ prev_label (rtx insn)
   return insn;
 }
 
-/* Return the last label to mark the same position as LABEL.  Return null
-   if LABEL itself is null.  */
+/* Return the last label to mark the same position as LABEL.  Return LABEL
+   itself if it is null or any return rtx.  */
 
 rtx
 skip_consecutive_labels (rtx label)
 {
   rtx insn;
 
+  if (label && ANY_RETURN_P (label))
+    return label;
+
   for (insn = label; insn != 0 && !INSN_P (insn); insn = NEXT_INSN (insn))
     if (LABEL_P (insn))
       label = insn;
@@ -5148,7 +5167,7 @@ classify_insn (rtx x)
     return CODE_LABEL;
   if (GET_CODE (x) == CALL)
     return CALL_INSN;
-  if (GET_CODE (x) == RETURN)
+  if (GET_CODE (x) == RETURN || GET_CODE (x) == SIMPLE_RETURN)
     return JUMP_INSN;
   if (GET_CODE (x) == SET)
     {
@@ -5655,8 +5674,10 @@ init_emit_regs (void)
   init_reg_modes_target ();
 
   /* Assign register numbers to the globally defined register rtx.  */
-  pc_rtx = gen_rtx_PC (VOIDmode);
-  cc0_rtx = gen_rtx_CC0 (VOIDmode);
+  pc_rtx = gen_rtx_fmt_ (PC, VOIDmode);
+  ret_rtx = gen_rtx_fmt_ (RETURN, VOIDmode);
+  simple_return_rtx = gen_rtx_fmt_ (SIMPLE_RETURN, VOIDmode);
+  cc0_rtx = gen_rtx_fmt_ (CC0, VOIDmode);
   stack_pointer_rtx = gen_raw_REG (Pmode, STACK_POINTER_REGNUM);
   frame_pointer_rtx = gen_raw_REG (Pmode, FRAME_POINTER_REGNUM);
   hard_frame_pointer_rtx = gen_raw_REG (Pmode, HARD_FRAME_POINTER_REGNUM);
diff --git a/gcc/except.c b/gcc/except.c
index 251a550..d952676 100644
--- a/gcc/except.c
+++ b/gcc/except.c
@@ -197,6 +198,15 @@ static int sjlj_size_of_call_site_table (void);
 #endif
 static void dw2_output_call_site_table (int, int);
 static void sjlj_output_call_site_table (void);
+static int compact_count_like_headers (enum eh_compact_header_type,
+				       const unsigned char *);
+static int compact_count_eh_specs (HOST_WIDE_INT);
+static int compact_encode_catch_count (int, const unsigned char *);
+static void compact_emit_region_header (enum eh_compact_header_type rtype,
+                                        int region_no);
+static bool compact_ends_in_catchall (int, const unsigned char *);
+static const unsigned char *read_sleb128 (const unsigned char *p,
+                                          HOST_WIDE_INT *val);
 
 
 void
@@ -824,6 +834,7 @@ assign_filter_values (void)
   htab_t ttypes, ehspec;
   eh_region r;
   eh_catch c;
+  int compact_flt_no = 0;
 
   cfun->eh->ttype_data = VEC_alloc (tree, gc, 16);
   if (targetm.arm_eabi_unwinder)
@@ -836,6 +847,9 @@ assign_filter_values (void)
 
   for (i = 1; VEC_iterate (eh_region, cfun->eh->region_array, i, r); ++i)
     {
+
+      compact_flt_no = 0;
+
       if (r == NULL)
 	continue;
 
@@ -861,6 +875,15 @@ assign_filter_values (void)
 
 		      c->filter_list
 			= tree_cons (NULL_TREE, flt_node, c->filter_list);
+
+		      if (TARGET_COMPACT_EH)
+			{
+			  tree compact_flt_node = 
+			    build_int_cst (NULL_TREE, ++compact_flt_no);
+			  c->compact_filter_list
+			    = tree_cons (NULL_TREE, compact_flt_node,
+					 c->compact_filter_list);
+			}
 		    }
 		}
 	      else
@@ -2703,6 +2726,160 @@ push_sleb128 (VEC (uchar, gc) **data_area, int value)
   while (more);
 }
 
+/* The catch count is encoded.  Multiply the count by two and
+   use bit zero to indicate the presence of a catchall handler.  */
+static int
+compact_encode_catch_count (int count, const unsigned char *p)
+{
+  bool encode_catchall = compact_ends_in_catchall (count, p);
+
+  if (encode_catchall)
+    {
+      count--;
+      count <<= 1;
+      count |= 1;
+    }
+  else
+    count <<= 1;
+
+  return count;
+}
+
+static int
+compact_count_eh_specs (HOST_WIDE_INT filter)
+{
+  HOST_WIDE_INT eh_filter;
+  int count = 0;
+
+  eh_filter = VEC_index (uchar, cfun->eh->ehspec_data.other, filter - 1);
+
+  while (eh_filter != 0)
+    {
+      eh_filter = VEC_index (uchar, cfun->eh->ehspec_data.other, eh_filter);
+      count++;
+    }
+  return count;
+}
+
+/* Return true if the catch type list ends in a NULL_TREE,
+   otherwise return false.  */
+static bool
+compact_ends_in_catchall (int count, const unsigned char *p)
+{
+  int i;
+  HOST_WIDE_INT ar_filter;
+  HOST_WIDE_INT ar_disp = -1;
+  tree type = NULL_TREE;
+
+  for (i = 0; i < count; i++)
+    {
+      p = read_sleb128 (p, &ar_filter);
+      p = read_sleb128 (p, &ar_disp);
+      p = p + ar_disp - 1;
+    }
+
+  type = VEC_index (tree, cfun->eh->ttype_data, ar_filter - 1);
+
+  if (type == NULL_TREE)
+    return true;
+  else
+    return false;
+}
+
+/* Count the number of adjacent cleanup or catch headers.  */
+static int
+compact_count_like_headers (enum eh_compact_header_type rtype,
+			    const unsigned char *p)
+{
+  int count = 0;
+  HOST_WIDE_INT ar_filter;
+  HOST_WIDE_INT ar_disp = -1;
+
+  while (ar_disp != 0)
+    {
+      p = read_sleb128 (p, &ar_filter);
+      p = read_sleb128 (p, &ar_disp);
+
+      if (rtype == ECHT_CLEANUP && ar_filter != 0)
+	return count;
+
+      if (rtype == ECHT_CATCH && ar_filter <= 0)
+	return count;
+
+      p = p + ar_disp - 1;
+      count++;
+    }
+
+  return count;
+}
+
+/* Emit the region header for this entry.  */
+static void
+compact_emit_region_header (enum eh_compact_header_type rtype, int region_no)
+{
+  const char *begin;
+  char region_start[32];
+  char region_end[32];
+  char landing_pad[32];
+
+  struct call_site_record_d *cs =
+    VEC_index (call_site_record, crtl->eh.call_site_record[0], region_no);
+
+  begin = current_function_func_begin_label;
+  ASM_GENERATE_INTERNAL_LABEL (region_start, "LEHB",
+                               call_site_base + region_no);
+  ASM_GENERATE_INTERNAL_LABEL (region_end, "LEHE",
+                               call_site_base + region_no);
+
+  if (cs->landing_pad)
+    ASM_GENERATE_INTERNAL_LABEL (landing_pad, "L",
+                                 CODE_LABEL_NUMBER (cs->landing_pad));
+
+  switch (rtype)
+    {
+    case ECHT_CLEANUP:
+      /* Offset bit 0 = 0.  */
+      /* Length bit 0 = 0.  */
+      dw2_asm_output_comment ("Cleanup Region");
+      dw2_asm_output_delta_uleb128 (region_end, region_start, "Length");
+      dw2_asm_output_delta_uleb128 (region_start, begin, "Offset");
+      dw2_asm_output_delta_uleb128 (landing_pad, begin, "Landing Pad Offset");
+      break;
+
+    case ECHT_CATCH:
+      /* Offset bit 0 = 0.  */
+      /* Length bit 0 = 1.  */
+      dw2_asm_output_comment ("Catch Region");
+      dw2_asm_output_delta_setbit0_uleb128 (region_end,
+                                            region_start, "Length");
+      dw2_asm_output_delta_uleb128 (region_start, begin, "Offset");
+      dw2_asm_output_delta_uleb128 (landing_pad, begin, "Landing Pad Offset");
+      break;
+
+    case ECHT_SPEC:
+      /* Offset bit 0 = 1.  */
+      /* Length bit 0 = 1.  */
+      dw2_asm_output_comment ("Exception Specification Region");
+      dw2_asm_output_delta_setbit0_uleb128 (region_end,
+                                            region_start, "Length");
+      dw2_asm_output_delta_setbit0_uleb128 (region_start, begin, "Offset");
+      dw2_asm_output_delta_uleb128 (landing_pad, begin, "Landing Pad Offset");
+      break;
+
+    case ECHT_CONTINUE_UNWINDING:
+      /* Offset bit 0 = 1.  */
+      /* Length bit 0 = 0.  */
+      dw2_asm_output_comment ("Continue Unwinding Region");
+      dw2_asm_output_delta_uleb128 (region_end, region_start, "Length");
+      dw2_asm_output_delta_setbit0_uleb128 (region_start, begin, "Offset");
+      break;
+
+   case ECHT_NOT_INITIALIZED:
+      break;
+
+    }
+}
+
 
 #ifndef HAVE_AS_LEB128
 static int
@@ -2852,11 +3029,16 @@ switch_to_exception_section (const char * ARG_UNUSED (fnname))
 	    flags = SECTION_WRITE;
 
 #ifdef HAVE_LD_EH_GC_SECTIONS
-	  if (flag_function_sections)
+	  if (flag_function_sections
+	      || (DECL_ONE_ONLY (current_function_decl) && HAVE_COMDAT_GROUP))
 	    {
 	      char *section_name = XNEWVEC (char, strlen (fnname) + 32);
+	      /* The EH table must match the code section, so only mark
+		 it linkonce if we have COMDAT groups to tie them together.  */
+	      if (DECL_ONE_ONLY (current_function_decl) && HAVE_COMDAT_GROUP)
+		flags |= SECTION_LINKONCE;
 	      sprintf (section_name, ".gcc_except_table.%s", fnname);
-	      s = get_section (section_name, flags, NULL);
+	      s = get_section (section_name, flags, current_function_decl);
 	      free (section_name);
 	    }
 	  else
@@ -2929,6 +3111,159 @@ output_ttype (tree type, int tt_format, int tt_format_size)
     dw2_asm_output_encoded_addr_rtx (tt_format, value, is_public, NULL);
 }
 
+static const unsigned char *
+read_sleb128 (const unsigned char *p, HOST_WIDE_INT *val)
+{
+  unsigned int shift = 0;
+  unsigned char byte;
+  HOST_WIDE_INT result;
+
+  result = 0;
+  do
+    {
+      byte = *p++;
+      result |= ((HOST_WIDE_INT) byte & 0x7f) << shift;
+      shift += 7;
+    }
+  while (byte & 0x80);
+
+  /* Sign-extend a negative value.  */
+  if (shift < 8 * sizeof(result) && (byte & 0x40) != 0)
+    result |= -(((unsigned HOST_WIDE_INT)1L) << shift);
+
+  *val = (HOST_WIDE_INT) result;
+  return p;
+}
+
+/* Walk the DWARF2 exception-handling tables and emit a compact-encoding
+   of the information.  */
+
+static void
+output_one_function_compact_eh_table (void)
+{
+  int i, k, count;
+  enum eh_compact_header_type this_region = ECHT_NOT_INITIALIZED;
+  int tt_format = ASM_PREFERRED_EH_DATA_FORMAT (/*code=*/0, /*global=*/1);
+  int tt_format_size = size_of_encoded_value (tt_format);
+
+  int nregions = VEC_length (call_site_record, crtl->eh.call_site_record[0]);
+
+  targetm.asm_out.internal_label (asm_out_file, "LLSDA",
+                                  current_function_funcdef_no);
+
+  for (i = 0; i < nregions; ++i)
+    {
+      HOST_WIDE_INT ar_filter, ar_disp;
+      const unsigned char *p, *like_start_p;
+      bool seen_cleanup = false;
+
+      struct call_site_record_d *cs =
+        VEC_index (call_site_record, crtl->eh.call_site_record[0], i);
+
+      if (cs->action == 0 && cs->landing_pad == 0)
+        {
+	  compact_emit_region_header (ECHT_CONTINUE_UNWINDING, i);
+	  continue;
+	}
+
+      else if (cs->action == 0)
+        {
+	  compact_emit_region_header (ECHT_CLEANUP, i);
+	  continue;
+	}
+
+      /* cs->action is an offset into ctrl->eh.action_record_data
+	 The action_record_data will be a pair of values.  Each
+	 pair is a catch/exception spec type.  The first is an
+	 index into cfun->eh->ttype_data and the second is the
+	 offset of the next action.  If the index is positive, it's
+	 a catch.  If it's negative, it's an exception spec.  */
+
+      /* Exception Specification, Catch or Cleanup.  */
+      p = VEC_address (uchar, crtl->eh.action_record_data)
+			+ cs->action - 1;
+
+      ar_disp = -1;
+      while (ar_disp != 0)
+	{
+	  int eh_filter, j, eh_count, encoded_count;
+	  tree type;
+
+	  like_start_p = p;
+	  p = read_sleb128 (p, &ar_filter);
+	  p = read_sleb128 (p, &ar_disp);
+
+	  if (ar_filter == 0)
+	    this_region = ECHT_CLEANUP;
+	  else if (ar_filter > 0)
+	    this_region = ECHT_CATCH;
+	  else if (ar_filter < 0)
+	    {
+	      this_region = ECHT_SPEC;
+	      ar_filter = abs (ar_filter);
+	    }
+
+	  /* Cleanups are emitted later.  */
+	  if (this_region != ECHT_CLEANUP)
+	    compact_emit_region_header (this_region, i);
+
+	  switch (this_region)
+	    {
+	    case ECHT_SPEC:
+	      eh_count = compact_count_eh_specs (ar_filter);
+	      dw2_asm_output_data_uleb128
+		(eh_count, "Number of exception specifications");
+	      eh_filter = VEC_index (uchar, cfun->eh->ehspec_data.other,
+				     ar_filter - 1);
+	      for (j = 0; j < eh_count; j++)
+		{
+		  type = VEC_index (tree, cfun->eh->ttype_data, eh_filter - 1);
+		  output_ttype (type, tt_format, tt_format_size);
+		  eh_filter++;
+		 }
+	      break;
+	    
+	    case ECHT_CATCH:
+	      count = compact_count_like_headers (this_region, like_start_p);
+	      encoded_count = compact_encode_catch_count (count, like_start_p);
+	      dw2_asm_output_data_uleb128 (encoded_count,
+					   "Number of catch clauses (encoded)");
+	      for (k = 0; k < count; k++)
+		{
+		  type = VEC_index (tree, cfun->eh->ttype_data, ar_filter - 1);
+		  /* Don't emit the catchall, if present.  */
+		  if (type != NULL_TREE)
+		    output_ttype (type, tt_format, tt_format_size);
+
+		  if ((k + 1) < count)
+		    {
+		      p = p + ar_disp - 1;
+		      p = read_sleb128 (p, &ar_filter);
+		      p = read_sleb128 (p, &ar_disp);
+		      ar_filter = abs (ar_filter);
+		    }
+		}
+	      break;
+	    
+	    case ECHT_CLEANUP:
+	      seen_cleanup = true;
+	      break;
+
+	    default:
+	      break;
+	    }
+	    
+	  p += ar_disp - 1;
+	}
+      /* Emit the saved cleanup region entry.  */
+      if (seen_cleanup)
+        compact_emit_region_header (ECHT_CLEANUP, i);
+    }
+
+  dw2_asm_output_data_uleb128 (0, "End of Region List");
+  call_site_base += nregions;
+}
+
 static void
 output_one_function_exception_table (int section)
 {
@@ -3127,9 +3462,16 @@ output_function_exception_table (const char *fnname)
   /* If the target wants a label to begin the table, emit it here.  */
   targetm.asm_out.emit_except_table_label (asm_out_file);
 
-  output_one_function_exception_table (0);
-  if (crtl->eh.call_site_record[1] != NULL)
-    output_one_function_exception_table (1);
+  if (TARGET_COMPACT_EH)
+    {
+      output_one_function_compact_eh_table ();
+    }
+  else
+    {
+      output_one_function_exception_table (0);
+      if (crtl->eh.call_site_record[1] != NULL)
+	output_one_function_exception_table (1);
+    }
 
   switch_to_section (current_function_section ());
 }
diff --git a/gcc/except.h b/gcc/except.h
index 14eca87..25a808f 100644
--- a/gcc/except.h
+++ b/gcc/except.h
@@ -68,6 +68,15 @@ enum eh_region_type
   ERT_MUST_NOT_THROW
 };
 
+/* Region header types in the compact EH scheme.  */
+enum eh_compact_header_type
+{
+  ECHT_NOT_INITIALIZED,
+  ECHT_CLEANUP,
+  ECHT_CATCH,
+  ECHT_CONTINUE_UNWINDING,
+  ECHT_SPEC
+};
 
 /* A landing pad for a given exception region.  Any transfer of control
    from the EH runtime to the function happens at a landing pad.  */
@@ -113,6 +122,12 @@ struct GTY(()) eh_catch_d
      compared against the __builtin_eh_filter value.  */
   tree filter_list;
 
+  /* For the compact exception handling encoding:  A TREE_LIST
+     of INTEGER_CSTs that correspond to the type_list entries,
+     having been mapped by compact_assign_filter_values.  These integers
+     are to be compared against the __builtin_eh_filter value.  */
+  tree compact_filter_list;
+
   /* The code that should be executed if this catch handler matches the
      thrown exception.  This label is only maintained until
      pass_lower_eh_dispatch, at which point it is cleared.  */
diff --git a/gcc/explow.c b/gcc/explow.c
index 34adcb9..ecb717d 100644
--- a/gcc/explow.c
+++ b/gcc/explow.c
@@ -771,6 +771,17 @@ enum machine_mode
 promote_function_mode (const_tree type, enum machine_mode mode, int *punsignedp,
 		       const_tree funtype, int for_return)
 {
+  /* Called without a type node for a libcall.  */
+  if (type == NULL_TREE)
+    {
+      if (INTEGRAL_MODE_P (mode))
+	return targetm.calls.promote_function_mode (NULL_TREE, mode,
+						    punsignedp, funtype,
+						    for_return);
+      else
+	return mode;
+    }
+
   switch (TREE_CODE (type))
     {
     case INTEGER_TYPE:   case ENUMERAL_TYPE:   case BOOLEAN_TYPE:
@@ -791,12 +802,23 @@ enum machine_mode
 promote_mode (const_tree type ATTRIBUTE_UNUSED, enum machine_mode mode,
 	      int *punsignedp ATTRIBUTE_UNUSED)
 {
+#ifdef PROMOTE_MODE
+  enum tree_code code;
+  int unsignedp;
+#endif
+
+  /* For libcalls this is invoked without TYPE from the backends
+     TARGET_PROMOTE_FUNCTION_MODE hooks.  Don't do anything in that
+     case.  */
+  if (type == NULL_TREE)
+    return mode;
+
   /* FIXME: this is the same logic that was there until GCC 4.4, but we
      probably want to test POINTERS_EXTEND_UNSIGNED even if PROMOTE_MODE
      is not defined.  The affected targets are M32C, S390, SPARC.  */
 #ifdef PROMOTE_MODE
-  const enum tree_code code = TREE_CODE (type);
-  int unsignedp = *punsignedp;
+  code = TREE_CODE (type);
+  unsignedp = *punsignedp;
 
   switch (code)
     {
diff --git a/gcc/expmed.c b/gcc/expmed.c
index 0d44b2b..09eedf8 100644
--- a/gcc/expmed.c
+++ b/gcc/expmed.c
@@ -47,7 +47,7 @@ struct target_expmed *this_target_expmed = &default_target_expmed;
 
 static void store_fixed_bit_field (rtx, unsigned HOST_WIDE_INT,
 				   unsigned HOST_WIDE_INT,
-				   unsigned HOST_WIDE_INT, rtx);
+				   unsigned HOST_WIDE_INT, rtx, bool);
 static void store_split_bit_field (rtx, unsigned HOST_WIDE_INT,
 				   unsigned HOST_WIDE_INT, rtx);
 static rtx extract_fixed_bit_field (enum machine_mode, rtx,
@@ -349,7 +349,8 @@ check_predicate_volatile_ok (enum insn_code icode, int opno,
 
 static bool
 store_bit_field_1 (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,
-		   unsigned HOST_WIDE_INT bitnum, enum machine_mode fieldmode,
+		   unsigned HOST_WIDE_INT bitnum, bool packedp,
+		   enum machine_mode fieldmode,
 		   rtx value, bool fallback_p)
 {
   unsigned int unit
@@ -599,7 +600,7 @@ store_bit_field_1 (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,
 
 	  if (!store_bit_field_1 (op0, MIN (BITS_PER_WORD,
 					    bitsize - i * BITS_PER_WORD),
-				  bitnum + bit_offset, word_mode,
+				  bitnum + bit_offset, false, word_mode,
 				  value_word, fallback_p))
 	    {
 	      delete_insns_since (last);
@@ -657,6 +658,10 @@ store_bit_field_1 (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,
       && GET_MODE (value) != BLKmode
       && bitsize > 0
       && GET_MODE_BITSIZE (op_mode) >= bitsize
+      /* Do not use insv for volatile bitfields when
+         -fstrict-volatile-bitfields is in effect.  */
+      && !(MEM_P (op0) && MEM_VOLATILE_P (op0)
+	   && flag_strict_volatile_bitfields > 0)
       && ! ((REG_P (op0) || GET_CODE (op0) == SUBREG)
 	    && (bitsize + bitpos > GET_MODE_BITSIZE (op_mode)))
       && insn_data[CODE_FOR_insv].operand[1].predicate (GEN_INT (bitsize),
@@ -700,19 +705,19 @@ store_bit_field_1 (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,
 	  copy_back = true;
 	}
 
-      /* On big-endian machines, we count bits from the most significant.
-	 If the bit field insn does not, we must invert.  */
-
-      if (BITS_BIG_ENDIAN != BYTES_BIG_ENDIAN)
-	xbitpos = unit - bitsize - xbitpos;
-
       /* We have been counting XBITPOS within UNIT.
 	 Count instead within the size of the register.  */
-      if (BITS_BIG_ENDIAN && !MEM_P (xop0))
+      if (BYTES_BIG_ENDIAN && !MEM_P (xop0))
 	xbitpos += GET_MODE_BITSIZE (op_mode) - unit;
 
       unit = GET_MODE_BITSIZE (op_mode);
 
+      /* On big-endian machines, we count bits from the most significant.
+	 If the bit field insn does not, we must invert.  */
+
+      if (BITS_BIG_ENDIAN != BYTES_BIG_ENDIAN)
+	xbitpos = unit - bitsize - xbitpos;
+
       /* Convert VALUE to op_mode (which insv insn wants) in VALUE1.  */
       value1 = value;
       if (GET_MODE (value) != op_mode)
@@ -808,7 +813,7 @@ store_bit_field_1 (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,
 	  /* Fetch that unit, store the bitfield in it, then store
 	     the unit.  */
 	  tempreg = copy_to_reg (xop0);
-	  if (store_bit_field_1 (tempreg, bitsize, xbitpos,
+	  if (store_bit_field_1 (tempreg, bitsize, xbitpos, false,
 				 fieldmode, orig_value, false))
 	    {
 	      emit_move_insn (xop0, tempreg);
@@ -821,7 +826,7 @@ store_bit_field_1 (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,
   if (!fallback_p)
     return false;
 
-  store_fixed_bit_field (op0, offset, bitsize, bitpos, value);
+  store_fixed_bit_field (op0, offset, bitsize, bitpos, value, packedp);
   return true;
 }
 
@@ -832,12 +837,52 @@ store_bit_field_1 (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,
 
 void
 store_bit_field (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,
-		 unsigned HOST_WIDE_INT bitnum, enum machine_mode fieldmode,
-		 rtx value)
+		 unsigned HOST_WIDE_INT bitnum, bool packedp,
+		 enum machine_mode fieldmode, rtx value)
 {
-  if (!store_bit_field_1 (str_rtx, bitsize, bitnum, fieldmode, value, true))
+  if (!store_bit_field_1 (str_rtx, bitsize, bitnum, packedp, fieldmode, value,
+			  true))
     gcc_unreachable ();
 }
+
+static void
+warn_misaligned_bitfield (bool struct_member, bool packedp)
+{
+  static bool informed_about_misalignment = false;
+  bool warned;
+
+  if (packedp)
+    {
+      if (struct_member)
+	warning_at (input_location, OPT_fstrict_volatile_bitfields,
+			     "multiple accesses to volatile structure member"
+			     " because of packed attribute");
+      else
+	warning_at (input_location, OPT_fstrict_volatile_bitfields,
+			     "multiple accesses to volatile structure bitfield"
+			     " because of packed attribute");
+    }
+  else
+    {
+      if (struct_member)
+	warned = warning_at (input_location, OPT_fstrict_volatile_bitfields,
+			     "mis-aligned access used for structure member");
+      else
+	warned = warning_at (input_location, OPT_fstrict_volatile_bitfields,
+			     "mis-aligned access used for structure bitfield");
+    }
+
+  if (! informed_about_misalignment && warned && !packedp)
+    {
+      informed_about_misalignment = true;
+      inform (input_location,
+	      "When a volatile object spans multiple type-sized locations,"
+	      " the compiler must choose between using a single mis-aligned"
+	      " access to preserve the volatility, or using multiple aligned"
+	      " accesses to avoid runtime faults.  This code may fail at"
+	      " runtime if the hardware does not allow this access.");
+    }
+}
 
 /* Use shifts and boolean operations to store VALUE
    into a bit field of width BITSIZE
@@ -851,7 +896,8 @@ store_bit_field (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,
 static void
 store_fixed_bit_field (rtx op0, unsigned HOST_WIDE_INT offset,
 		       unsigned HOST_WIDE_INT bitsize,
-		       unsigned HOST_WIDE_INT bitpos, rtx value)
+		       unsigned HOST_WIDE_INT bitpos, rtx value,
+		       bool packedp)
 {
   enum machine_mode mode;
   unsigned int total_bits = BITS_PER_WORD;
@@ -882,6 +928,7 @@ store_fixed_bit_field (rtx op0, unsigned HOST_WIDE_INT offset,
 	 includes the entire field.  If such a mode would be larger than
 	 a word, we won't be doing the extraction the normal way.
 	 We don't want a mode bigger than the destination.  */
+      bool realign = true;
 
       mode = GET_MODE (op0);
       if (GET_MODE_BITSIZE (mode) == 0
@@ -891,7 +938,25 @@ store_fixed_bit_field (rtx op0, unsigned HOST_WIDE_INT offset,
       if (MEM_VOLATILE_P (op0)
           && GET_MODE_BITSIZE (GET_MODE (op0)) > 0
 	  && flag_strict_volatile_bitfields > 0)
-	mode = GET_MODE (op0);
+	{
+	  /* We must use the specified access size.  */
+	  mode = GET_MODE (op0);
+	  total_bits = GET_MODE_BITSIZE (mode);
+	  if (bitpos + bitsize <= total_bits
+	      && bitpos + bitsize 
+		 + (offset % (total_bits / BITS_PER_UNIT)) * BITS_PER_UNIT
+		 > total_bits)
+	    {
+	      realign = false;
+	      if (STRICT_ALIGNMENT)
+		{
+		  warn_misaligned_bitfield (bitsize == GET_MODE_BITSIZE (mode),
+					    packedp);
+		  if (packedp)
+		    mode = VOIDmode;
+		}
+	    }
+	}
       else
 	mode = get_best_mode (bitsize, bitpos + offset * BITS_PER_UNIT,
 			      MEM_ALIGN (op0), mode, MEM_VOLATILE_P (op0));
@@ -899,7 +964,8 @@ store_fixed_bit_field (rtx op0, unsigned HOST_WIDE_INT offset,
       if (mode == VOIDmode)
 	{
 	  /* The only way this should occur is if the field spans word
-	     boundaries.  */
+	     boundaries, or container bondaries with
+	     -fstrict-volatile-bitfields.  */
 	  store_split_bit_field (op0, bitsize, bitpos + offset * BITS_PER_UNIT,
 				 value);
 	  return;
@@ -917,12 +983,15 @@ store_fixed_bit_field (rtx op0, unsigned HOST_WIDE_INT offset,
 		     * BITS_PER_UNIT);
 	}
 
-      /* Get ref to an aligned byte, halfword, or word containing the field.
-	 Adjust BITPOS to be position within a word,
-	 and OFFSET to be the offset of that word.
-	 Then alter OP0 to refer to that word.  */
-      bitpos += (offset % (total_bits / BITS_PER_UNIT)) * BITS_PER_UNIT;
-      offset -= (offset % (total_bits / BITS_PER_UNIT));
+      if (realign)
+	{
+	  /* Get ref to an aligned byte, halfword, or word containing the field.
+	     Adjust BITPOS to be position within a word,
+	     and OFFSET to be the offset of that word.
+	     Then alter OP0 to refer to that word.  */
+	  bitpos += (offset % (total_bits / BITS_PER_UNIT)) * BITS_PER_UNIT;
+	  offset -= (offset % (total_bits / BITS_PER_UNIT));
+	}
       op0 = adjust_address (op0, mode, offset);
     }
 
@@ -1136,7 +1205,7 @@ store_split_bit_field (rtx op0, unsigned HOST_WIDE_INT bitsize,
 	 it is just an out-of-bounds access.  Ignore it.  */
       if (word != const0_rtx)
 	store_fixed_bit_field (word, offset * unit / BITS_PER_UNIT, thissize,
-			       thispos, part);
+			       thispos, part, false);
       bitsdone += thissize;
     }
 }
@@ -1528,6 +1597,10 @@ extract_bit_field_1 (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,
   if (ext_mode != MAX_MACHINE_MODE
       && bitsize > 0
       && GET_MODE_BITSIZE (ext_mode) >= bitsize
+      /* Do not use extv/extzv for volatile bitfields when
+         -fstrict-volatile-bitfields is in effect.  */
+      && !(MEM_P (op0) && MEM_VOLATILE_P (op0)
+	   && flag_strict_volatile_bitfields > 0)
       /* If op0 is a register, we need it in EXT_MODE to make it
 	 acceptable to the format of ext(z)v.  */
       && !(GET_CODE (op0) == SUBREG && GET_MODE (op0) != ext_mode)
@@ -1552,17 +1625,17 @@ extract_bit_field_1 (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,
 	/* Get ref to first byte containing part of the field.  */
 	xop0 = adjust_address (xop0, byte_mode, xoffset);
 
-      /* On big-endian machines, we count bits from the most significant.
-	 If the bit field insn does not, we must invert.  */
-      if (BITS_BIG_ENDIAN != BYTES_BIG_ENDIAN)
-	xbitpos = unit - bitsize - xbitpos;
-
       /* Now convert from counting within UNIT to counting in EXT_MODE.  */
-      if (BITS_BIG_ENDIAN && !MEM_P (xop0))
+      if (BYTES_BIG_ENDIAN && !MEM_P (xop0))
 	xbitpos += GET_MODE_BITSIZE (ext_mode) - unit;
 
       unit = GET_MODE_BITSIZE (ext_mode);
 
+      /* On big-endian machines, we count bits from the most significant.
+	 If the bit field insn does not, we must invert.  */
+      if (BITS_BIG_ENDIAN != BYTES_BIG_ENDIAN)
+	xbitpos = unit - bitsize - xbitpos;
+
       if (xtarget == 0)
 	xtarget = xspec_target = gen_reg_rtx (tmode);
 
@@ -1783,42 +1856,13 @@ extract_fixed_bit_field (enum machine_mode tmode, rtx op0,
 	{
 	  if (STRICT_ALIGNMENT)
 	    {
-	      static bool informed_about_misalignment = false;
-	      bool warned;
-
+	      warn_misaligned_bitfield (bitsize == total_bits, packedp);
 	      if (packedp)
 		{
-		  if (bitsize == total_bits)
-		    warned = warning_at (input_location, OPT_fstrict_volatile_bitfields,
-					 "multiple accesses to volatile structure member"
-					 " because of packed attribute");
-		  else
-		    warned = warning_at (input_location, OPT_fstrict_volatile_bitfields,
-					 "multiple accesses to volatile structure bitfield"
-					 " because of packed attribute");
-
 		  return extract_split_bit_field (op0, bitsize,
 						  bitpos + offset * BITS_PER_UNIT,
 						  unsignedp);
 		}
-
-	      if (bitsize == total_bits)
-		warned = warning_at (input_location, OPT_fstrict_volatile_bitfields,
-				     "mis-aligned access used for structure member");
-	      else
-		warned = warning_at (input_location, OPT_fstrict_volatile_bitfields,
-				     "mis-aligned access used for structure bitfield");
-
-	      if (! informed_about_misalignment && warned)
-		{
-		  informed_about_misalignment = true;
-		  inform (input_location,
-			  "when a volatile object spans multiple type-sized locations,"
-			  " the compiler must choose between using a single mis-aligned access to"
-			  " preserve the volatility, or using multiple aligned accesses to avoid"
-			  " runtime faults; this code may fail at runtime if the hardware does"
-			  " not allow this access");
-		}
 	    }
 	}
       else
diff --git a/gcc/expr.c b/gcc/expr.c
index c931396..47494d9 100644
--- a/gcc/expr.c
+++ b/gcc/expr.c
@@ -143,7 +143,7 @@ static void store_constructor_field (rtx, unsigned HOST_WIDE_INT,
 				     tree, tree, int, alias_set_type);
 static void store_constructor (tree, rtx, int, HOST_WIDE_INT);
 static rtx store_field (rtx, HOST_WIDE_INT, HOST_WIDE_INT, enum machine_mode,
-			tree, tree, alias_set_type, bool);
+			tree, tree, alias_set_type, bool, bool);
 
 static unsigned HOST_WIDE_INT highest_pow2_factor_for_target (const_tree, const_tree);
 
@@ -1387,7 +1387,7 @@ init_block_move_fn (const char *asmspec)
 {
   if (!block_move_fn)
     {
-      tree args, fn;
+      tree args, fn, attrs, attr_args;
 
       fn = get_identifier ("memcpy");
       args = build_function_type_list (ptr_type_node, ptr_type_node,
@@ -1402,6 +1402,11 @@ init_block_move_fn (const char *asmspec)
       DECL_VISIBILITY (fn) = VISIBILITY_DEFAULT;
       DECL_VISIBILITY_SPECIFIED (fn) = 1;
 
+      attr_args = build_tree_list (NULL_TREE, build_string (1, "1"));
+      attrs = tree_cons (get_identifier ("fn spec"), attr_args, NULL);
+
+      decl_attributes (&fn, attrs, ATTR_FLAG_BUILT_IN);
+
       block_move_fn = fn;
     }
 
@@ -2077,7 +2082,7 @@ emit_group_store (rtx orig_dst, rtx src, tree type ATTRIBUTE_UNUSED, int ssize)
 	emit_move_insn (adjust_address (dest, mode, bytepos), tmps[i]);
       else
 	store_bit_field (dest, bytelen * BITS_PER_UNIT, bytepos * BITS_PER_UNIT,
-			 mode, tmps[i]);
+			 false, mode, tmps[i]);
     }
 
   /* Copy from the pseudo into the (probable) hard reg.  */
@@ -2171,7 +2176,7 @@ copy_blkmode_from_reg (rtx tgtblk, rtx srcreg, tree type)
 
       /* Use xbitpos for the source extraction (right justified) and
 	 bitpos for the destination store (left justified).  */
-      store_bit_field (dst, bitsize, bitpos % BITS_PER_WORD, copy_mode,
+      store_bit_field (dst, bitsize, bitpos % BITS_PER_WORD, false, copy_mode,
 		       extract_bit_field (src, bitsize,
 					  xbitpos % BITS_PER_WORD, 1, false,
 					  NULL_RTX, copy_mode, copy_mode));
@@ -2249,7 +2254,8 @@ copy_blkmode_to_reg (enum machine_mode mode, tree src)
 
       /* Use bitpos for the source extraction (left justified) and
 	 xbitpos for the destination store (right justified).  */
-      store_bit_field (dst_word, bitsize, xbitpos % BITS_PER_WORD, word_mode,
+      store_bit_field (dst_word, bitsize, xbitpos % BITS_PER_WORD, false,
+		       word_mode,
 		       extract_bit_field (src_word, bitsize,
 					  bitpos % BITS_PER_WORD, 1, false,
 					  NULL_RTX, word_mode, word_mode));
@@ -2935,7 +2941,7 @@ write_complex_part (rtx cplx, rtx val, bool imag_p)
 	gcc_assert (MEM_P (cplx) && ibitsize < BITS_PER_WORD);
     }
 
-  store_bit_field (cplx, ibitsize, imag_p ? ibitsize : 0, imode, val);
+  store_bit_field (cplx, ibitsize, imag_p ? ibitsize : 0, false, imode, val);
 }
 
 /* Extract one of the components of the complex value CPLX.  Extract the
@@ -4409,12 +4415,12 @@ expand_assignment (tree to, tree from, bool nontemporal)
 	  else if (bitpos + bitsize <= mode_bitsize / 2)
 	    result = store_field (XEXP (to_rtx, 0), bitsize, bitpos,
 				  mode1, from, TREE_TYPE (tem),
-				  get_alias_set (to), nontemporal);
+				  get_alias_set (to), nontemporal, false);
 	  else if (bitpos >= mode_bitsize / 2)
 	    result = store_field (XEXP (to_rtx, 1), bitsize,
 				  bitpos - mode_bitsize / 2, mode1, from,
 				  TREE_TYPE (tem), get_alias_set (to),
-				  nontemporal);
+				  nontemporal, false);
 	  else if (bitpos == 0 && bitsize == mode_bitsize)
 	    {
 	      rtx from_rtx;
@@ -4435,7 +4441,7 @@ expand_assignment (tree to, tree from, bool nontemporal)
 	      write_complex_part (temp, XEXP (to_rtx, 1), true);
 	      result = store_field (temp, bitsize, bitpos, mode1, from,
 				    TREE_TYPE (tem), get_alias_set (to),
-				    nontemporal);
+				    nontemporal, false);
 	      emit_move_insn (XEXP (to_rtx, 0), read_complex_part (temp, false));
 	      emit_move_insn (XEXP (to_rtx, 1), read_complex_part (temp, true));
 	    }
@@ -4462,9 +4468,19 @@ expand_assignment (tree to, tree from, bool nontemporal)
 					       to_rtx, to, from))
 	    result = NULL;
 	  else
-	    result = store_field (to_rtx, bitsize, bitpos, mode1, from,
-				  TREE_TYPE (tem), get_alias_set (to),
-				  nontemporal);
+	    {
+	      bool packedp = false;
+
+	      if (TREE_CODE(to) == COMPONENT_REF
+		  && (TYPE_PACKED (TREE_TYPE (TREE_OPERAND (to, 0)))
+		      || (TREE_CODE (TREE_OPERAND (to, 1)) == FIELD_DECL
+			  && DECL_PACKED (TREE_OPERAND (to, 1)))))
+		packedp = true;
+
+	      result = store_field (to_rtx, bitsize, bitpos, mode1, from,
+				    TREE_TYPE (tem), get_alias_set (to),
+				    nontemporal, packedp);
+	    }
 	}
 
       if (result)
@@ -4875,7 +4891,7 @@ store_expr (tree exp, rtx target, int call_param_p, bool nontemporal)
 			      : BLOCK_OP_NORMAL));
 	  else if (GET_MODE (target) == BLKmode)
 	    store_bit_field (target, INTVAL (expr_size (exp)) * BITS_PER_UNIT,
-			     0, GET_MODE (temp), temp);
+			     0, false, GET_MODE (temp), temp);
 	  else
 	    convert_move (target, temp, unsignedp);
 	}
@@ -5318,7 +5334,8 @@ store_constructor_field (rtx target, unsigned HOST_WIDE_INT bitsize,
       store_constructor (exp, target, cleared, bitsize / BITS_PER_UNIT);
     }
   else
-    store_field (target, bitsize, bitpos, mode, exp, type, alias_set, false);
+    store_field (target, bitsize, bitpos, mode, exp, type, alias_set, false,
+		 false);
 }
 
 /* Store the value of constructor EXP into the rtx TARGET.
@@ -5906,7 +5923,7 @@ store_constructor (tree exp, rtx target, int cleared, HOST_WIDE_INT size)
 static rtx
 store_field (rtx target, HOST_WIDE_INT bitsize, HOST_WIDE_INT bitpos,
 	     enum machine_mode mode, tree exp, tree type,
-	     alias_set_type alias_set, bool nontemporal)
+	     alias_set_type alias_set, bool nontemporal, bool packedp)
 {
   if (TREE_CODE (exp) == ERROR_MARK)
     return const0_rtx;
@@ -5938,7 +5955,7 @@ store_field (rtx target, HOST_WIDE_INT bitsize, HOST_WIDE_INT bitpos,
 	emit_move_insn (object, target);
 
       store_field (blk_object, bitsize, bitpos, mode, exp, type, alias_set,
-		   nontemporal);
+		   nontemporal, false);
 
       emit_move_insn (target, object);
 
@@ -6055,7 +6072,7 @@ store_field (rtx target, HOST_WIDE_INT bitsize, HOST_WIDE_INT bitpos,
 	}
 
       /* Store the value in the bitfield.  */
-      store_bit_field (target, bitsize, bitpos, mode, temp);
+      store_bit_field (target, bitsize, bitpos, packedp, mode, temp);
 
       return const0_rtx;
     }
@@ -7503,7 +7520,7 @@ expand_expr_real_2 (sepops ops, rtx target, enum machine_mode tmode,
 				 * BITS_PER_UNIT),
 				(HOST_WIDE_INT) GET_MODE_BITSIZE (mode)),
 			   0, TYPE_MODE (valtype), treeop0,
-			   type, 0, false);
+			   type, 0, false, false);
 	    }
 
 	  /* Return the entire union.  */
@@ -10576,9 +10593,8 @@ const_vector_from_tree (tree exp)
 /* Build a decl for a personality function given a language prefix.  */
 
 tree
-build_personality_function (const char *lang)
+build_personality_function (const char *lang, bool alternative)
 {
-  const char *unwind_and_version;
   tree decl, type;
   char *name;
 
@@ -10587,18 +10603,26 @@ build_personality_function (const char *lang)
     case UI_NONE:
       return NULL;
     case UI_SJLJ:
-      unwind_and_version = "_sj0";
+      name = ACONCAT (("__", lang, "_personality_sj0", NULL));
       break;
     case UI_DWARF2:
     case UI_TARGET:
-      unwind_and_version = "_v0";
+
+      if (TARGET_COMPACT_EH
+          && (strcmp (lang_hooks.name, "GNU C++") == 0))
+	{
+	  if (alternative)
+	    name = ACONCAT (("__gnu_compact_pr3", NULL));
+	  else
+	    name = ACONCAT (("__gnu_compact_pr2", NULL));
+	}
+      else
+	name = ACONCAT (("__", lang, "_personality_v0", NULL));
       break;
     default:
       gcc_unreachable ();
     }
 
-  name = ACONCAT (("__", lang, "_personality", unwind_and_version, NULL));
-
   type = build_function_type_list (integer_type_node, integer_type_node,
 				   long_long_unsigned_type_node,
 				   ptr_type_node, ptr_type_node, NULL_TREE);
diff --git a/gcc/expr.h b/gcc/expr.h
index eff6db2..7d79023 100644
--- a/gcc/expr.h
+++ b/gcc/expr.h
@@ -668,7 +668,8 @@ extern enum machine_mode
 mode_for_extraction (enum extraction_pattern, int);
 
 extern void store_bit_field (rtx, unsigned HOST_WIDE_INT,
-			     unsigned HOST_WIDE_INT, enum machine_mode, rtx);
+			     unsigned HOST_WIDE_INT, bool,
+			     enum machine_mode, rtx);
 extern rtx extract_bit_field (rtx, unsigned HOST_WIDE_INT,
 			      unsigned HOST_WIDE_INT, int, bool, rtx,
 			      enum machine_mode, enum machine_mode);
diff --git a/gcc/final.c b/gcc/final.c
index 7d3ff14..3883a63 100644
--- a/gcc/final.c
+++ b/gcc/final.c
@@ -232,6 +232,7 @@ static int final_addr_vec_align (rtx);
 #ifdef HAVE_ATTR_length
 static int align_fuzz (rtx, rtx, int, unsigned);
 #endif
+static void collect_fn_hard_reg_usage (void);
 
 /* Initialize data in final at the beginning of a compilation.  */
 
@@ -704,6 +705,8 @@ compute_alignments (void)
   int freq_max = 0;
   int freq_threshold = 0;
 
+  collect_fn_hard_reg_usage ();
+
   if (label_align)
     {
       free (label_align);
@@ -1129,7 +1132,9 @@ shorten_branches (rtx first ATTRIBUTE_UNUSED)
 	      int inner_uid = INSN_UID (inner_insn);
 	      int inner_length;
 
-	      if (GET_CODE (body) == ASM_INPUT
+              if (LABEL_P (inner_insn) || DELETED_NOTE_P (inner_insn))
+                inner_length = 0;
+	      else if (GET_CODE (body) == ASM_INPUT
 		  || asm_noperands (PATTERN (XVECEXP (body, 0, i))) >= 0)
 		inner_length = (asm_insn_count (PATTERN (inner_insn))
 				* insn_default_length (inner_insn));
@@ -1139,8 +1144,9 @@ shorten_branches (rtx first ATTRIBUTE_UNUSED)
 	      insn_lengths[inner_uid] = inner_length;
 	      if (const_delay_slots)
 		{
-		  if ((varying_length[inner_uid]
-		       = insn_variable_length_p (inner_insn)) != 0)
+		  if (!(LABEL_P (inner_insn) || DELETED_NOTE_P (inner_insn))
+                      && (varying_length[inner_uid]
+                          = insn_variable_length_p (inner_insn)) != 0)
 		    varying_length[uid] = 1;
 		  INSN_ADDRESSES (inner_uid) = (insn_current_address
 						+ insn_lengths[uid]);
@@ -2290,9 +2296,15 @@ final_scan_insn (rtx insn, FILE *file, int optimize_p ATTRIBUTE_UNUSED,
 
 	    /* Record the delay slots' frame information before the branch.
 	       This is needed for delayed calls: see execute_cfa_program().  */
-	    if (dwarf2out_do_frame ())
+	    if (!JUMP_P (XVECEXP (body, 0, 0))
+		&& dwarf2out_do_frame ())
 	      for (i = 1; i < XVECLEN (body, 0); i++)
-		dwarf2out_frame_debug (XVECEXP (body, 0, i), false);
+                {
+                  if (LABEL_P (XVECEXP (body, 0, i))
+		      || DELETED_NOTE_P (XVECEXP (body, 0, i)))
+                    continue;
+                  dwarf2out_frame_debug (XVECEXP (body, 0, i), false);
+                }
 
 	    /* The first insn in this SEQUENCE might be a JUMP_INSN that will
 	       force the restoration of a comparison that was previously
@@ -2319,6 +2331,16 @@ final_scan_insn (rtx insn, FILE *file, int optimize_p ATTRIBUTE_UNUSED,
 #ifdef DBR_OUTPUT_SEQEND
 	    DBR_OUTPUT_SEQEND (file);
 #endif
+	    if (JUMP_P (XVECEXP (body, 0, 0))
+		&& dwarf2out_do_frame ())
+	      for (i = 1; i < XVECLEN (body, 0); i++)
+                {
+                  if (LABEL_P (XVECEXP (body, 0, i))
+		      || DELETED_NOTE_P (XVECEXP (body, 0, i)))
+                    continue;
+                  dwarf2out_frame_debug (XVECEXP (body, 0, i), false);
+                }
+
 	    final_sequence = 0;
 
 	    /* If the insn requiring the delay slot was a CALL_INSN, the
@@ -2428,7 +2450,8 @@ final_scan_insn (rtx insn, FILE *file, int optimize_p ATTRIBUTE_UNUSED,
 	        delete_insn (insn);
 		break;
 	      }
-	    else if (GET_CODE (SET_SRC (body)) == RETURN)
+	    else if (GET_CODE (SET_SRC (body)) == RETURN
+		     || GET_CODE (SET_SRC (body)) == SIMPLE_RETURN)
 	      /* Replace (set (pc) (return)) with (return).  */
 	      PATTERN (insn) = body = SET_SRC (body);
 
@@ -2751,6 +2774,12 @@ notice_source_line (rtx insn, bool *is_stmt)
   if (filename == NULL)
     return false;
 
+#ifdef HAVE_ATTR_length
+  /* Prevent duplicate line markers at the same location.  */
+  if (get_attr_length (insn) == 0)
+    return false;
+#endif
+
   if (force_source_line
       || filename != last_filename
       || last_linenum != linenum)
@@ -3576,8 +3605,7 @@ output_addr_const (FILE *file, rtx x)
       break;
 
     case CONST_FIXED:
-      fprintf (file, HOST_WIDE_INT_PRINT_HEX,
-	       (unsigned HOST_WIDE_INT) CONST_FIXED_VALUE_LOW (x));
+      fprintf (file, HOST_WIDE_INT_PRINT_DEC, CONST_FIXED_VALUE_LOW (x));
       break;
 
     case PLUS:
@@ -4520,3 +4548,190 @@ struct rtl_opt_pass pass_clean_state =
   0                                     /* todo_flags_finish */
  }
 };
+
+struct GTY (()) fn_hard_reg_usage
+{
+  /* Function declaration.  */
+  tree fndecl;
+
+  /* Hardware registers used by this function.  */
+  HARD_REG_SET used_regs;
+};
+
+/* Real hardware register usage for all seen functions.  Hash table is used
+   because there is no suitable structure which is not freed after function
+   compilation.  */
+
+static GTY ((param_is (struct fn_hard_reg_usage))) htab_t fn_hard_reg_usage_htab;
+
+/* Returns a hash value for p.  */
+
+static hashval_t
+fn_hard_reg_usage_hash (const void *p)
+{
+  /* Function declaration pointer is used as a hash index.  */
+  const struct fn_hard_reg_usage *entry =
+    (const struct fn_hard_reg_usage *) p;
+  return (hashval_t) (uintptr_t) entry->fndecl;
+}
+
+/* Used by hashtable to compare two fn_hard_reg_usage entries.  */
+
+static int
+fn_hard_reg_usage_eq (const void *p1, const void *p2)
+{
+  const struct fn_hard_reg_usage *entry1 =
+    (const struct fn_hard_reg_usage *) p1;
+  const struct fn_hard_reg_usage *entry2 =
+    (const struct fn_hard_reg_usage *) p2;
+  return entry1->fndecl == entry2->fndecl;
+}
+
+/* Collect function hard register usage for the current function.  */
+
+static void
+collect_fn_hard_reg_usage (void)
+{
+  basic_block bb;
+  rtx insn;
+  struct fn_hard_reg_usage dummy;
+  void **slot;
+  struct fn_hard_reg_usage *element;
+  int i;
+
+  if (!flag_use_caller_save)
+    return;
+
+  df_analyze ();
+
+  if (!fn_hard_reg_usage_htab)
+    fn_hard_reg_usage_htab = htab_create_ggc (128, fn_hard_reg_usage_hash,
+					      fn_hard_reg_usage_eq, NULL);
+
+  dummy.fndecl = current_function_decl;
+  slot = htab_find_slot (fn_hard_reg_usage_htab, &dummy, INSERT);
+  if (*slot != HTAB_EMPTY_ENTRY)
+    {
+      element = (struct fn_hard_reg_usage *) *slot;
+      gcc_assert (current_function_decl == element->fndecl);
+      return;
+    }
+
+  /* Insert node into hash table.  */
+  element = ggc_alloc_fn_hard_reg_usage ();
+  memset (element, 0, sizeof (struct fn_hard_reg_usage));
+  element->fndecl = current_function_decl;
+  *slot = element;
+
+  FOR_EACH_BB (bb)
+  {
+    for (insn = BB_HEAD (bb); insn != NEXT_INSN (BB_END (bb));
+	 insn = NEXT_INSN (insn))
+      {
+	df_ref *def_rec;
+
+	unsigned int uid = INSN_UID (insn);
+
+	if (!NONDEBUG_INSN_P (insn))
+	  continue;
+
+	for (def_rec = DF_INSN_UID_DEFS (uid); *def_rec; def_rec++)
+	  {
+	    df_ref def = *def_rec;
+	    unsigned int dregno = DF_REF_REGNO (def);
+	    if (dregno < FIRST_PSEUDO_REGISTER)
+	      SET_HARD_REG_BIT (element->used_regs, dregno);
+	  }
+      }
+  }
+
+  /* Be conservative - mark fixed and global registers as used.  */
+  IOR_HARD_REG_SET (element->used_regs, fixed_reg_set);
+  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
+    if (global_regs[i])
+      SET_HARD_REG_BIT (element->used_regs, i);
+
+#ifdef STACK_REGS
+  /* Handle STACK_REGS conservatively, since the df-framework does not
+     provide accurate information for them.  */
+
+  for (i = FIRST_STACK_REG; i <= LAST_STACK_REG; i++)
+    SET_HARD_REG_BIT (element->used_regs, i);
+#endif
+}
+
+/* Get registers used by given function.  */
+
+static bool
+get_fn_reg_set_usage (tree fndecl, HARD_REG_SET *reg_set)
+{
+  struct fn_hard_reg_usage dummy;
+  void **slot = NULL;
+  struct fn_hard_reg_usage *element;
+
+  dummy.fndecl = fndecl;
+  if (fn_hard_reg_usage_htab)
+    slot = htab_find_slot (fn_hard_reg_usage_htab, &dummy, NO_INSERT);
+
+  if (slot == NULL || !targetm.binds_local_p (fndecl))
+    return FALSE;
+  else
+    {
+      element = (struct fn_hard_reg_usage *) (*slot);
+      COPY_HARD_REG_SET (*reg_set, element->used_regs);
+      return TRUE;
+    }
+}
+
+/* Recursive part of the function get_call_fndecl.  */
+
+static tree
+get_call_fndecl_rec (rtx x)
+{
+  if (GET_CODE (x) == PARALLEL)
+    return get_call_fndecl_rec (XVECEXP (x, 0, 0));
+  if (GET_CODE (x) == SET)
+    return get_call_fndecl_rec (SET_SRC (x));
+  if (GET_CODE (x) == CALL)
+    {
+      if (GET_CODE (XEXP (x, 0)) != MEM)
+	return NULL_TREE;
+
+      if (GET_CODE (XEXP (XEXP (x, 0), 0)) != SYMBOL_REF)
+	return NULL_TREE;
+
+      return SYMBOL_REF_DECL (XEXP (XEXP (x, 0), 0));
+    }
+  return NULL_TREE;
+}
+
+/* Get the declaration of the function called by instruction insn.  */
+
+static bool
+get_call_fndecl (rtx insn, tree *fndecl)
+{
+  *fndecl = get_call_fndecl_rec (PATTERN (insn));
+  if (*fndecl)
+    return true;
+  return false;
+}
+
+/* Get registers used by given function call instruction.  */
+
+void
+get_call_reg_set_usage (rtx insn, HARD_REG_SET *reg_set,
+			HARD_REG_SET default_set)
+{
+  tree fndecl;
+  if (get_call_fndecl (insn, &fndecl))
+    {
+      if (get_fn_reg_set_usage (fndecl, reg_set))
+	AND_HARD_REG_SET (*reg_set, default_set);
+      else
+	COPY_HARD_REG_SET (*reg_set, default_set);
+    }
+  else
+    COPY_HARD_REG_SET (*reg_set, default_set);
+}
+
+#include "gt-final.h"
diff --git a/gcc/function.c b/gcc/function.c
index d728384..54bdbbf 100644
--- a/gcc/function.c
+++ b/gcc/function.c
@@ -146,9 +146,6 @@ extern tree debug_find_var_in_block_tree (tree, tree);
    can always export `prologue_epilogue_contains'.  */
 static void record_insns (rtx, rtx, htab_t *) ATTRIBUTE_UNUSED;
 static bool contains (const_rtx, htab_t);
-#ifdef HAVE_return
-static void emit_return_into_block (basic_block);
-#endif
 static void prepare_function_start (void);
 static void do_clobber_return_reg (rtx, void *);
 static void do_use_return_reg (rtx, void *);
@@ -274,7 +271,11 @@ get_stack_local_alignment (tree type, enum machine_mode mode)
   if (! type)
     type = lang_hooks.types.type_for_mode (mode, 0);
 
-  return STACK_SLOT_ALIGNMENT (type, mode, alignment);
+  alignment = STACK_SLOT_ALIGNMENT (type, mode, alignment);
+
+  alignment = alignment_for_aligned_arrays (type, alignment);
+
+  return alignment;
 }
 
 /* Determine whether it is possible to fit a stack slot of size SIZE and
@@ -2914,21 +2915,6 @@ assign_parm_setup_block (struct assign_parm_data_all *all,
   SET_DECL_RTL (parm, stack_parm);
 }
 
-/* A subroutine of assign_parm_setup_reg, called through note_stores.
-   This collects sets and clobbers of hard registers in a HARD_REG_SET,
-   which is pointed to by DATA.  */
-static void
-record_hard_reg_sets (rtx x, const_rtx pat ATTRIBUTE_UNUSED, void *data)
-{
-  HARD_REG_SET *pset = (HARD_REG_SET *)data;
-  if (REG_P (x) && REGNO (x) < FIRST_PSEUDO_REGISTER)
-    {
-      int nregs = hard_regno_nregs[REGNO (x)][GET_MODE (x)];
-      while (nregs-- > 0)
-	SET_HARD_REG_BIT (*pset, REGNO (x) + nregs);
-    }
-}
-
 /* A subroutine of assign_parms.  Allocate a pseudo to hold the current
    parameter.  Get it there.  Perform all ABI specified conversions.  */
 
@@ -5263,42 +5249,313 @@ prologue_epilogue_contains (const_rtx insn)
   return 0;
 }
 
+#ifdef HAVE_simple_return
+/* A subroutine of requires_stack_frame_p, called via for_each_rtx.
+   If any change is made, set CHANGED
+   to true.  */
+
+static int
+frame_required_for_rtx (rtx *loc, void *data ATTRIBUTE_UNUSED)
+{
+  rtx x = *loc;
+  if (x == stack_pointer_rtx || x == hard_frame_pointer_rtx
+      || x == arg_pointer_rtx || x == pic_offset_table_rtx
+#ifdef RETURN_ADDR_REGNUM
+      || (REG_P (x) && REGNO (x) == RETURN_ADDR_REGNUM)
+#endif
+      )
+    return 1;
+  return 0;
+}
+
+/* Return true if INSN requires the stack frame to be set up.
+   PROLOGUE_USED contains the hard registers used in the function
+   prologue.  */
+static bool
+requires_stack_frame_p (rtx insn, HARD_REG_SET prologue_used)
+{
+  HARD_REG_SET hardregs;
+  unsigned regno;
+
+  if (!INSN_P (insn) || DEBUG_INSN_P (insn))
+    return false;
+  if (CALL_P (insn))
+    return !SIBLING_CALL_P (insn);
+  if (for_each_rtx (&PATTERN (insn), frame_required_for_rtx, NULL))
+    return true;
+  CLEAR_HARD_REG_SET (hardregs);
+  note_stores (PATTERN (insn), record_hard_reg_sets, &hardregs);
+  if (hard_reg_set_intersect_p (hardregs, prologue_used))
+    return true;
+  AND_COMPL_HARD_REG_SET (hardregs, call_used_reg_set);
+  for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+    if (TEST_HARD_REG_BIT (hardregs, regno)
+	&& df_regs_ever_live_p (regno))
+      return true;
+  return false;
+}
+
+/* Look for sets of call-saved registers in the first block of the
+   function, and move them down into successor blocks if the register
+   is used only on one path.  This exposes more opportunities for
+   shrink-wrapping.
+   These kinds of sets often occur when incoming argument registers are
+   moved to call-saved registers because their values are live across
+   one or more calls during the function.  */
+
+static void
+prepare_shrink_wrap (basic_block entry_block)
+{
+  rtx insn, curr;
+  FOR_BB_INSNS_SAFE (entry_block, insn, curr)
+    {
+      basic_block next_bb;
+      edge e, live_edge;
+      edge_iterator ei;
+      rtx set, scan;
+      unsigned destreg, srcreg;
+
+      if (!NONDEBUG_INSN_P (insn))
+	continue;
+      set = single_set (insn);
+      if (!set)
+	continue;
+
+      if (!REG_P (SET_SRC (set)) || !REG_P (SET_DEST (set)))
+	continue;
+      srcreg = REGNO (SET_SRC (set));
+      destreg = REGNO (SET_DEST (set));
+      if (hard_regno_nregs[srcreg][GET_MODE (SET_SRC (set))] > 1
+	  || hard_regno_nregs[destreg][GET_MODE (SET_DEST (set))] > 1)
+	continue;
+
+      next_bb = entry_block;
+      scan = insn;
+
+      for (;;)
+	{
+	  live_edge = NULL;
+	  FOR_EACH_EDGE (e, ei, next_bb->succs)
+	    {
+	      if (REGNO_REG_SET_P (df_get_live_in (e->dest), destreg))
+		{
+		  if (live_edge)
+		    {
+		      live_edge = NULL;
+		      break;
+		    }
+		  live_edge = e;
+		}
+	    }
+	  if (!live_edge)
+	    break;
+	  /* We can sometimes encounter dead code.  Don't try to move it
+	     into the exit block.  */
+	  if (live_edge->dest == EXIT_BLOCK_PTR)
+	    break;
+	  if (EDGE_COUNT (live_edge->dest->preds) > 1)
+	    break;
+	  while (scan != BB_END (next_bb))
+	    {
+	      scan = NEXT_INSN (scan);
+	      if (NONDEBUG_INSN_P (scan))
+		{
+		  rtx link;
+		  HARD_REG_SET set_regs;
+
+		  CLEAR_HARD_REG_SET (set_regs);
+		  note_stores (PATTERN (scan), record_hard_reg_sets,
+			       &set_regs);
+		  if (CALL_P (scan))
+		    IOR_HARD_REG_SET (set_regs, call_used_reg_set);
+		  for (link = REG_NOTES (scan); link; link = XEXP (link, 1))
+		    if (REG_NOTE_KIND (link) == REG_INC)
+		      record_hard_reg_sets (XEXP (link, 0), NULL, &set_regs);
+
+		  if (TEST_HARD_REG_BIT (set_regs, srcreg)
+		      || TEST_HARD_REG_BIT (set_regs, destreg)
+		      || reg_referenced_p (SET_DEST (set),
+					   PATTERN (scan)))
+		    {
+		      scan = NULL_RTX;
+		      break;
+		    }
+		  if (CALL_P (scan))
+		    {
+		      rtx link = CALL_INSN_FUNCTION_USAGE (scan);
+		      while (link)
+			{
+			  rtx tmp = XEXP (link, 0);
+			  if (GET_CODE (tmp) == USE
+			      && reg_referenced_p (SET_DEST (set), tmp))
+			    break;
+			  link = XEXP (link, 1);
+			}
+		      if (link)
+			{
+			  scan = NULL_RTX;
+			  break;
+			}
+		    }
+		}
+	    }
+	  if (!scan)
+	    break;
+	  next_bb = live_edge->dest;
+	}
+
+      if (next_bb != entry_block)
+	{
+	  rtx after = BB_HEAD (next_bb);
+	  while (!NOTE_P (after)
+		 || NOTE_KIND (after) != NOTE_INSN_BASIC_BLOCK)
+	    after = NEXT_INSN (after);
+	  emit_insn_after (PATTERN (insn), after);
+	  delete_insn (insn);
+	}
+    }
+}
+
+#endif
+
 #ifdef HAVE_return
-/* Insert gen_return at the end of block BB.  This also means updating
-   block_for_insn appropriately.  */
+
+static rtx
+gen_return_pattern (bool simple_p)
+{
+#ifdef HAVE_simple_return
+  return simple_p ? gen_simple_return () : gen_return ();
+#else
+  gcc_assert (!simple_p);
+  return gen_return ();
+#endif
+}
+
+/* Insert an appropriate return pattern at the end of block BB.  This
+   also means updating block_for_insn appropriately.  */
 
 static void
-emit_return_into_block (basic_block bb)
+emit_return_into_block (bool simple_p, basic_block bb)
+{
+  rtx jump;
+  jump = emit_jump_insn_after (gen_return_pattern (simple_p), BB_END (bb));
+  JUMP_LABEL (jump) = simple_p ? simple_return_rtx : ret_rtx;
+}
+#endif
+
+/* Return true if BB has any active insns.  */
+static bool
+bb_active_p (basic_block bb)
 {
-  emit_jump_insn_after (gen_return (), BB_END (bb));
+  rtx label;
+
+  /* Test whether there are active instructions in the last block.  */
+  label = BB_END (bb);
+  while (label && !LABEL_P (label))
+    {
+      if (active_insn_p (label))
+	break;
+      label = PREV_INSN (label);
+    }
+  return BB_HEAD (bb) != label || !LABEL_P (label);
 }
-#endif /* HAVE_return */
 
 /* Generate the prologue and epilogue RTL if the machine supports it.  Thread
    this into place with notes indicating where the prologue ends and where
-   the epilogue begins.  Update the basic block information when possible.  */
+   the epilogue begins.  Update the basic block information when possible.
+
+   Notes on epilogue placement:
+   There are several kinds of edges to the exit block:
+   * a single fallthru edge from LAST_BB
+   * possibly, edges from blocks containing sibcalls
+   * possibly, fake edges from infinite loops
+
+   The epilogue is always emitted on the fallthru edge from the last basic
+   block in the function, LAST_BB, into the exit block.
+
+   If LAST_BB is empty except for a label, it is the target of every
+   other basic block in the function that ends in a return.  If a
+   target has a return or simple_return pattern (possibly with
+   conditional variants), these basic blocks can be changed so that a
+   return insn is emitted into them, and their target is adjusted to
+   the real exit block.
+
+   Notes on shrink wrapping: We implement a fairly conservative
+   version of shrink-wrapping rather than the textbook one.  We only
+   generate a single prologue and a single epilogue.  This is
+   sufficient to catch a number of interesting cases involving early
+   exits.
+
+   First, we identify the blocks that require the prologue to occur before
+   them.  These are the ones that modify a call-saved register, or reference
+   any of the stack or frame pointer registers.  To simplify things, we then
+   mark everything reachable from these blocks as also requiring a prologue.
+   This takes care of loops automatically, and avoids the need to examine
+   whether MEMs reference the frame, since it is sufficient to check for
+   occurrences of the stack or frame pointer.
+
+   We then compute the set of blocks for which the need for a prologue
+   is anticipatable (borrowing terminology from the shrink-wrapping
+   description in Muchnick's book).  These are the blocks which either
+   require a prologue themselves, or those that have only successors
+   where the prologue is anticipatable.  The prologue needs to be
+   inserted on all edges from BB1->BB2 where BB2 is in ANTIC and BB1
+   is not.  For the moment, we ensure that only one such edge exists.
+
+   The epilogue is placed as described above, but we make a
+   distinction between inserting return and simple_return patterns
+   when modifying other blocks that end in a return.  Blocks that end
+   in a sibcall omit the sibcall_epilogue if the block is not in
+   ANTIC.  */
 
 static void
 thread_prologue_and_epilogue_insns (void)
 {
   bool inserted;
+  basic_block last_bb;
+  bool last_bb_active;
+#ifdef HAVE_simple_return
+  bool unconverted_simple_returns = false;
+  basic_block simple_return_block = NULL;
+#endif
+  rtx returnjump ATTRIBUTE_UNUSED;
   rtx seq ATTRIBUTE_UNUSED, epilogue_end ATTRIBUTE_UNUSED;
-  edge entry_edge ATTRIBUTE_UNUSED;
+  rtx prologue_seq ATTRIBUTE_UNUSED, split_prologue_seq ATTRIBUTE_UNUSED;
+  edge entry_edge, orig_entry_edge, exit_fallthru_edge;
   edge e;
   edge_iterator ei;
+  bitmap_head bb_flags;
+
+  df_analyze ();
 
   rtl_profile_for_bb (ENTRY_BLOCK_PTR);
 
   inserted = false;
   seq = NULL_RTX;
+  prologue_seq = NULL_RTX;
   epilogue_end = NULL_RTX;
+  returnjump = NULL_RTX;
 
   /* Can't deal with multiple successors of the entry block at the
      moment.  Function should always have at least one entry
      point.  */
   gcc_assert (single_succ_p (ENTRY_BLOCK_PTR));
   entry_edge = single_succ_edge (ENTRY_BLOCK_PTR);
+  orig_entry_edge = entry_edge;
 
+  exit_fallthru_edge = find_fallthru_edge (EXIT_BLOCK_PTR->preds);
+  if (exit_fallthru_edge != NULL)
+    {
+      last_bb = exit_fallthru_edge->src;
+      last_bb_active = bb_active_p (last_bb);
+    }
+  else
+    {
+      last_bb = NULL;
+      last_bb_active = false;
+    }
+
+  split_prologue_seq = NULL_RTX;
   if (flag_split_stack
       && (lookup_attribute ("no_split_stack", DECL_ATTRIBUTES (cfun->decl))
 	  == NULL))
@@ -5310,21 +5567,15 @@ thread_prologue_and_epilogue_insns (void)
 
       start_sequence ();
       emit_insn (gen_split_stack_prologue ());
-      seq = get_insns ();
+      split_prologue_seq = get_insns ();
       end_sequence ();
 
-      record_insns (seq, NULL, &prologue_insn_hash);
-      set_insn_locators (seq, prologue_locator);
-
-      /* This relies on the fact that committing the edge insertion
-	 will look for basic blocks within the inserted instructions,
-	 which in turn relies on the fact that we are not in CFG
-	 layout mode here.  */
-      insert_insn_on_edge (seq, entry_edge);
-      inserted = true;
+      record_insns (split_prologue_seq, NULL, &prologue_insn_hash);
+      set_insn_locators (split_prologue_seq, prologue_locator);
 #endif
     }
 
+  prologue_seq = NULL_RTX;
 #ifdef HAVE_prologue
   if (HAVE_prologue)
     {
@@ -5347,15 +5598,201 @@ thread_prologue_and_epilogue_insns (void)
       if (!targetm.profile_before_prologue () && crtl->profile)
         emit_insn (gen_blockage ());
 
-      seq = get_insns ();
+      prologue_seq = get_insns ();
       end_sequence ();
-      set_insn_locators (seq, prologue_locator);
+      set_insn_locators (prologue_seq, prologue_locator);
+    }
+#endif
 
-      insert_insn_on_edge (seq, entry_edge);
-      inserted = true;
+  bitmap_initialize (&bb_flags, &bitmap_default_obstack);
+
+#ifdef HAVE_simple_return
+  /* Try to perform a kind of shrink-wrapping, making sure the
+     prologue/epilogue is emitted only around those parts of the
+     function that require it.  */
+
+  if (flag_shrink_wrap && HAVE_simple_return && !flag_non_call_exceptions
+      && HAVE_prologue && !crtl->calls_eh_return)
+    {
+      HARD_REG_SET prologue_clobbered, prologue_used, live_on_edge;
+      rtx p_insn;
+
+      VEC(basic_block, heap) *vec;
+      basic_block bb;
+      bitmap_head bb_antic_flags;
+      bitmap_head bb_on_list;
+
+      prepare_shrink_wrap (entry_edge->dest);
+
+      /* That may have inserted instructions into the last block.  */
+      if (last_bb && !last_bb_active)
+	last_bb_active = bb_active_p (last_bb);
+
+      /* Compute the registers set and used in the prologue.  */
+      CLEAR_HARD_REG_SET (prologue_clobbered);
+      CLEAR_HARD_REG_SET (prologue_used);
+      for (p_insn = prologue_seq; p_insn; p_insn = NEXT_INSN (p_insn))
+	{
+	  HARD_REG_SET this_used;
+	  if (!NONDEBUG_INSN_P (p_insn))
+	    continue;
+
+	  CLEAR_HARD_REG_SET (this_used);
+	  note_uses (&PATTERN (p_insn), record_hard_reg_uses,
+		     &this_used);
+	  AND_COMPL_HARD_REG_SET (this_used, prologue_clobbered);
+	  IOR_HARD_REG_SET (prologue_used, this_used);
+	  note_stores (PATTERN (p_insn), record_hard_reg_sets,
+		       &prologue_clobbered);
+	}
+
+      bitmap_initialize (&bb_antic_flags, &bitmap_default_obstack);
+      bitmap_initialize (&bb_on_list, &bitmap_default_obstack);
+
+      vec = VEC_alloc (basic_block, heap, n_basic_blocks);
+
+      FOR_EACH_BB (bb)
+	{
+	  rtx insn;
+	  FOR_BB_INSNS (bb, insn)
+	    {
+	      if (requires_stack_frame_p (insn, prologue_used))
+		{
+		  bitmap_set_bit (&bb_flags, bb->index);
+		  VEC_quick_push (basic_block, vec, bb);
+		  break;
+		}
+	    }
+	}
+
+      /* For every basic block that needs a prologue, mark all blocks
+	 reachable from it, so as to ensure they are also seen as
+	 requiring a prologue.  */
+      while (!VEC_empty (basic_block, vec))
+	{
+	  basic_block tmp_bb = VEC_pop (basic_block, vec);
+	  edge e;
+	  edge_iterator ei;
+	  FOR_EACH_EDGE (e, ei, tmp_bb->succs)
+	    {
+	      if (e->dest == EXIT_BLOCK_PTR
+		  || bitmap_bit_p (&bb_flags, e->dest->index))
+		continue;
+	      bitmap_set_bit (&bb_flags, e->dest->index);
+	      VEC_quick_push (basic_block, vec, e->dest);
+	    }
+	}
+      /* If the last basic block contains only a label, we'll be able
+	 to convert jumps to it to (potentially conditional) return
+	 insns later.  This means we don't necessarily need a prologue
+	 for paths reaching it.  */
+      if (last_bb)
+	{
+	  if (!last_bb_active)
+	    bitmap_clear_bit (&bb_flags, last_bb->index);
+	  else if (!bitmap_bit_p (&bb_flags, last_bb->index))
+	    goto fail_shrinkwrap;
+	}
+
+      /* Now walk backwards from every block that is marked as needing
+	 a prologue to compute the bb_antic_flags bitmap.  */
+      bitmap_copy (&bb_antic_flags, &bb_flags);
+      FOR_EACH_BB (bb)
+	{
+	  edge e;
+	  edge_iterator ei;
+	  if (!bitmap_bit_p (&bb_flags, bb->index))
+	    continue;
+	  FOR_EACH_EDGE (e, ei, bb->preds)
+	    if (!bitmap_bit_p (&bb_antic_flags, e->src->index))
+	      {
+		VEC_quick_push (basic_block, vec, e->src);
+		bitmap_set_bit (&bb_on_list, e->src->index);
+	      }
+	}
+      while (!VEC_empty (basic_block, vec))
+	{
+	  basic_block tmp_bb = VEC_pop (basic_block, vec);
+	  edge e;
+	  edge_iterator ei;
+	  bool all_set = true;
+
+	  bitmap_clear_bit (&bb_on_list, tmp_bb->index);
+	  FOR_EACH_EDGE (e, ei, tmp_bb->succs)
+	    {
+	      if (!bitmap_bit_p (&bb_antic_flags, e->dest->index))
+		{
+		  all_set = false;
+		  break;
+		}
+	    }
+	  if (all_set)
+	    {
+	      bitmap_set_bit (&bb_antic_flags, tmp_bb->index);
+	      FOR_EACH_EDGE (e, ei, tmp_bb->preds)
+		if (!bitmap_bit_p (&bb_antic_flags, e->src->index))
+		  {
+		    VEC_quick_push (basic_block, vec, e->src);
+		    bitmap_set_bit (&bb_on_list, e->src->index);
+		  }
+	    }
+	}
+      /* Find exactly one edge that leads to a block in ANTIC from
+	 a block that isn't.  */
+      if (!bitmap_bit_p (&bb_antic_flags, entry_edge->dest->index))
+	FOR_EACH_BB (bb)
+	  {
+	    if (!bitmap_bit_p (&bb_antic_flags, bb->index))
+	      continue;
+	    FOR_EACH_EDGE (e, ei, bb->preds)
+	      if (!bitmap_bit_p (&bb_antic_flags, e->src->index))
+		{
+		  if (entry_edge != orig_entry_edge)
+		    {
+		      entry_edge = orig_entry_edge;
+		      goto fail_shrinkwrap;
+		    }
+		  entry_edge = e;
+		}
+	  }
+
+      /* Test whether the prologue is known to clobber any register
+	 (other than FP or SP) which are live on the edge.  */
+      CLEAR_HARD_REG_BIT (prologue_clobbered, STACK_POINTER_REGNUM);
+      if (frame_pointer_needed)
+	CLEAR_HARD_REG_BIT (prologue_clobbered, HARD_FRAME_POINTER_REGNUM);
+
+      CLEAR_HARD_REG_SET (live_on_edge);
+      reg_set_to_hard_reg_set (&live_on_edge,
+			       df_get_live_in (entry_edge->dest));
+      if (hard_reg_set_intersect_p (live_on_edge, prologue_clobbered))
+	entry_edge = orig_entry_edge;
+
+      if (dump_file && entry_edge != orig_entry_edge)
+	fprintf (dump_file, "Prologue moved down by shrink-wrapping.\n");
+
+    fail_shrinkwrap:
+      bitmap_clear (&bb_antic_flags);
+      bitmap_clear (&bb_on_list);
+      VEC_free (basic_block, heap, vec);
     }
 #endif
 
+  if (split_prologue_seq != NULL_RTX)
+    {
+      /* This relies on the fact that committing the edge insertion
+	 will look for basic blocks within the inserted instructions,
+	 which in turn relies on the fact that we are not in CFG
+	 layout mode here.  */
+      insert_insn_on_edge (split_prologue_seq, entry_edge);
+      inserted = true;
+    }
+  if (prologue_seq != NULL_RTX)
+    {
+      insert_insn_on_edge (prologue_seq, entry_edge);
+      inserted = true;
+    }
+
   /* If the exit block has no non-fake predecessors, we don't need
      an epilogue.  */
   FOR_EACH_EDGE (e, ei, EXIT_BLOCK_PTR->preds)
@@ -5365,98 +5802,130 @@ thread_prologue_and_epilogue_insns (void)
     goto epilogue_done;
 
   rtl_profile_for_bb (EXIT_BLOCK_PTR);
-#ifdef HAVE_return
-  if (optimize && HAVE_return)
-    {
-      /* If we're allowed to generate a simple return instruction,
-	 then by definition we don't need a full epilogue.  Examine
-	 the block that falls through to EXIT.   If it does not
-	 contain any code, examine its predecessors and try to
-	 emit (conditional) return instructions.  */
 
-      basic_block last;
+#ifdef HAVE_return
+  /* If we're allowed to generate a simple return instruction, then by
+     definition we don't need a full epilogue.  If the last basic
+     block before the exit block does not contain active instructions,
+     examine its predecessors and try to emit (conditional) return
+     instructions.  */
+  if (optimize && !last_bb_active
+      && (HAVE_return || entry_edge != orig_entry_edge))
+    {
+      edge_iterator ei2;
+      int i;
+      basic_block bb;
       rtx label;
+      VEC(basic_block,heap) *src_bbs;
 
-      e = find_fallthru_edge (EXIT_BLOCK_PTR->preds);
-      if (e == NULL)
+      if (exit_fallthru_edge == NULL)
 	goto epilogue_done;
-      last = e->src;
+      label = BB_HEAD (last_bb);
 
-      /* Verify that there are no active instructions in the last block.  */
-      label = BB_END (last);
-      while (label && !LABEL_P (label))
-	{
-	  if (active_insn_p (label))
-	    break;
-	  label = PREV_INSN (label);
-	}
+      src_bbs = VEC_alloc (basic_block, heap, EDGE_COUNT (last_bb->preds));
+      FOR_EACH_EDGE (e, ei2, last_bb->preds)
+	if (e->src != ENTRY_BLOCK_PTR)
+	  VEC_quick_push (basic_block, src_bbs, e->src);
 
-      if (BB_HEAD (last) == label && LABEL_P (label))
+      FOR_EACH_VEC_ELT (basic_block, src_bbs, i, bb)
 	{
-	  edge_iterator ei2;
+	  bool simple_p;
+	  rtx jump;
+	  e = find_edge (bb, last_bb);
 
-	  for (ei2 = ei_start (last->preds); (e = ei_safe_edge (ei2)); )
-	    {
-	      basic_block bb = e->src;
-	      rtx jump;
-
-	      if (bb == ENTRY_BLOCK_PTR)
-		{
-		  ei_next (&ei2);
-		  continue;
-		}
+	  jump = BB_END (bb);
 
-	      jump = BB_END (bb);
-	      if (!JUMP_P (jump) || JUMP_LABEL (jump) != label)
-		{
-		  ei_next (&ei2);
-		  continue;
-		}
+#ifdef HAVE_simple_return
+	  simple_p = (entry_edge != orig_entry_edge
+		      ? !bitmap_bit_p (&bb_flags, bb->index) : false);
+#else
+	  simple_p = false;
+#endif
 
-	      /* If we have an unconditional jump, we can replace that
-		 with a simple return instruction.  */
-	      if (simplejump_p (jump))
-		{
-		  emit_return_into_block (bb);
-		  delete_insn (jump);
-		}
+	  if (!simple_p
+	      && (!HAVE_return || !JUMP_P (jump)
+		  || JUMP_LABEL (jump) != label))
+	    continue;
 
-	      /* If we have a conditional jump, we can try to replace
-		 that with a conditional return instruction.  */
-	      else if (condjump_p (jump))
-		{
-		  if (! redirect_jump (jump, 0, 0))
-		    {
-		      ei_next (&ei2);
-		      continue;
-		    }
+	  /* If we have an unconditional jump, we can replace that
+	     with a simple return instruction.  */
+	  if (!JUMP_P (jump))
+	    {
+	      emit_barrier_after (BB_END (bb));
+	      emit_return_into_block (simple_p, bb);
+	    }
+	  else if (simplejump_p (jump))
+	    {
+	      emit_return_into_block (simple_p, bb);
+	      delete_insn (jump);
+	    }
+	  else if (condjump_p (jump) && JUMP_LABEL (jump) != label)
+	    {
+	      basic_block new_bb;
+	      edge new_e;
+
+	      gcc_assert (simple_p);
+	      new_bb = split_edge (e);
+	      emit_barrier_after (BB_END (new_bb));
+	      emit_return_into_block (simple_p, new_bb);
+#ifdef HAVE_simple_return
+	      simple_return_block = new_bb;
+#endif
+	      new_e = single_succ_edge (new_bb);
+	      redirect_edge_succ (new_e, EXIT_BLOCK_PTR);
 
-		  /* If this block has only one successor, it both jumps
-		     and falls through to the fallthru block, so we can't
-		     delete the edge.  */
-		  if (single_succ_p (bb))
-		    {
-		      ei_next (&ei2);
-		      continue;
-		    }
-		}
+	      continue;
+	    }
+	  /* If we have a conditional jump branching to the last
+	     block, we can try to replace that with a conditional
+	     return instruction.  */
+	  else if (condjump_p (jump))
+	    {
+	      rtx dest;
+	      if (simple_p)
+		dest = simple_return_rtx;
 	      else
+		dest = ret_rtx;
+	      if (! redirect_jump (jump, dest, 0))
 		{
-		  ei_next (&ei2);
+#ifdef HAVE_simple_return
+		  if (simple_p)
+		    unconverted_simple_returns = true;
+#endif
 		  continue;
 		}
 
-	      /* Fix up the CFG for the successful change we just made.  */
-	      redirect_edge_succ (e, EXIT_BLOCK_PTR);
+	      /* If this block has only one successor, it both jumps
+		 and falls through to the fallthru block, so we can't
+		 delete the edge.  */
+	      if (single_succ_p (bb))
+		continue;
 	    }
+	  else
+	    {
+#ifdef HAVE_simple_return
+	      if (simple_p)
+		unconverted_simple_returns = true;
+#endif
+	      continue;
+	    }
+
+	  /* Fix up the CFG for the successful change we just made.  */
+	  redirect_edge_succ (e, EXIT_BLOCK_PTR);
+	}
+      VEC_free (basic_block, heap, src_bbs);
 
+      if (HAVE_return)
+	{
 	  /* Emit a return insn for the exit fallthru block.  Whether
 	     this is still reachable will be determined later.  */
 
-	  emit_barrier_after (BB_END (last));
-	  emit_return_into_block (last);
-	  epilogue_end = BB_END (last);
-	  single_succ_edge (last)->flags &= ~EDGE_FALLTHRU;
+	  emit_barrier_after (BB_END (last_bb));
+	  emit_return_into_block (false, last_bb);
+	  epilogue_end = BB_END (last_bb);
+	  if (JUMP_P (epilogue_end))
+	    JUMP_LABEL (epilogue_end) = ret_rtx;
+	  single_succ_edge (last_bb)->flags &= ~EDGE_FALLTHRU;
 	  goto epilogue_done;
 	}
     }
@@ -5493,13 +5962,10 @@ thread_prologue_and_epilogue_insns (void)
     }
 #endif
 
-  /* Find the edge that falls through to EXIT.  Other edges may exist
-     due to RETURN instructions, but those don't need epilogues.
-     There really shouldn't be a mixture -- either all should have
-     been converted or none, however...  */
+  /* If nothing falls through into the exit block, we don't need an
+     epilogue.  */
 
-  e = find_fallthru_edge (EXIT_BLOCK_PTR->preds);
-  if (e == NULL)
+  if (exit_fallthru_edge == NULL)
     goto epilogue_done;
 
 #ifdef HAVE_epilogue
@@ -5516,25 +5982,38 @@ thread_prologue_and_epilogue_insns (void)
       set_insn_locators (seq, epilogue_locator);
 
       seq = get_insns ();
+      returnjump = get_last_insn ();
       end_sequence ();
 
-      insert_insn_on_edge (seq, e);
+      insert_insn_on_edge (seq, exit_fallthru_edge);
       inserted = true;
+      if (JUMP_P (returnjump))
+	{
+	  rtx pat = PATTERN (returnjump);
+	  if (GET_CODE (pat) == PARALLEL)
+	    pat = XVECEXP (pat, 0, 0);
+	  if (ANY_RETURN_P (pat))
+	    JUMP_LABEL (returnjump) = pat;
+	  else
+	    JUMP_LABEL (returnjump) = ret_rtx;
+	}
+      else
+	returnjump = NULL_RTX;
     }
   else
 #endif
     {
       basic_block cur_bb;
 
-      if (! next_active_insn (BB_END (e->src)))
+      if (! next_active_insn (BB_END (exit_fallthru_edge->src)))
 	goto epilogue_done;
       /* We have a fall-through edge to the exit block, the source is not
          at the end of the function, and there will be an assembler epilogue
          at the end of the function.
          We can't use force_nonfallthru here, because that would try to
-         use return.  Inserting a jump 'by hand' is extremely messy, so
+	 use return.  Inserting a jump 'by hand' is extremely messy, so
 	 we take advantage of cfg_layout_finalize using
-	fixup_fallthru_exit_predecessor.  */
+	 fixup_fallthru_exit_predecessor.  */
       cfg_layout_initialize (0);
       FOR_EACH_BB (cur_bb)
 	if (cur_bb->index >= NUM_FIXED_BLOCKS
@@ -5543,6 +6022,7 @@ thread_prologue_and_epilogue_insns (void)
       cfg_layout_finalize ();
     }
 epilogue_done:
+
   default_rtl_profile ();
 
   if (inserted)
@@ -5559,33 +6039,93 @@ epilogue_done:
 	}
     }
 
+#ifdef HAVE_simple_return
+  /* If there were branches to an empty LAST_BB which we tried to
+     convert to conditional simple_returns, but couldn't for some
+     reason, create a block to hold a simple_return insn and redirect
+     those remaining edges.  */
+  if (unconverted_simple_returns)
+    {
+      edge_iterator ei2;
+      basic_block exit_pred = EXIT_BLOCK_PTR->prev_bb;
+
+      gcc_assert (entry_edge != orig_entry_edge);
+
+#ifdef HAVE_epilogue
+      if (simple_return_block == NULL && returnjump != NULL_RTX
+	  && JUMP_LABEL (returnjump) == simple_return_rtx)
+	{
+	  edge e = split_block (exit_fallthru_edge->src,
+				PREV_INSN (returnjump));
+	  simple_return_block = e->dest;
+	}
+#endif
+      if (simple_return_block == NULL)
+	{
+	  basic_block bb;
+	  rtx start;
+
+	  bb = create_basic_block (NULL, NULL, exit_pred);
+	  start = emit_jump_insn_after (gen_simple_return (),
+					BB_END (bb));
+	  JUMP_LABEL (start) = simple_return_rtx;
+	  emit_barrier_after (start);
+
+	  simple_return_block = bb;
+	  make_edge (bb, EXIT_BLOCK_PTR, 0);
+	}
+
+    restart_scan:
+      for (ei2 = ei_start (last_bb->preds); (e = ei_safe_edge (ei2)); )
+	{
+	  basic_block bb = e->src;
+
+	  if (bb != ENTRY_BLOCK_PTR
+	      && !bitmap_bit_p (&bb_flags, bb->index))
+	    {
+	      redirect_edge_and_branch_force (e, simple_return_block);
+	      goto restart_scan;
+	    }
+	  ei_next (&ei2);
+
+	}
+    }
+#endif
+
 #ifdef HAVE_sibcall_epilogue
   /* Emit sibling epilogues before any sibling call sites.  */
   for (ei = ei_start (EXIT_BLOCK_PTR->preds); (e = ei_safe_edge (ei)); )
     {
       basic_block bb = e->src;
       rtx insn = BB_END (bb);
+      rtx ep_seq;
 
       if (!CALL_P (insn)
-	  || ! SIBLING_CALL_P (insn))
+	  || ! SIBLING_CALL_P (insn)
+	  || (entry_edge != orig_entry_edge
+	      && !bitmap_bit_p (&bb_flags, bb->index)))
 	{
 	  ei_next (&ei);
 	  continue;
 	}
 
-      start_sequence ();
-      emit_note (NOTE_INSN_EPILOGUE_BEG);
-      emit_insn (gen_sibcall_epilogue ());
-      seq = get_insns ();
-      end_sequence ();
+      ep_seq = gen_sibcall_epilogue ();
+      if (ep_seq)
+	{
+	  start_sequence ();
+	  emit_note (NOTE_INSN_EPILOGUE_BEG);
+	  emit_insn (ep_seq);
+	  seq = get_insns ();
+	  end_sequence ();
 
-      /* Retain a map of the epilogue insns.  Used in life analysis to
-	 avoid getting rid of sibcall epilogue insns.  Do this before we
-	 actually emit the sequence.  */
-      record_insns (seq, NULL, &epilogue_insn_hash);
-      set_insn_locators (seq, epilogue_locator);
+	  /* Retain a map of the epilogue insns.  Used in life analysis to
+	     avoid getting rid of sibcall epilogue insns.  Do this before we
+	     actually emit the sequence.  */
+	  record_insns (seq, NULL, &epilogue_insn_hash);
+	  set_insn_locators (seq, epilogue_locator);
 
-      emit_insn_before (seq, insn);
+	  emit_insn_before (seq, insn);
+	}
       ei_next (&ei);
     }
 #endif
@@ -5610,6 +6150,8 @@ epilogue_done:
     }
 #endif
 
+  bitmap_clear (&bb_flags);
+
   /* Threading the prologue and epilogue changes the artificial refs
      in the entry and exit blocks.  */
   epilogue_completed = 1;
@@ -5731,6 +6273,57 @@ current_function_name (void)
     return "<none>";
   return lang_hooks.decl_printable_name (cfun->decl, 2);
 }
+
+/* This function adjusts alignments as appropriate according to the
+   setting of -falign-arrays.  If that is specified then the minimum
+   alignment for array variables is set to be the largest power of two
+   less than or equal to their total storage size, or the biggest
+   alignment used on the machine, whichever is smaller.  */
+
+unsigned int
+alignment_for_aligned_arrays (tree ty, unsigned int existing_alignment)
+{
+  unsigned int min_alignment;
+  tree size;
+
+  /* Return the existing alignment if not using -falign-arrays or if
+     the type is not an array type.  */
+  if (!flag_align_arrays || !ty || TREE_CODE (ty) != ARRAY_TYPE)
+    return existing_alignment;
+
+  /* Extract the total storage size of the array in bits.  */
+  size = TYPE_SIZE (ty);
+  gcc_assert (size);
+
+  /* At least for variable-length arrays, TREE_CODE (size) might not be an
+     integer constant; check it now.  If it is not, give the array at
+     least BIGGEST_ALIGNMENT just to be safe.   Furthermore, we assume that
+     alignments always fit into a host integer.  So if we can't fit the
+     size of the array in bits into a host integer, it must also be large
+     enough to deserve at least BIGGEST_ALIGNMENT (see below).  */
+  if (TREE_CODE (size) != INTEGER_CST || !host_integerp (size, 1))
+    min_alignment = BIGGEST_ALIGNMENT;
+  else
+    {
+      unsigned HOST_WIDE_INT bits = TREE_INT_CST_LOW (size);
+      bits = (bits ? bits : 1);
+
+      /* An array with size greater than BIGGEST_ALIGNMENT is assigned
+	 at least that alignment.  In all other cases the minimum
+	 alignment of the array is set to be the largest power of two
+	 less than or equal to the total storage size of the array.
+	 We assume that BIGGEST_ALIGNMENT fits in "unsigned int"; thus,
+	 the shift below will not overflow.  */
+      if (bits >= BIGGEST_ALIGNMENT)
+	min_alignment = BIGGEST_ALIGNMENT;
+      else
+	min_alignment = 1 << (floor_log2 (bits));
+    }
+
+  /* Having computed the minimum permissible alignment, enlarge it
+     if EXISTING_ALIGNMENT is greater.  */
+  return MAX (min_alignment, existing_alignment); 
+}
 
 
 static unsigned int
diff --git a/gcc/function.h b/gcc/function.h
index 6e7f539..7bb34fb 100644
--- a/gcc/function.h
+++ b/gcc/function.h
@@ -755,6 +755,8 @@ extern void used_types_insert (tree);
 
 extern int get_next_funcdef_no (void);
 
+extern unsigned int alignment_for_aligned_arrays (tree, unsigned int);
+
 /* In predict.c */
 extern bool optimize_function_for_size_p (struct function *);
 extern bool optimize_function_for_speed_p (struct function *);
diff --git a/gcc/gcc.c b/gcc/gcc.c
index 6efd037..d86e255 100644
--- a/gcc/gcc.c
+++ b/gcc/gcc.c
@@ -44,6 +44,13 @@ compilation is specified by a string called a "spec".  */
 #include "flags.h"
 #include "opts.h"
 #include "vec.h"
+#ifdef CSL_LICENSE_FEATURE
+#  include <csl/license.h>
+#else
+   /* TARGET_FLEXLM requires the CodeSourcery license library be
+      present.  */
+#  undef TARGET_FLEXLM
+#endif
 
 /* By default there is no special suffix for target executables.  */
 /* FIXME: when autoconf is fixed, remove the host check - dj */
@@ -144,6 +151,28 @@ static enum save_temps {
 static char *save_temps_prefix = 0;
 static size_t save_temps_length = 0;
 
+/* Nonzero means that libgcc is being linked automatically by the
+   compiler from its normal installed location; that is, neither -B,
+   -nostdlib nor -nodefaultlibs was passed.  */
+
+static int using_libgcc = 1;
+
+/* Nonzero means that the current spec is executing the linker.  */
+
+static int executing_linker = 0;
+
+#ifdef CSL_LICENSE_FEATURE
+/* 0 if we have not checked for a license, 1 if a license was
+   obtained, -1 if license checkout failed.  */
+   
+static int license_checked = 0;
+
+# ifndef TARGET_FLEXLM
+#  undef license_me_flag
+#  define license_me_flag 1
+# endif /* defined (TARGET_FLELM) */
+#endif /* defined (CSL_LICENSE_FEATURE) */
+
 /* The compiler version.  */
 
 static const char *compiler_version;
@@ -659,6 +688,8 @@ proper position among the other output files.  */
     %{flto} %{flto=*} %l " LINK_PIE_SPEC \
    "%X %{o*} %{e*} %{N} %{n} %{r}\
     %{s} %{t} %{u*} %{z} %{Z} %{!nostdlib:%{!nostartfiles:%S}}\
+    %{Wno-poison-system-directories:--no-poison-system-directories}\
+    %{Werror=poison-system-directories:--error-poison-system-directories}\
     %{static:} %{L*} %(mfwrap) %(link_libgcc) %o\
     %{fopenmp|ftree-parallelize-loops=*:%:include(libgomp.spec)%(link_gomp)}\
     %(mflib) " STACK_SPLIT_SPEC "\
@@ -2473,6 +2504,29 @@ execute (void)
 
   gcc_assert (!processing_spec_function);
 
+  if (executing_linker && using_libgcc)
+    {
+      const char *libgcc_a_filename;
+
+      /* Verify that the multilib being used is actually installed.  */
+      libgcc_a_filename = (gcc_exec_prefix
+			   ? gcc_exec_prefix
+			   : concat (standard_exec_prefix,
+				     machine_suffix, NULL));
+      if (multilib_dir && strcmp (multilib_dir, ".") != 0)
+	libgcc_a_filename = concat (libgcc_a_filename, multilib_dir,
+				    dir_separator_str, NULL);
+      libgcc_a_filename = concat (libgcc_a_filename, "libgcc.a", NULL);
+      if (access (libgcc_a_filename, R_OK) != 0)
+	{
+	  if (errno == ENOENT)
+	    fatal_error ("selected multilib %qs not installed",
+			 multilib_dir ? multilib_dir : ".");
+	  else
+	    pfatal_with_name (libgcc_a_filename);
+	}
+    }
+
   if (wrapper_string)
     {
       string = find_a_file (&exec_prefixes,
@@ -3432,6 +3486,23 @@ driver_handle_option (struct gcc_options *opts,
       do_save = false;
       break;
 
+#ifdef TARGET_FLEXLM
+    case OPT_flicense_me:
+    case OPT_ffeature_proxy:
+    case OPT_fno_feature_proxy:
+      /* These variables are all set automatically via common.opt.  */
+      do_save = false;
+      break;
+
+      /* WRS LOCAL only invoke get_feature if we are running the
+	 compiler proper.  */
+    case OPT_E:
+    case OPT_M:
+    case OPT_MM:
+      license_checked = 1;
+      break;
+#endif
+
     case OPT_B:
       {
 	size_t len = strlen (arg);
@@ -3460,6 +3531,7 @@ driver_handle_option (struct gcc_options *opts,
 	add_prefix (&include_prefixes, arg, NULL,
 		    PREFIX_PRIORITY_B_OPT, 0, 0);
       }
+      using_libgcc = 0;
       validated = true;
       break;
 
@@ -3497,6 +3569,11 @@ driver_handle_option (struct gcc_options *opts,
       validated = true;
       break;
 
+    case OPT_nodefaultlibs:
+    case OPT_nostdlib:
+      using_libgcc = 0;
+      break;
+
     default:
       /* Various driver options need no special processing at this
 	 point, having been handled in a prescan above or being
@@ -6135,6 +6212,10 @@ main (int argc, char **argv)
   char **old_argv = argv;
   struct cl_decoded_option *decoded_options;
   unsigned int decoded_options_count;
+#ifdef CSL_LICENSE_FEATURE
+  csl_license_impl *license_impl = csl_license_subproc;
+  csl_license *license = NULL;
+#endif
 
   /* Initialize here, not in definition.  The IRIX 6 O32 cc sometimes chokes
      on ?: in file-scope variable initializations.  */
@@ -6749,7 +6830,59 @@ warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n"
 		  debug_check_temp_file[1] = NULL;
 		}
 
-	      value = do_spec (input_file_compiler->spec);
+	      value = 0;
+	      
+#ifdef CSL_LICENSE_FEATURE
+	      if (!license_checked)
+		{
+		  const char *subproc, *found_subproc;
+# ifdef TARGET_FLEXLM
+		  const char *subproc_argv[10];
+		  const char **p;
+		  subproc = "get_feature";
+		  p = subproc_argv;
+		  *p++ = subproc;
+		  if (feature_proxy_flag)
+		    *p++ = "-p";
+		  *p++ = "-co";
+		  *p++ = xstrdup (DEFAULT_TARGET_MACHINE);
+		  *p++ = "-v";
+		  *p++ = "3.3";
+		  *p++ = "gnu";
+		  *p++ = infiles[i].language;
+		  *p++ = (license_me_flag ? "-flicense-me" : "");
+		  *p++ = 0;
+# else /* !defined (TARGET_FLEXLM) */
+		  const char **subproc_argv = NULL;
+		  subproc = CSL_LICENSE_PROG;
+# endif /* !defined (TARGET_FLEXLM) */
+		  /* Find the licensing program.  */
+		  found_subproc = find_a_file (&exec_prefixes,
+					       subproc,
+					       X_OK,
+					       /*multilib=*/false);
+		  if (found_subproc)
+		    subproc = found_subproc;
+		  /* Begin checking out the license.  */
+		  license
+		    = csl_subproc_license_new (CSL_LICENSE_FEATURE,
+					       CSL_LICENSE_VERSION,
+					       /*argcp=*/NULL,
+					       /*argvp=*/NULL,
+					       subproc,
+					       subproc_argv);
+		  if (!license)
+		    {
+		      error ("could not invoke license program");
+		      license_checked = -1;
+		    }
+		}
+	      if (license_checked == -1 && license_me_flag)
+		value = -1;
+#endif /* defined (CSL_LICENSE_FEATURE) */
+	      /* Now do the compile.  */
+	      if (!value)
+		value = do_spec (input_file_compiler->spec);
 	      infiles[i].compiled = true;
 	      if (value < 0)
 		this_file_error = 1;
@@ -6786,6 +6919,28 @@ warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n"
 		  if (compare_files (debug_check_temp_file))
 		    this_file_error = 1;
 		}
+#ifdef CSL_LICENSE_FEATURE
+	      if (!license_checked && license)
+		{
+		  /* Finish checking out the license.  */
+		  const csl_license_status *license_status;
+		  if (!license_impl->license_check (license,
+						    &license_status))
+		    {
+		      if (license_me_flag) /* WRS LOCAL */
+			error ("%s", license_status->msg);
+		      /* Remember that the license check failed so
+			 that we (a) do not check again, and (b) issue
+			 errors about other files as well.  */
+		      license_checked = -1;
+		      if (license_me_flag) /* WRS LOCAL */
+			/* Remove this file.  */
+			this_file_error = 1;
+		    }
+		  else
+		    license_checked = 1;
+		}
+#endif /* defined (CSL_LICENSE_FEATURE) */
 
 	      if (compare_debug)
 		{
@@ -6897,7 +7052,9 @@ warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n"
 		    " to the linker.\n\n"));
 	  fflush (stdout);
 	}
+      executing_linker = 1;
       value = do_spec (link_command_spec);
+      executing_linker = 0;
       if (value < 0)
 	errorcount = 1;
       linker_was_run = (tmp != execution_count);
@@ -6925,6 +7082,12 @@ warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n"
       printf ("%s\n", bug_report_url);
     }
 
+#ifdef CSL_LICENSE_FEATURE
+  /* Relinquish the license.  */
+  if (license)
+    license_impl->license_delete (license);
+#endif
+
  out:
   return (signal_count != 0 ? 2
 	  : seen_error () ? (pass_exit_codes ? greatest_status : 1)
diff --git a/gcc/genautomata.c b/gcc/genautomata.c
index cbf8343..5e0028d 100644
--- a/gcc/genautomata.c
+++ b/gcc/genautomata.c
@@ -9131,44 +9131,43 @@ base_file_name (const char *file_name)
   return file_name + directory_name_length + 1;
 }
 
+/* A function passed as argument to init_rtx_reader_args_cb.  It parses the
+   options available for genautomata.  Returns true if the option was
+   recognized.  */
+static bool
+parse_automata_opt (const char *str)
+{
+  if (strcmp (str, NO_MINIMIZATION_OPTION) == 0)
+    no_minimization_flag = 1;
+  else if (strcmp (str, TIME_OPTION) == 0)
+    time_flag = 1;
+  else if (strcmp (str, STATS_OPTION) == 0)
+    stats_flag = 1;
+  else if (strcmp (str, V_OPTION) == 0)
+    v_flag = 1;
+  else if (strcmp (str, W_OPTION) == 0)
+    w_flag = 1;
+  else if (strcmp (str, NDFA_OPTION) == 0)
+    ndfa_flag = 1;
+  else if (strcmp (str, PROGRESS_OPTION) == 0)
+    progress_flag = 1;
+  else if (strcmp (str, "-split") == 0)
+    {
+      fatal ("option `-split' has not been implemented yet\n");
+      /* split_argument = atoi (argument_vect [i + 1]); */
+    }
+  else
+    return false;
+
+  return true;
+}
+
 /* The following is top level function to initialize the work of
    pipeline hazards description translator.  */
 static void
 initiate_automaton_gen (int argc, char **argv)
 {
   const char *base_name;
-  int i;
-
-  ndfa_flag = 0;
-  split_argument = 0;  /* default value */
-  no_minimization_flag = 0;
-  time_flag = 0;
-  stats_flag = 0;
-  v_flag = 0;
-  w_flag = 0;
-  progress_flag = 0;
-  for (i = 2; i < argc; i++)
-    if (strcmp (argv [i], NO_MINIMIZATION_OPTION) == 0)
-      no_minimization_flag = 1;
-    else if (strcmp (argv [i], TIME_OPTION) == 0)
-      time_flag = 1;
-    else if (strcmp (argv [i], STATS_OPTION) == 0)
-      stats_flag = 1;
-    else if (strcmp (argv [i], V_OPTION) == 0)
-      v_flag = 1;
-    else if (strcmp (argv [i], W_OPTION) == 0)
-      w_flag = 1;
-    else if (strcmp (argv [i], NDFA_OPTION) == 0)
-      ndfa_flag = 1;
-    else if (strcmp (argv [i], PROGRESS_OPTION) == 0)
-      progress_flag = 1;
-    else if (strcmp (argv [i], "-split") == 0)
-      {
-	if (i + 1 >= argc)
-	  fatal ("-split has no argument.");
-	fatal ("option `-split' has not been implemented yet\n");
-	/* split_argument = atoi (argument_vect [i + 1]); */
-      }
 
   /* Initialize IR storage.  */
   obstack_init (&irp);
@@ -9462,7 +9461,7 @@ main (int argc, char **argv)
 
   progname = "genautomata";
 
-  if (!init_rtx_reader_args (argc, argv))
+  if (!init_rtx_reader_args_cb (argc, argv, parse_automata_opt))
     return (FATAL_EXIT_CODE);
 
   initiate_automaton_gen (argc, argv);
diff --git a/gcc/genemit.c b/gcc/genemit.c
index 6b1ae7d..9a08cae 100644
--- a/gcc/genemit.c
+++ b/gcc/genemit.c
@@ -223,6 +223,12 @@ gen_exp (rtx x, enum rtx_code subroutine_type, char *used)
     case PC:
       printf ("pc_rtx");
       return;
+    case RETURN:
+      printf ("ret_rtx");
+      return;
+    case SIMPLE_RETURN:
+      printf ("simple_return_rtx");
+      return;
     case CLOBBER:
       if (REG_P (XEXP (x, 0)))
 	{
@@ -546,8 +552,8 @@ gen_expand (rtx expand)
 	  || (GET_CODE (next) == PARALLEL
 	      && ((GET_CODE (XVECEXP (next, 0, 0)) == SET
 		   && GET_CODE (SET_DEST (XVECEXP (next, 0, 0))) == PC)
-		  || GET_CODE (XVECEXP (next, 0, 0)) == RETURN))
-	  || GET_CODE (next) == RETURN)
+		  || ANY_RETURN_P (XVECEXP (next, 0, 0))))
+	  || ANY_RETURN_P (next))
 	printf ("  emit_jump_insn (");
       else if ((GET_CODE (next) == SET && GET_CODE (SET_SRC (next)) == CALL)
 	       || GET_CODE (next) == CALL
@@ -665,7 +671,7 @@ gen_split (rtx split)
 	  || (GET_CODE (next) == PARALLEL
 	      && GET_CODE (XVECEXP (next, 0, 0)) == SET
 	      && GET_CODE (SET_DEST (XVECEXP (next, 0, 0))) == PC)
-	  || GET_CODE (next) == RETURN)
+	  || ANY_RETURN_P (next))
 	printf ("  emit_jump_insn (");
       else if ((GET_CODE (next) == SET && GET_CODE (SET_SRC (next)) == CALL)
 	       || GET_CODE (next) == CALL
diff --git a/gcc/gengenrtl.c b/gcc/gengenrtl.c
index fc53028..67688ac 100644
--- a/gcc/gengenrtl.c
+++ b/gcc/gengenrtl.c
@@ -128,6 +128,10 @@ special_rtx (int idx)
 	  || strcmp (defs[idx].enumname, "REG") == 0
 	  || strcmp (defs[idx].enumname, "SUBREG") == 0
 	  || strcmp (defs[idx].enumname, "MEM") == 0
+	  || strcmp (defs[idx].enumname, "PC") == 0
+	  || strcmp (defs[idx].enumname, "CC0") == 0
+	  || strcmp (defs[idx].enumname, "RETURN") == 0
+	  || strcmp (defs[idx].enumname, "SIMPLE_RETURN") == 0
 	  || strcmp (defs[idx].enumname, "CONST_VECTOR") == 0);
 }
 
diff --git a/gcc/genmultilib b/gcc/genmultilib
index 154ff4b..1a69f8d 100644
--- a/gcc/genmultilib
+++ b/gcc/genmultilib
@@ -75,6 +75,20 @@
 
 # The optional eighth argument is the multiarch name.
 
+# The optional eighth option is a list of multilib aliases.  This takes the
+# same form as the third argument.  It specifies that the second multilib is
+# a synonym for the first.  This allows a suitable multilib to be selected
+# for all option combinations while only building a subset of all possible
+# multilibs.
+# For example:
+#   genmultilib "mbig-endian mthumb" "eb thumb" "" "" "" "" "" \
+#		"mbig-endian=mbig-endian/mthumb" yes
+# This produces:
+#   ". !mbig-endian !mthumb;",
+#   "be mbig-endian !mthumb;",
+#   "be mbig-endian mthumb;",
+#   "thumb !mbig-endian mthumb;",
+
 # The last option should be "yes" if multilibs are enabled.  If it is not
 # "yes", all GCC multilib dir names will be ".".
 
@@ -123,7 +137,7 @@ exceptions=$4
 extra=$5
 exclusions=$6
 osdirnames=$7
-multiarch=$8
+aliases=$8
 enable_multilib=$9
 
 echo "static const char *const multilib_raw[] = {"
@@ -132,6 +146,23 @@ mkdir tmpmultilib.$$ || exit 1
 # Use cd ./foo to avoid CDPATH output.
 cd ./tmpmultilib.$$ || exit 1
 
+# Handle aliases
+cat >tmpmultilib3 <<\EOF
+#!/bin/sh
+# Output a list of aliases (including the original name) for a multilib.
+
+echo $1
+EOF
+for a in ${aliases}; do
+  l=`echo $a | sed -e 's/=.*$//' -e 's/?/=/g'`
+  r=`echo $a | sed -e 's/^.*=//' -e 's/?/=/g'`
+  echo "[ \$1 == /$l/ ] && echo /$r/" >>tmpmultilib3
+  
+  # Also add the alias to the exclusion list
+  exceptions="${exceptions} $r"
+done
+chmod +x tmpmultilib3
+
 # What we want to do is select all combinations of the sets in
 # options.  Each combination which includes a set of mutually
 # exclusive options must then be output multiple times, once for each
@@ -198,6 +229,21 @@ EOF
   combinations=`./tmpmultilib2 ${combinations}`
 fi
 
+# Check that all the aliases actually exist
+for a in ${aliases}; do
+  l=`echo $a | sed -e 's/=.*$//' -e 's/?/=/g'`
+  for c in ${combinations}; do
+    if [ "/$l/" = "$c" ]; then
+      l=""
+      break;
+    fi
+  done
+  if [ -n "$l" ] ;then
+    echo "Missing multilib $l for alias $a" 1>&2
+    exit 1
+  fi
+done
+
 # Construct a sed pattern which will convert option names to directory
 # names.
 todirnames=
@@ -352,23 +398,25 @@ for combo in ${combinations}; do
     fi
   fi
 
-  # Look through the options.  We must output each option that is
-  # present, and negate each option that is not present.
-  optout=
-  for set in ${options}; do
-    setopts=`echo ${set} | sed -e 's_[/|]_ _g'`
-    for opt in ${setopts}; do
-      if expr "${combo} " : ".*/${opt}/.*" > /dev/null; then
-	optout="${optout} ${opt}"
-      else
-	optout="${optout} !${opt}"
-      fi
+  for optcombo in `./tmpmultilib3 ${combo}`; do
+    # Look through the options.  We must output each option that is
+    # present, and negate each option that is not present.
+    optout=
+    for set in ${options}; do
+      setopts=`echo ${set} | sed -e 's_[/|]_ _g'`
+      for opt in ${setopts}; do
+	if expr "${optcombo} " : ".*/${opt}/.*" > /dev/null; then
+	  optout="${optout} ${opt}"
+	else
+	  optout="${optout} !${opt}"
+	fi
+      done
     done
-  done
-  optout=`echo ${optout} | sed -e 's/^ //'`
+    optout=`echo ${optout} | sed -e 's/^ //'`
 
-  # Output the line with all appropriate matches.
-  dirout="${dirout}" optout="${optout}" ./tmpmultilib2
+    # Output the line with all appropriate matches.
+    dirout="${dirout}" optout="${optout}" ./tmpmultilib2
+  done
 done
 
 # Terminate the list of string.
diff --git a/gcc/gimplify.c b/gcc/gimplify.c
index 1af6de6..a08f44e 100644
--- a/gcc/gimplify.c
+++ b/gcc/gimplify.c
@@ -1117,6 +1117,62 @@ build_stack_save_restore (gimple *save, gimple *restore)
 			    1, tmp_var);
 }
 
+/* Returns the function decl that corresponds the function called in
+   CALL_EXPR if FLAG_INSTRUMENT_CALL_ENTRY_EXIT is enabled.  */
+
+static tree
+fndecl_for_call_instrumentation (tree call_expr)
+{
+  tree fndecl = NULL_TREE;
+
+  if (flag_instrument_call_entry_exit && !gimplify_ctxp->into_ssa)
+    {
+      fndecl = get_callee_fndecl (call_expr);
+      if (is_inexpensive_builtin (fndecl))
+	  fndecl = NULL_TREE;
+    }
+
+  return fndecl;
+}
+
+/* Prepare call to PROFILE_CALL_* builtin (specified by CODE) for
+   function with decl FNDECL and add it to the sequence of GIMPLE
+   statements in PRE_P.  */
+
+static void
+maybe_add_profile_call (tree fndecl, enum built_in_function code,
+			gimple_seq *pre_p)
+{
+  if (fndecl)
+    {
+      tree x = implicit_built_in_decls[code];
+      gimple call = gimple_build_call (x, 1, build_fold_addr_expr (fndecl));
+      gimplify_seq_add_stmt (pre_p, call);
+    }
+}
+
+/* Replace existing body of function with decl FNDECL with the sequence
+   of GIMPLE statements in BODY and return the new sequence of the
+   function body.  */
+
+static gimple_seq
+replace_body_for_instrumentation (tree fndecl, gimple *bind, gimple_seq body)
+{
+  gimple_seq seq;
+  gimple old_bind = *bind;
+  *bind = gimple_build_bind (NULL, body, gimple_bind_block (old_bind));
+  /* Clear the block for OLD_BIND, since it is no longer directly inside
+     the function, but within the new block.  */
+  gimple_bind_set_block (old_bind, NULL);
+
+  /* Replace the current function body with the new one.  */
+  seq = gimple_seq_alloc ();
+  gimple_seq_add_stmt (&seq, *bind);
+  gimple_set_body (fndecl, seq);
+
+  return seq;
+}
+
 /* Gimplify a BIND_EXPR.  Just voidify and recurse.  */
 
 static enum gimplify_status
@@ -2491,6 +2547,10 @@ gimplify_call_expr (tree *expr_p, gimple_seq *pre_p, bool want_value)
      gimplify_modify_expr.  */
   if (!want_value)
     {
+      tree fndecl = fndecl_for_call_instrumentation (*expr_p);
+
+      maybe_add_profile_call (fndecl, BUILT_IN_PROFILE_CALL_ENTER, pre_p);
+
       /* The CALL_EXPR in *EXPR_P is already in GIMPLE form, so all we
 	 have to do is replicate it as a GIMPLE_CALL tuple.  */
       gimple_stmt_iterator gsi;
@@ -2498,6 +2558,9 @@ gimplify_call_expr (tree *expr_p, gimple_seq *pre_p, bool want_value)
       gimplify_seq_add_stmt (pre_p, call);
       gsi = gsi_last (*pre_p);
       fold_stmt (&gsi);
+
+      maybe_add_profile_call (fndecl, BUILT_IN_PROFILE_CALL_EXIT, pre_p);
+
       *expr_p = NULL_TREE;
     }
 
@@ -4498,6 +4561,7 @@ gimplify_modify_expr (tree *expr_p, gimple_seq *pre_p, gimple_seq *post_p,
   enum gimplify_status ret = GS_UNHANDLED;
   gimple assign;
   location_t loc = EXPR_LOCATION (*expr_p);
+  tree fndecl = NULL_TREE;
 
   gcc_assert (TREE_CODE (*expr_p) == MODIFY_EXPR
 	      || TREE_CODE (*expr_p) == INIT_EXPR);
@@ -4612,6 +4676,8 @@ gimplify_modify_expr (tree *expr_p, gimple_seq *pre_p, gimple_seq *post_p,
 
   if (TREE_CODE (*from_p) == CALL_EXPR)
     {
+      fndecl = fndecl_for_call_instrumentation (*from_p);
+
       /* Since the RHS is a CALL_EXPR, we need to create a GIMPLE_CALL
 	 instead of a GIMPLE_ASSIGN.  */
       assign = gimple_build_call_from_tree (*from_p);
@@ -4624,7 +4690,9 @@ gimplify_modify_expr (tree *expr_p, gimple_seq *pre_p, gimple_seq *post_p,
       gimple_set_location (assign, EXPR_LOCATION (*expr_p));
     }
 
+  maybe_add_profile_call (fndecl, BUILT_IN_PROFILE_CALL_ENTER, pre_p);
   gimplify_seq_add_stmt (pre_p, assign);
+  maybe_add_profile_call (fndecl, BUILT_IN_PROFILE_CALL_EXIT, pre_p);
 
   if (gimplify_ctxp->into_ssa && is_gimple_reg (*to_p))
     {
@@ -7876,6 +7944,16 @@ gimplify_function_tree (tree fndecl)
   gimple_seq_add_stmt (&seq, bind);
   gimple_set_body (fndecl, seq);
 
+  if (flag_instrument_call_entry_exit)
+    {
+      gimple_seq body = NULL;
+
+      maybe_add_profile_call (fndecl, BUILT_IN_PROFILE_CALL_INSIDE, &body);
+      gimplify_seq_add_seq (&body, seq);
+
+      seq = replace_body_for_instrumentation (fndecl, &bind, body);
+    }
+
   /* If we're instrumenting function entry/exit, then prepend the call to
      the entry hook and wrap the whole function in a TRY_FINALLY_EXPR to
      catch the exit hook.  */
@@ -7885,7 +7963,6 @@ gimplify_function_tree (tree fndecl)
       && !flag_instrument_functions_exclude_p (fndecl))
     {
       tree x;
-      gimple new_bind;
       gimple tf;
       gimple_seq cleanup = NULL, body = NULL;
       tree tmp_var;
@@ -7914,16 +7991,7 @@ gimplify_function_tree (tree fndecl)
 				tmp_var);
       gimplify_seq_add_stmt (&body, call);
       gimplify_seq_add_stmt (&body, tf);
-      new_bind = gimple_build_bind (NULL, body, gimple_bind_block (bind));
-      /* Clear the block for BIND, since it is no longer directly inside
-         the function, but within a try block.  */
-      gimple_bind_set_block (bind, NULL);
-
-      /* Replace the current function body with the body
-         wrapped in the try/finally TF.  */
-      seq = gimple_seq_alloc ();
-      gimple_seq_add_stmt (&seq, new_bind);
-      gimple_set_body (fndecl, seq);
+      seq = replace_body_for_instrumentation (fndecl, &bind, body);
     }
 
   DECL_SAVED_TREE (fndecl) = NULL_TREE;
diff --git a/gcc/haifa-sched.c b/gcc/haifa-sched.c
index cc7c0da..51321c6 100644
--- a/gcc/haifa-sched.c
+++ b/gcc/haifa-sched.c
@@ -129,9 +129,9 @@ along with GCC; see the file COPYING3.  If not see
 #include "coretypes.h"
 #include "tm.h"
 #include "diagnostic-core.h"
+#include "hard-reg-set.h"
 #include "rtl.h"
 #include "tm_p.h"
-#include "hard-reg-set.h"
 #include "regs.h"
 #include "function.h"
 #include "flags.h"
@@ -148,6 +148,7 @@ along with GCC; see the file COPYING3.  If not see
 #include "cfgloop.h"
 #include "ira.h"
 #include "emit-rtl.h"  /* FIXME: Can go away once crtl is moved to rtl.h.  */
+#include "hashtab.h"
 
 #ifdef INSN_SCHEDULING
 
@@ -157,6 +158,35 @@ along with GCC; see the file COPYING3.  If not see
 
 int issue_rate;
 
+/* This can be set to true by a backend if the scheduler should not
+   enable a DCE pass.  */
+bool sched_no_dce;
+
+/* The current initiation interval used when modulo scheduling.  */
+static int modulo_ii;
+
+/* The maximum number of stages we are prepared to handle.  */
+static int modulo_max_stages;
+
+/* The number of insns that exist in each iteration of the loop.  We use this
+   to detect when we've scheduled all insns from the first iteration.  */
+static int modulo_n_insns;
+
+/* The current count of insns in the first iteration of the loop that have
+   already been scheduled.  */
+static int modulo_insns_scheduled;
+
+/* The maximum uid of insns from the first iteration of the loop.  */
+static int modulo_iter0_max_uid;
+
+/* The number of times we should attempt to backtrack when modulo scheduling.
+   Decreased each time we have to backtrack.  */
+static int modulo_backtracks_left;
+
+/* The stage in which the last insn from the original loop was
+   scheduled.  */
+static int modulo_last_stage;
+
 /* sched-verbose controls the amount of debugging output the
    scheduler prints.  It is controlled by -fsched-verbose=N:
    N>0 and no -DSR : the output is directed to stderr.
@@ -177,7 +207,12 @@ FILE *sched_dump = 0;
 struct common_sched_info_def *common_sched_info;
 
 #define INSN_TICK(INSN)	(HID (INSN)->tick)
+#define INSN_EXACT_TICK(INSN) (HID (INSN)->exact_tick)
+#define INSN_TICK_ESTIMATE(INSN) (HID (INSN)->tick_estimate)
 #define INTER_TICK(INSN) (HID (INSN)->inter_tick)
+#define FEEDS_BACKTRACK_INSN(INSN) (HID (INSN)->feeds_backtrack_insn)
+#define SHADOW_P(INSN) (HID (INSN)->shadow_p)
+#define MUST_RECOMPUTE_SPEC_P(INSN) (HID (INSN)->must_recompute_spec)
 
 /* If INSN_TICK of an instruction is equal to INVALID_TICK,
    then it should be recalculated from scratch.  */
@@ -302,6 +337,22 @@ static struct ready_list *readyp = &ready;
 /* Scheduling clock.  */
 static int clock_var;
 
+/* Clock at which the previous instruction was issued.  */
+static int last_clock_var;
+
+/* Set to true if, when queuing a shadow insn, we discover that it would be
+   scheduled too late.  */
+static bool must_backtrack;
+
+/* The following variable value is number of essential insns issued on
+   the current cycle.  An insn is essential one if it changes the
+   processors state.  */
+int cycle_issued_insns;
+
+/* This records the actual schedule.  It is built up during the main phase
+   of schedule_block, and afterwards used to reorder the insns in the RTL.  */
+static VEC(rtx, heap) *scheduled_insns;
+
 static int may_trap_exp (const_rtx, int);
 
 /* Nonzero iff the address is comprised from at most 1 register.  */
@@ -483,13 +534,267 @@ haifa_classify_insn (const_rtx insn)
 {
   return haifa_classify_rtx (PATTERN (insn));
 }
+
+/* After the scheduler initialization function has been called, this function
+   can be called to enable modulo scheduling.  II is the initiation interval
+   we should use, it affects the delays for delay_pairs that were recorded as
+   separated by a given number of stages.
+
+   MAX_STAGES provides us with a limit
+   after which we give up scheduling; the caller must have unrolled at least
+   as many copies of the loop body and recorded delay_pairs for them.
+   
+   INSNS is the number of real (non-debug) insns in one iteration of
+   the loop.  MAX_UID can be used to test whether an insn belongs to
+   the first iteration of the loop; all of them have a uid lower than
+   MAX_UID.  */
+void
+set_modulo_params (int ii, int max_stages, int insns, int max_uid)
+{
+  modulo_ii = ii;
+  modulo_max_stages = max_stages;
+  modulo_n_insns = insns;
+  modulo_iter0_max_uid = max_uid;
+  modulo_backtracks_left = PARAM_VALUE (PARAM_MAX_MODULO_BACKTRACK_ATTEMPTS);
+}
+
+/* A structure to record a pair of insns where the first one is a real
+   insn that has delay slots, and the second is its delayed shadow.
+   I1 is scheduled normally and will emit an assembly instruction,
+   while I2 describes the side effect that takes place at the
+   transition between cycles CYCLES and (CYCLES + 1) after I1.  */
+struct delay_pair
+{
+  struct delay_pair *next_same_i1;
+  rtx i1, i2;
+  int cycles;
+  /* When doing modulo scheduling, we a delay_pair can also be used to
+     show that I1 and I2 are the same insn in a different stage.  If that
+     is the case, STAGES will be nonzero.  */
+  int stages;
+};
+
+/* Two hash tables to record delay_pairs, one indexed by I1 and the other
+   indexed by I2.  */
+static htab_t delay_htab;
+static htab_t delay_htab_i2;
+
+/* Called through htab_traverse.  Walk the hashtable using I2 as
+   index, and delete all elements involving an UID higher than
+   that pointed to by *DATA.  */
+static int
+htab_i2_traverse (void **slot, void *data)
+{
+  int maxuid = *(int *)data;
+  struct delay_pair *p = *(struct delay_pair **)slot;
+  if (INSN_UID (p->i2) >= maxuid || INSN_UID (p->i1) >= maxuid)
+    {
+      htab_clear_slot (delay_htab_i2, slot);
+    }
+  return 1;
+}
+
+/* Called through htab_traverse.  Walk the hashtable using I2 as
+   index, and delete all elements involving an UID higher than
+   that pointed to by *DATA.  */
+static int
+htab_i1_traverse (void **slot, void *data)
+{
+  int maxuid = *(int *)data;
+  struct delay_pair **pslot = (struct delay_pair **)slot;
+  struct delay_pair *p, *first, **pprev;
+
+  if (INSN_UID ((*pslot)->i1) >= maxuid)
+    {
+      htab_clear_slot (delay_htab, slot);
+      return 1;
+    }
+  pprev = &first;
+  for (p = *pslot; p; p = p->next_same_i1)
+    {
+      if (INSN_UID (p->i2) < maxuid)
+	{
+	  *pprev = p;
+	  pprev = &p->next_same_i1;
+	}
+    }
+  *pprev = NULL;
+  if (first == NULL)
+    htab_clear_slot (delay_htab, slot);
+  else
+    *pslot = first;
+  return 1;
+}
+
+/* Discard all delay pairs which involve an insn with an UID higher
+   than MAX_UID.  */
+void
+discard_delay_pairs_above (int max_uid)
+{
+  htab_traverse (delay_htab, htab_i1_traverse, &max_uid);
+  htab_traverse (delay_htab_i2, htab_i2_traverse, &max_uid);
+}
+
+/* Returns a hash value for X (which really is a delay_pair), based on
+   hashing just I1.  */
+static hashval_t
+delay_hash_i1 (const void *x)
+{
+  return htab_hash_pointer (((const struct delay_pair *) x)->i1);
+}
+
+/* Returns a hash value for X (which really is a delay_pair), based on
+   hashing just I2.  */
+static hashval_t
+delay_hash_i2 (const void *x)
+{
+  return htab_hash_pointer (((const struct delay_pair *) x)->i2);
+}
+
+/* Return nonzero if I1 of pair X is the same as that of pair Y.  */
+static int
+delay_i1_eq (const void *x, const void *y)
+{
+  return ((const struct delay_pair *) x)->i1 == y;
+}
+
+/* Return nonzero if I2 of pair X is the same as that of pair Y.  */
+static int
+delay_i2_eq (const void *x, const void *y)
+{
+  return ((const struct delay_pair *) x)->i2 == y;
+}
+
+/* This function can be called by a port just before it starts the final
+   scheduling pass.  It records the fact that an instruction with delay
+   slots has been split into two insns, I1 and I2.  The first one will be
+   scheduled normally and initiates the operation.  The second one is a
+   shadow which must follow a specific number of cycles after I1; its only
+   purpose is to show the side effect that occurs at that cycle in the RTL.
+   If a JUMP_INSN or a CALL_INSN has been split, I1 should be a normal INSN,
+   while I2 retains the original insn type.
+
+   There are two ways in which the number of cycles can be specified,
+   involving the CYCLES and STAGES arguments to this function.  If STAGES
+   is zero, we just use the value of CYCLES.  Otherwise, STAGES is a factor
+   which is multiplied by MODULO_II to give the number of cycles.  This is
+   only useful if the caller also calls set_modulo_params to enable modulo
+   scheduling.  */
+
+void
+record_delay_slot_pair (rtx i1, rtx i2, int cycles, int stages)
+{
+  struct delay_pair *p = XNEW (struct delay_pair);
+  struct delay_pair **slot;
+
+  p->i1 = i1;
+  p->i2 = i2;
+  p->cycles = cycles;
+  p->stages = stages;
+
+  if (!delay_htab)
+    {
+      delay_htab = htab_create (10, delay_hash_i1, delay_i1_eq, NULL);
+      delay_htab_i2 = htab_create (10, delay_hash_i2, delay_i2_eq, free);
+    }
+  slot = ((struct delay_pair **)
+	  htab_find_slot_with_hash (delay_htab, i1, htab_hash_pointer (i1),
+				    INSERT));
+  p->next_same_i1 = *slot;
+  *slot = p;
+  slot = ((struct delay_pair **)
+	  htab_find_slot_with_hash (delay_htab_i2, i2, htab_hash_pointer (i2),
+				    INSERT));
+  *slot = p;
+}
+
+/* Examine the delay pair hashtable to see if INSN is a shadow for another,
+   and return the other insn if so.  Return NULL otherwise.  */
+rtx
+real_insn_for_shadow (rtx insn)
+{
+  struct delay_pair *pair;
+
+  if (delay_htab == NULL)
+    return NULL_RTX;
+
+  pair
+    = (struct delay_pair *)htab_find_with_hash (delay_htab_i2, insn,
+						htab_hash_pointer (insn));
+  if (!pair || pair->stages > 0)
+    return NULL_RTX;
+  return pair->i1;
+}
+
+/* For a pair P of insns, return the fixed distance in cycles from the first
+   insn after which the second must be scheduled.  */
+static int
+pair_delay (struct delay_pair *p)
+{
+  if (p->stages == 0)
+    return p->cycles;
+  else
+    return p->stages * modulo_ii;
+}
+
+/* Given an insn INSN, add a dependence on its delayed shadow if it
+   has one.  Also try to find situations where shadows depend on each other
+   and add dependencies to the real insns to limit the amount of backtracking
+   needed.  */
+void
+add_delay_dependencies (rtx insn)
+{
+  struct delay_pair *pair;
+  sd_iterator_def sd_it;
+  dep_t dep;
+
+  if (!delay_htab)
+    return;
+
+  pair
+    = (struct delay_pair *)htab_find_with_hash (delay_htab_i2, insn,
+						htab_hash_pointer (insn));
+  if (!pair)
+    return;
+  add_dependence (insn, pair->i1, REG_DEP_ANTI);
+  if (pair->stages)
+    return;
 
+  FOR_EACH_DEP (pair->i2, SD_LIST_BACK, sd_it, dep)
+    {
+      rtx pro = DEP_PRO (dep);
+      struct delay_pair *other_pair
+	= (struct delay_pair *)htab_find_with_hash (delay_htab_i2, pro,
+						    htab_hash_pointer (pro));
+      if (!other_pair || other_pair->stages)
+	continue;
+      if (pair_delay (other_pair) >= pair_delay (pair))
+	{
+	  if (sched_verbose >= 4)
+	    {
+	      fprintf (sched_dump, ";;\tadding dependence %d <- %d\n",
+		       INSN_UID (other_pair->i1),
+		       INSN_UID (pair->i1));
+	      fprintf (sched_dump, ";;\tpair1 %d <- %d, cost %d\n",
+		       INSN_UID (pair->i1),
+		       INSN_UID (pair->i2),
+		       pair_delay (pair));
+	      fprintf (sched_dump, ";;\tpair2 %d <- %d, cost %d\n",
+		       INSN_UID (other_pair->i1),
+		       INSN_UID (other_pair->i2),
+		       pair_delay (other_pair));
+	    }
+	  add_dependence (pair->i1, other_pair->i1, REG_DEP_ANTI);
+	}
+    }
+}
+
 /* Forward declarations.  */
 
 static int priority (rtx);
 static int rank_for_schedule (const void *, const void *);
 static void swap_sort (rtx *, int);
-static void queue_insn (rtx, int);
+static void queue_insn (rtx, int, const char *);
 static int schedule_insn (rtx);
 static void adjust_priority (rtx);
 static void advance_one_cycle (void);
@@ -535,6 +840,7 @@ static void change_queue_index (rtx, int);
 
 static void extend_h_i_d (void);
 static void init_h_i_d (rtx);
+static int haifa_speculate_insn (rtx, ds_t, rtx *);
 static void generate_recovery_code (rtx);
 static void process_insn_forw_deps_be_in_spec (rtx, rtx, ds_t);
 static void begin_speculative_block (rtx);
@@ -542,7 +848,7 @@ static void add_to_speculative_block (rtx);
 static void init_before_recovery (basic_block *);
 static void create_check_block_twin (rtx, bool);
 static void fix_recovery_deps (basic_block);
-static void haifa_change_pattern (rtx, rtx);
+static bool haifa_change_pattern (rtx, rtx);
 static void dump_new_block_header (int, basic_block, rtx, rtx);
 static void restore_bb_notes (basic_block);
 static void fix_jump_move (rtx);
@@ -773,13 +1079,192 @@ print_curr_reg_pressure (void)
     }
   fprintf (sched_dump, "\n");
 }
+
+/* Determine if INSN has a condition that is clobbered if a register
+   in SET_REGS is modified.  */
+static bool
+cond_clobbered_p (rtx insn, HARD_REG_SET set_regs)
+{
+  rtx pat = PATTERN (insn);
+  gcc_assert (GET_CODE (pat) == COND_EXEC);
+  if (TEST_HARD_REG_BIT (set_regs, REGNO (XEXP (COND_EXEC_TEST (pat), 0))))
+    {
+      sd_iterator_def sd_it;
+      dep_t dep;
+      haifa_change_pattern (insn, ORIG_PAT (insn));
+      FOR_EACH_DEP (insn, SD_LIST_BACK, sd_it, dep)
+	DEP_STATUS (dep) &= ~DEP_CANCELLED;
+      TODO_SPEC (insn) = HARD_DEP;
+      if (sched_verbose >= 2)
+	fprintf (sched_dump,
+		 ";;\t\tdequeue insn %s because of clobbered condition\n",
+		 (*current_sched_info->print_insn) (insn, 0));
+      return true;
+    }
+
+  return false;
+}
+
+/* Look at the remaining dependencies for insn NEXT, and compute and return
+   the TODO_SPEC value we should use for it.  This is called after one of
+   NEXT's dependencies has been resolved.  */
+
+static ds_t
+recompute_todo_spec (rtx next)
+{
+  ds_t new_ds;
+  sd_iterator_def sd_it;
+  dep_t dep, control_dep = NULL;
+  int n_spec = 0;
+  int n_control = 0;
+  bool first_p = true;
+
+  if (sd_lists_empty_p (next, SD_LIST_BACK))
+    /* NEXT has all its dependencies resolved.  */
+    return 0;
+
+  if (!sd_lists_empty_p (next, SD_LIST_HARD_BACK))
+    return HARD_DEP;
+
+  /* Now we've got NEXT with speculative deps only.
+     1. Look at the deps to see what we have to do.
+     2. Check if we can do 'todo'.  */
+  new_ds = 0;
+
+  FOR_EACH_DEP (next, SD_LIST_BACK, sd_it, dep)
+    {
+      ds_t ds = DEP_STATUS (dep) & SPECULATIVE;
+
+      if (DEBUG_INSN_P (DEP_PRO (dep)) && !DEBUG_INSN_P (next))
+	continue;
+
+      if (ds)
+	{
+	  n_spec++;
+	  if (first_p)
+	    {
+	      first_p = false;
+
+	      new_ds = ds;
+	    }
+	  else
+	    new_ds = ds_merge (new_ds, ds);
+	}
+      if (DEP_TYPE (dep) == REG_DEP_CONTROL)
+	{
+	  n_control++;
+	  control_dep = dep;
+	  DEP_STATUS (dep) &= ~DEP_CANCELLED;
+	}
+    }
+
+  if (n_control == 1 && n_spec == 0)
+    {
+      rtx pro, other, new_pat;
+      rtx cond = NULL_RTX;
+      bool success;
+      rtx prev = NULL_RTX;
+      int i;
+      unsigned regno;
+  
+      if ((current_sched_info->flags & DO_PREDICATION) == 0
+	  || (ORIG_PAT (next) != NULL_RTX
+	      && PREDICATED_PAT (next) == NULL_RTX))
+	return HARD_DEP;
+
+      pro = DEP_PRO (control_dep);
+      other = real_insn_for_shadow (pro);
+      if (other != NULL_RTX)
+	pro = other;
+
+      cond = sched_get_reverse_condition_uncached (pro);
+      regno = REGNO (XEXP (cond, 0));
+
+      /* Find the last scheduled insn that modifies the condition register.
+	 If we have a true dependency on it, it sets it to the correct value,
+	 otherwise it must be a later insn scheduled in-between that clobbers
+	 the condition.  */
+      FOR_EACH_VEC_ELT_REVERSE (rtx, scheduled_insns, i, prev)
+	{
+	  sd_iterator_def sd_it;
+	  dep_t dep;
+	  HARD_REG_SET t;
+	  bool found;
+
+	  find_all_hard_reg_sets (prev, &t);
+	  if (!TEST_HARD_REG_BIT (t, regno))
+	    continue;
+
+	  found = false;
+	  FOR_EACH_DEP (next, SD_LIST_RES_BACK, sd_it, dep)
+	    {
+	      if (DEP_PRO (dep) == prev && DEP_TYPE (dep) == REG_DEP_TRUE)
+		{
+		  found = true;
+		  break;
+		}
+	    }
+	  if (!found)
+	    return HARD_DEP;
+	  break;
+	}
+      if (ORIG_PAT (next) == NULL_RTX)
+	{
+	  ORIG_PAT (next) = PATTERN (next);
+
+	  new_pat = gen_rtx_COND_EXEC (VOIDmode, cond, PATTERN (next));
+	  success = haifa_change_pattern (next, new_pat);
+	  if (!success)
+	    return HARD_DEP;
+	  PREDICATED_PAT (next) = new_pat;
+	}
+      else if (PATTERN (next) != PREDICATED_PAT (next))
+	{
+	  bool success = haifa_change_pattern (next,
+					       PREDICATED_PAT (next));
+	  gcc_assert (success);
+	}
+      DEP_STATUS (control_dep) |= DEP_CANCELLED;
+      return DEP_CONTROL;
+    }
 
-/* Pointer to the last instruction scheduled.  Used by rank_for_schedule,
-   so that insns independent of the last scheduled insn will be preferred
-   over dependent instructions.  */
+  if (PREDICATED_PAT (next) != NULL_RTX)
+    {
+      int tick = INSN_TICK (next);
+      bool success = haifa_change_pattern (next,
+					   ORIG_PAT (next));
+      INSN_TICK (next) = tick;
+      gcc_assert (success);
+    }
 
+  /* We can't handle the case where there are both speculative and control
+     dependencies, so we return HARD_DEP in such a case.  Also fail if
+     we have speculative dependencies with not enough points, or more than
+     one control dependency.  */
+  if ((n_spec > 0 && n_control > 0)
+      || (n_spec > 0
+	  /* Too few points?  */
+	  && ds_weak (new_ds) < spec_info->data_weakness_cutoff)
+      || (n_control > 1))
+    return HARD_DEP;
+
+  return new_ds;
+}
+
+/* Pointer to the last instruction scheduled.  */
 static rtx last_scheduled_insn;
 
+/* Pointer to the last nondebug instruction scheduled within the
+   block, or the prev_head of the scheduling block.  Used by
+   rank_for_schedule, so that insns independent of the last scheduled
+   insn will be preferred over dependent instructions.  */
+static rtx last_nondebug_scheduled_insn;
+
+/* Pointer that iterates through the list of unscheduled insns if we
+   have a dbg_cnt enabled.  It always points at an insn prior to the
+   first unscheduled one.  */
+static rtx nonscheduled_insns_begin;
+
 /* Cached cost of the instruction.  Use below function to get cost of the
    insn.  -1 here means that the field is not initialized.  */
 #define INSN_COST(INSN)	(HID (INSN)->cost)
@@ -841,6 +1326,25 @@ dep_cost_1 (dep_t link, dw_t dw)
   rtx used = DEP_CON (link);
   int cost;
 
+  if (DEP_COST (link) != UNKNOWN_DEP_COST)
+    return DEP_COST (link);
+
+  if (delay_htab)
+    {
+      struct delay_pair *delay_entry;
+      delay_entry
+	= (struct delay_pair *)htab_find_with_hash (delay_htab_i2, used,
+						    htab_hash_pointer (used));
+      if (delay_entry)
+	{
+	  if (delay_entry->i1 == insn)
+	    {
+	      DEP_COST (link) = pair_delay (delay_entry);
+	      return DEP_COST (link);
+	    }
+	}
+    }
+
   /* A USE insn should never require the value used to be computed.
      This allows the computation of a function's result and parameter
      values to overlap the return and call.  We don't care about the
@@ -898,6 +1402,7 @@ dep_cost_1 (dep_t link, dw_t dw)
 	cost = 0;
     }
 
+  DEP_COST (link) = cost;
   return cost;
 }
 
@@ -1149,7 +1654,6 @@ rank_for_schedule (const void *x, const void *y)
 {
   rtx tmp = *(const rtx *) y;
   rtx tmp2 = *(const rtx *) x;
-  rtx last;
   int tmp_class, tmp2_class;
   int val, priority_val, info_val;
 
@@ -1196,6 +1700,17 @@ rank_for_schedule (const void *x, const void *y)
       else
 	return INSN_TICK (tmp) - INSN_TICK (tmp2);
     }
+
+  /* If we are doing backtracking in this schedule, prefer insns that
+     have forward dependencies with negative cost against an insn that
+     was already scheduled.  */
+  if (current_sched_info->flags & DO_BACKTRACKING)
+    {
+      priority_val = FEEDS_BACKTRACK_INSN (tmp2) - FEEDS_BACKTRACK_INSN (tmp);
+      if (priority_val)
+	return priority_val;
+    }
+
   /* Prefer insn with higher priority.  */
   priority_val = INSN_PRIORITY (tmp2) - INSN_PRIORITY (tmp);
 
@@ -1230,23 +1745,13 @@ rank_for_schedule (const void *x, const void *y)
   if(flag_sched_rank_heuristic && info_val)
     return info_val;
 
-  if (flag_sched_last_insn_heuristic)
-    {
-      last = last_scheduled_insn;
-
-      if (DEBUG_INSN_P (last) && last != current_sched_info->prev_head)
-	do
-	  last = PREV_INSN (last);
-	while (!NONDEBUG_INSN_P (last)
-	       && last != current_sched_info->prev_head);
-    }
-
   /* Compare insns based on their relation to the last scheduled
      non-debug insn.  */
-  if (flag_sched_last_insn_heuristic && NONDEBUG_INSN_P (last))
+  if (flag_sched_last_insn_heuristic && last_nondebug_scheduled_insn)
     {
       dep_t dep1;
       dep_t dep2;
+      rtx last = last_nondebug_scheduled_insn;
 
       /* Classify the instructions into three classes:
          1) Data dependent on last schedule insn.
@@ -1310,13 +1815,15 @@ swap_sort (rtx *a, int n)
 
 /* Add INSN to the insn queue so that it can be executed at least
    N_CYCLES after the currently executing insn.  Preserve insns
-   chain for debugging purposes.  */
+   chain for debugging purposes.  REASON will be printed in debugging
+   output.  */
 
 HAIFA_INLINE static void
-queue_insn (rtx insn, int n_cycles)
+queue_insn (rtx insn, int n_cycles, const char *reason)
 {
   int next_q = NEXT_Q_AFTER (q_ptr, n_cycles);
   rtx link = alloc_INSN_LIST (insn, insn_queue[next_q]);
+  int new_tick;
 
   gcc_assert (n_cycles <= max_insn_queue_index);
   gcc_assert (!DEBUG_INSN_P (insn));
@@ -1329,10 +1836,25 @@ queue_insn (rtx insn, int n_cycles)
       fprintf (sched_dump, ";;\t\tReady-->Q: insn %s: ",
 	       (*current_sched_info->print_insn) (insn, 0));
 
-      fprintf (sched_dump, "queued for %d cycles.\n", n_cycles);
+      fprintf (sched_dump, "queued for %d cycles (%s).\n", n_cycles, reason);
     }
 
   QUEUE_INDEX (insn) = next_q;
+
+  if (current_sched_info->flags & DO_BACKTRACKING)
+    {
+      new_tick = clock_var + n_cycles;
+      if (INSN_TICK (insn) == INVALID_TICK || INSN_TICK (insn) < new_tick)
+	INSN_TICK (insn) = new_tick;
+
+      if (INSN_EXACT_TICK (insn) != INVALID_TICK
+	  && INSN_EXACT_TICK (insn) < clock_var + n_cycles)
+	{
+	  must_backtrack = true;
+	  if (sched_verbose >= 2)
+	    fprintf (sched_dump, ";;\t\tcausing a backtrack.\n");
+	}
+    }
 }
 
 /* Remove INSN from queue.  */
@@ -1392,6 +1914,12 @@ ready_add (struct ready_list *ready, rtx insn, bool first_p)
 
   gcc_assert (QUEUE_INDEX (insn) != QUEUE_READY);
   QUEUE_INDEX (insn) = QUEUE_READY;
+
+  if (INSN_EXACT_TICK (insn) != INVALID_TICK
+      && INSN_EXACT_TICK (insn) < clock_var)
+    {
+      must_backtrack = true;
+    }
 }
 
 /* Remove the element with the highest priority from the ready list and
@@ -1538,9 +2066,6 @@ advance_one_cycle (void)
     fprintf (sched_dump, ";;\tAdvanced a state.\n");
 }
 
-/* Clock at which the previous instruction was issued.  */
-static int last_clock_var;
-
 /* Update register pressure after scheduling INSN.  */
 static void
 update_register_pressure (rtx insn)
@@ -1630,6 +2155,67 @@ sched_setup_bb_reg_pressure_info (basic_block bb, rtx after)
   initiate_bb_reg_pressure_info (bb);
   setup_insn_max_reg_pressure (after, false);
 }
+
+/* If doing predication while scheduling, verify whether INSN, which
+   has just been scheduled, clobbers the conditions of any
+   instructions that must be predicated in order to break their
+   dependencies.  If so, remove them from the queues so that they will
+   only be scheduled once their control dependency is resolved.  */
+
+static void
+check_clobbered_conditions (rtx insn)
+{
+  HARD_REG_SET t;
+  int i;
+
+  if ((current_sched_info->flags & DO_PREDICATION) == 0)
+    return;
+
+  find_all_hard_reg_sets (insn, &t);
+
+ restart:
+  for (i = 0; i < ready.n_ready; i++)
+    {
+      rtx x = ready_element (&ready, i);
+      if (TODO_SPEC (x) == DEP_CONTROL && cond_clobbered_p (x, t))
+	{
+	  ready_remove_insn (x);
+	  goto restart;
+	}
+    }
+  for (i = 0; i <= max_insn_queue_index; i++)
+    {
+      rtx link;
+      int q = NEXT_Q_AFTER (q_ptr, i);
+
+    restart_queue:
+      for (link = insn_queue[q]; link; link = XEXP (link, 1))
+	{
+	  rtx x = XEXP (link, 0);
+	  if (TODO_SPEC (x) == DEP_CONTROL && cond_clobbered_p (x, t))
+	    {
+	      queue_remove (x);
+	      goto restart_queue;
+	    }
+	}
+    }
+}
+
+/* A structure that holds local state for the loop in schedule_block.  */
+struct sched_block_state
+{
+  /* True if no real insns have been scheduled in the current cycle.  */
+  bool first_cycle_insn_p;
+  /* True if a shadow insn has been scheduled in the current cycle, which
+     means that no more normal insns can be issued.  */
+  bool shadows_only_p;
+  /* True if we're winding down a modulo schedule, which means that we only
+     issue insns with INSN_EXACT_TICK set.  */
+  bool modulo_epilogue;
+  /* Initialized with the machine's issue rate every cycle, and updated
+     by calls to the variable_issue hook.  */
+  int can_issue_more;
+};
 
 /* INSN is the "currently executing insn".  Launch each insn which was
    waiting on INSN.  READY is the ready list which contains the insns
@@ -1675,7 +2261,7 @@ schedule_insn (rtx insn)
 
   /* Scheduling instruction should have all its dependencies resolved and
      should have been removed from the ready list.  */
-  gcc_assert (sd_lists_empty_p (insn, SD_LIST_BACK));
+  gcc_assert (sd_lists_empty_p (insn, SD_LIST_HARD_BACK));
 
   /* Reset debug insns invalidated by moving this insn.  */
   if (MAY_HAVE_DEBUG_INSNS && !DEBUG_INSN_P (insn))
@@ -1685,6 +2271,12 @@ schedule_insn (rtx insn)
 	rtx dbg = DEP_PRO (dep);
 	struct reg_use_data *use, *next;
 
+	if (DEP_STATUS (dep) & DEP_CANCELLED)
+	  {
+	    sd_iterator_next (&sd_it);
+	    continue;
+	  }
+
 	gcc_assert (DEBUG_INSN_P (dbg));
 
 	if (sched_verbose >= 6)
@@ -1738,17 +2330,36 @@ schedule_insn (rtx insn)
      INSN_TICK untouched.  This is a machine-dependent issue, actually.  */
   INSN_TICK (insn) = clock_var;
 
+  check_clobbered_conditions (insn);
+
   /* Update dependent instructions.  */
   for (sd_it = sd_iterator_start (insn, SD_LIST_FORW);
        sd_iterator_cond (&sd_it, &dep);)
     {
       rtx next = DEP_CON (dep);
+      bool cancelled = (DEP_STATUS (dep) & DEP_CANCELLED) != 0;
 
       /* Resolve the dependence between INSN and NEXT.
 	 sd_resolve_dep () moves current dep to another list thus
 	 advancing the iterator.  */
       sd_resolve_dep (sd_it);
 
+      if (cancelled)
+	{
+	  if (QUEUE_INDEX (next) != QUEUE_SCHEDULED)
+	    {
+	      int tick = INSN_TICK (next);
+	      gcc_assert (ORIG_PAT (next) != NULL_RTX);
+	      haifa_change_pattern (next, ORIG_PAT (next));
+	      INSN_TICK (next) = tick;
+	      if (sd_lists_empty_p (next, SD_LIST_BACK))
+		TODO_SPEC (next) = 0;
+	      else if (!sd_lists_empty_p (next, SD_LIST_HARD_BACK))
+		TODO_SPEC (next) = HARD_DEP;
+	    }
+	  continue;
+	}
+
       /* Don't bother trying to mark next as ready if insn is a debug
 	 insn.  If insn is the last hard dependency, it will have
 	 already been discounted.  */
@@ -1775,18 +2386,6 @@ schedule_insn (rtx insn)
 	}
     }
 
-  /* This is the place where scheduler doesn't *basically* need backward and
-     forward dependencies for INSN anymore.  Nevertheless they are used in
-     heuristics in rank_for_schedule (), early_queue_to_ready () and in
-     some targets (e.g. rs6000).  Thus the earliest place where we *can*
-     remove dependencies is after targetm.sched.finish () call in
-     schedule_block ().  But, on the other side, the safest place to remove
-     dependencies is when we are finishing scheduling entire region.  As we
-     don't generate [many] dependencies during scheduling itself, we won't
-     need memory until beginning of next region.
-     Bottom line: Dependencies are removed for all insns in the end of
-     scheduling the region.  */
-
   /* Annotate the instruction with issue information -- TImode
      indicates that the instruction is expected not to be able
      to issue on the same cycle as the previous insn.  A machine
@@ -1882,22 +2481,511 @@ remove_notes (rtx head, rtx tail)
     }
 }
 
-
-/* Return the head and tail pointers of ebb starting at BEG and ending
-   at END.  */
-void
-get_ebb_head_tail (basic_block beg, basic_block end, rtx *headp, rtx *tailp)
+/* A structure to record enough data to allow us to backtrack the scheduler to
+   a previous state.  */
+struct haifa_saved_data
 {
-  rtx beg_head = BB_HEAD (beg);
-  rtx beg_tail = BB_END (beg);
-  rtx end_head = BB_HEAD (end);
-  rtx end_tail = BB_END (end);
-
-  /* Don't include any notes or labels at the beginning of the BEG
-     basic block, or notes at the end of the END basic blocks.  */
+  /* Next entry on the list.  */
+  struct haifa_saved_data *next;
+
+  /* Backtracking is associated with scheduling insns that have delay slots.
+     DELAY_PAIR points to the structure that contains the insns involved, and
+     the number of cycles between them.  */
+  struct delay_pair *delay_pair;
+
+  /* Data used by the frontend (e.g. sched-ebb or sched-rgn).  */
+  void *fe_saved_data;
+  /* Data used by the backend.  */
+  void *be_saved_data;
+
+  /* Copies of global state.  */
+  int clock_var, last_clock_var;
+  struct ready_list ready;
+  state_t curr_state;
+
+  rtx last_scheduled_insn;
+  rtx last_nondebug_scheduled_insn;
+  int cycle_issued_insns;
+
+  /* Copies of state used in the inner loop of schedule_block.  */
+  struct sched_block_state sched_block;
+
+  /* We don't need to save q_ptr, as its value is arbitrary and we can set it
+     to 0 when restoring.  */
+  int q_size;
+  rtx *insn_queue;
+};
 
-  if (LABEL_P (beg_head))
-    beg_head = NEXT_INSN (beg_head);
+/* A record, in reverse order, of all scheduled insns which have delay slots
+   and may require backtracking.  */
+static struct haifa_saved_data *backtrack_queue;
+
+/* For every dependency of INSN, set the FEEDS_BACKTRACK_INSN bit according
+   to SET_P.  */
+static void
+mark_backtrack_feeds (rtx insn, int set_p)
+{
+  sd_iterator_def sd_it;
+  dep_t dep;
+  FOR_EACH_DEP (insn, SD_LIST_HARD_BACK, sd_it, dep)
+    {
+      FEEDS_BACKTRACK_INSN (DEP_PRO (dep)) = set_p;
+    }
+}
+
+/* Save the current scheduler state so that we can backtrack to it
+   later if necessary.  PAIR gives the insns that make it necessary to
+   save this point.  SCHED_BLOCK is the local state of schedule_block
+   that need to be saved.  */
+static void
+save_backtrack_point (struct delay_pair *pair,
+		      struct sched_block_state sched_block)
+{
+  int i;
+  struct haifa_saved_data *save = XNEW (struct haifa_saved_data);
+
+  save->curr_state = xmalloc (dfa_state_size);
+  memcpy (save->curr_state, curr_state, dfa_state_size);
+
+  save->ready.first = ready.first;
+  save->ready.n_ready = ready.n_ready;
+  save->ready.n_debug = ready.n_debug;
+  save->ready.veclen = ready.veclen;
+  save->ready.vec = XNEWVEC (rtx, ready.veclen);
+  memcpy (save->ready.vec, ready.vec, ready.veclen * sizeof (rtx));
+
+  save->insn_queue = XNEWVEC (rtx, max_insn_queue_index + 1);
+  save->q_size = q_size;
+  for (i = 0; i <= max_insn_queue_index; i++)
+    {
+      int q = NEXT_Q_AFTER (q_ptr, i);
+      save->insn_queue[i] = copy_INSN_LIST (insn_queue[q]);
+    }
+
+  save->clock_var = clock_var;
+  save->last_clock_var = last_clock_var;
+  save->cycle_issued_insns = cycle_issued_insns;
+  save->last_scheduled_insn = last_scheduled_insn;
+  save->last_nondebug_scheduled_insn = last_nondebug_scheduled_insn;
+
+  save->sched_block = sched_block;
+
+  if (current_sched_info->save_state)
+    save->fe_saved_data = (*current_sched_info->save_state) ();
+
+  if (targetm.sched.alloc_sched_context)
+    {
+      save->be_saved_data = targetm.sched.alloc_sched_context ();
+      targetm.sched.init_sched_context (save->be_saved_data, false);
+    }
+  else
+    save->be_saved_data = NULL;
+
+  save->delay_pair = pair;
+
+  save->next = backtrack_queue;
+  backtrack_queue = save;
+
+  while (pair)
+    {
+      mark_backtrack_feeds (pair->i2, 1);
+      INSN_TICK (pair->i2) = INVALID_TICK;
+      INSN_EXACT_TICK (pair->i2) = clock_var + pair_delay (pair);
+      SHADOW_P (pair->i2) = pair->stages == 0;
+      pair = pair->next_same_i1;
+    }
+}
+
+/* Walk the ready list and all queues. If any insns have unresolved backwards
+   dependencies, these must be cancelled deps, broken by predication.  Set or
+   clear (depending on SET) the DEP_CANCELLED bit in DEP_STATUS.  */
+
+static void
+toggle_cancelled_flags (bool set)
+{
+  int i;
+  sd_iterator_def sd_it;
+  dep_t dep;
+
+  if (ready.n_ready > 0)
+    {
+      rtx *first = ready_lastpos (&ready);
+      for (i = 0; i < ready.n_ready; i++)
+	FOR_EACH_DEP (first[i], SD_LIST_BACK, sd_it, dep)
+	  if (!DEBUG_INSN_P (DEP_PRO (dep)))
+	    {
+	      if (set)
+		DEP_STATUS (dep) |= DEP_CANCELLED;
+	      else
+		DEP_STATUS (dep) &= ~DEP_CANCELLED;
+	    }
+    }
+  for (i = 0; i <= max_insn_queue_index; i++)
+    {
+      int q = NEXT_Q_AFTER (q_ptr, i);
+      rtx link;
+      for (link = insn_queue[q]; link; link = XEXP (link, 1))
+	{
+	  rtx insn = XEXP (link, 0);
+	  FOR_EACH_DEP (insn, SD_LIST_BACK, sd_it, dep)
+	    if (!DEBUG_INSN_P (DEP_PRO (dep)))
+	      {
+		if (set)
+		  DEP_STATUS (dep) |= DEP_CANCELLED;
+		else
+		  DEP_STATUS (dep) &= ~DEP_CANCELLED;
+	      }
+	}
+    }
+}
+
+/* Pop entries from the SCHEDULED_INSNS vector up to and including INSN.
+   Restore their dependencies to an unresolved state, and mark them as
+   queued nowhere.  */
+
+static void
+unschedule_insns_until (rtx insn)
+{
+  VEC (rtx, heap) *recompute_vec;
+
+  recompute_vec = VEC_alloc (rtx, heap, 0);
+
+  /* Make two passes over the insns to be unscheduled.  First, we clear out
+     dependencies and other trivial bookkeeping.  */
+  for (;;)
+    {
+      rtx last;
+      sd_iterator_def sd_it;
+      dep_t dep;
+
+      last = VEC_pop (rtx, scheduled_insns);
+
+      /* This will be changed by restore_backtrack_point if the insn is in
+	 any queue.  */
+      QUEUE_INDEX (last) = QUEUE_NOWHERE;
+      if (last != insn)
+	INSN_TICK (last) = INVALID_TICK;
+
+      if (modulo_ii > 0 && INSN_UID (last) < modulo_iter0_max_uid)
+	modulo_insns_scheduled--;
+
+      for (sd_it = sd_iterator_start (last, SD_LIST_RES_FORW);
+	   sd_iterator_cond (&sd_it, &dep);)
+	{
+	  rtx con = DEP_CON (dep);
+	  sd_unresolve_dep (sd_it);
+	  if (!MUST_RECOMPUTE_SPEC_P (con))
+	    {
+	      MUST_RECOMPUTE_SPEC_P (con) = 1;
+	      VEC_safe_push (rtx, heap, recompute_vec, con);
+	    }
+	}
+
+      if (last == insn)
+	break;
+    }
+
+  /* A second pass, to update ready and speculation status for insns
+     depending on the unscheduled ones.  The first pass must have
+     popped the scheduled_insns vector up to the point where we
+     restart scheduling, as recompute_todo_spec requires it to be
+     up-to-date.  */
+  while (!VEC_empty (rtx, recompute_vec))
+    {
+      rtx con;
+
+      con = VEC_pop (rtx, recompute_vec);
+      MUST_RECOMPUTE_SPEC_P (con) = 0;
+      if (!sd_lists_empty_p (con, SD_LIST_HARD_BACK))
+	{
+	  TODO_SPEC (con) = HARD_DEP;
+	  INSN_TICK (con) = INVALID_TICK;
+	  if (PREDICATED_PAT (con) != NULL_RTX)
+	    haifa_change_pattern (con, ORIG_PAT (con));
+	}
+      else if (QUEUE_INDEX (con) != QUEUE_SCHEDULED)
+	TODO_SPEC (con) = recompute_todo_spec (con);
+    }
+  VEC_free (rtx, heap, recompute_vec);
+}
+
+/* Restore scheduler state from the topmost entry on the backtracking queue.
+   PSCHED_BLOCK_P points to the local data of schedule_block that we must
+   overwrite with the saved data.
+   The caller must already have called unschedule_insns_until.  */
+
+static void
+restore_last_backtrack_point (struct sched_block_state *psched_block)
+{
+  rtx link;
+  int i;
+  struct haifa_saved_data *save = backtrack_queue;
+
+  backtrack_queue = save->next;
+
+  if (current_sched_info->restore_state)
+    (*current_sched_info->restore_state) (save->fe_saved_data);
+
+  if (targetm.sched.alloc_sched_context)
+    {
+      targetm.sched.set_sched_context (save->be_saved_data);
+      targetm.sched.free_sched_context (save->be_saved_data);
+    }
+
+  /* Clear the QUEUE_INDEX of everything in the ready list or one
+     of the queues.  */
+  if (ready.n_ready > 0)
+    {
+      rtx *first = ready_lastpos (&ready);
+      for (i = 0; i < ready.n_ready; i++)
+	{
+	  rtx insn = first[i];
+	  QUEUE_INDEX (insn) = QUEUE_NOWHERE;
+	  INSN_TICK (insn) = INVALID_TICK;
+	}
+    }
+  for (i = 0; i <= max_insn_queue_index; i++)
+    {
+      int q = NEXT_Q_AFTER (q_ptr, i);
+
+      for (link = insn_queue[q]; link; link = XEXP (link, 1))
+	{
+	  rtx x = XEXP (link, 0);
+	  QUEUE_INDEX (x) = QUEUE_NOWHERE;
+	  INSN_TICK (x) = INVALID_TICK;
+	}
+      free_INSN_LIST_list (&insn_queue[q]);
+    }
+
+  free (ready.vec);
+  ready = save->ready;
+
+  if (ready.n_ready > 0)
+    {
+      rtx *first = ready_lastpos (&ready);
+      for (i = 0; i < ready.n_ready; i++)
+	{
+	  rtx insn = first[i];
+	  QUEUE_INDEX (insn) = QUEUE_READY;
+	  TODO_SPEC (insn) = recompute_todo_spec (insn);
+	  INSN_TICK (insn) = save->clock_var;
+	}
+    }
+
+  q_ptr = 0;
+  q_size = save->q_size;
+  for (i = 0; i <= max_insn_queue_index; i++)
+    {
+      int q = NEXT_Q_AFTER (q_ptr, i);
+
+      insn_queue[q] = save->insn_queue[q];
+
+      for (link = insn_queue[q]; link; link = XEXP (link, 1))
+	{
+	  rtx x = XEXP (link, 0);
+	  QUEUE_INDEX (x) = i;
+	  TODO_SPEC (x) = recompute_todo_spec (x);
+	  INSN_TICK (x) = save->clock_var + i;
+	}
+    }
+  free (save->insn_queue);
+
+  toggle_cancelled_flags (true);
+
+  clock_var = save->clock_var;
+  last_clock_var = save->last_clock_var;
+  cycle_issued_insns = save->cycle_issued_insns;
+  last_scheduled_insn = save->last_scheduled_insn;
+  last_nondebug_scheduled_insn = save->last_nondebug_scheduled_insn;
+
+  *psched_block = save->sched_block;
+
+  memcpy (curr_state, save->curr_state, dfa_state_size);
+  free (save->curr_state);
+
+  mark_backtrack_feeds (save->delay_pair->i2, 0);
+
+  free (save);
+
+  for (save = backtrack_queue; save; save = save->next)
+    {
+      mark_backtrack_feeds (save->delay_pair->i2, 1);
+    }
+}
+
+/* Discard all data associated with the topmost entry in the backtrack
+   queue.  If RESET_TICK is false, we just want to free the data.  If true,
+   we are doing this because we discovered a reason to backtrack.  In the
+   latter case, also reset the INSN_TICK for the shadow insn.  */
+static void
+free_topmost_backtrack_point (bool reset_tick)
+{
+  struct haifa_saved_data *save = backtrack_queue;
+  int i;
+
+  backtrack_queue = save->next;
+
+  if (reset_tick)
+    {
+      struct delay_pair *pair = save->delay_pair;
+      while (pair)
+	{
+	  INSN_TICK (pair->i2) = INVALID_TICK;
+	  INSN_EXACT_TICK (pair->i2) = INVALID_TICK;
+	  pair = pair->next_same_i1;
+	}
+    }
+  if (targetm.sched.free_sched_context)
+    targetm.sched.free_sched_context (save->be_saved_data);
+  if (current_sched_info->restore_state)
+    free (save->fe_saved_data);
+  for (i = 0; i <= max_insn_queue_index; i++)
+    free_INSN_LIST_list (&save->insn_queue[i]);
+  free (save->insn_queue);
+  free (save->curr_state);
+  free (save->ready.vec);
+  free (save);
+}
+
+/* Free the entire backtrack queue.  */
+static void
+free_backtrack_queue (void)
+{
+  while (backtrack_queue)
+    free_topmost_backtrack_point (false);
+}
+
+/* Compute INSN_TICK_ESTIMATE for INSN.  PROCESSED is a bitmap of
+   instructions we've previously encountered, a set bit prevents
+   recursion.  BUDGET is a limit on how far ahead we look, it is
+   reduced on recursive calls.  Return true if we produced a good
+   estimate, or false if we exceeded the budget.  */
+static bool
+estimate_insn_tick (bitmap processed, rtx insn, int budget)
+{
+  sd_iterator_def sd_it;
+  dep_t dep;
+  int earliest = INSN_TICK (insn);
+
+  FOR_EACH_DEP (insn, SD_LIST_BACK, sd_it, dep)
+    {
+      rtx pro = DEP_PRO (dep);
+      int t;
+
+      if (DEP_STATUS (dep) & DEP_CANCELLED)
+	continue;
+
+      if (QUEUE_INDEX (pro) == QUEUE_SCHEDULED)
+	gcc_assert (INSN_TICK (pro) + dep_cost (dep) <= INSN_TICK (insn));
+      else
+	{
+	  int cost = dep_cost (dep);
+	  if (cost >= budget)
+	    return false;
+	  if (!bitmap_bit_p (processed, INSN_LUID (pro)))
+	    {
+	      if (!estimate_insn_tick (processed, pro, budget - cost))
+		return false;
+	    }
+	  gcc_assert (INSN_TICK_ESTIMATE (pro) != INVALID_TICK);
+	  t = INSN_TICK_ESTIMATE (pro) + cost;
+	  if (earliest == INVALID_TICK || t > earliest)
+	    earliest = t;
+	}
+    }
+  bitmap_set_bit (processed, INSN_LUID (insn));
+  INSN_TICK_ESTIMATE (insn) = earliest;
+  return true;
+}
+
+/* Examine the pair of insns in P, and estimate (optimistically, assuming
+   infinite resources) the cycle in which the delayed shadow can be issued.
+   Return the number of cycles that must pass before the real insn can be
+   issued in order to meet this constraint.  */
+static int
+estimate_shadow_tick (struct delay_pair *p)
+{
+  bitmap_head processed;
+  int t;
+  bool cutoff;
+  bitmap_initialize (&processed, 0);
+
+  cutoff = !estimate_insn_tick (&processed, p->i2,
+				max_insn_queue_index + pair_delay (p));
+  bitmap_clear (&processed);
+  if (cutoff)
+    return max_insn_queue_index;
+  t = INSN_TICK_ESTIMATE (p->i2) - (clock_var + pair_delay (p) + 1);
+  if (t > 0)
+    return t;
+  return 0;
+}
+
+/* If INSN has no unresolved backwards dependencies, add it to the schedule and
+   recursively resolve all its forward dependencies.  */
+static void
+resolve_dependencies (rtx insn)
+{
+  sd_iterator_def sd_it;
+  dep_t dep;
+
+  /* Don't use sd_lists_empty_p; it ignores debug insns.  */
+  if (DEPS_LIST_FIRST (INSN_HARD_BACK_DEPS (insn)) != NULL
+      || DEPS_LIST_FIRST (INSN_SPEC_BACK_DEPS (insn)) != NULL)
+    return;
+
+  if (sched_verbose >= 4)
+    fprintf (sched_dump, ";;\tquickly resolving %d\n", INSN_UID (insn));
+
+  if (QUEUE_INDEX (insn) >= 0)
+    queue_remove (insn);
+
+  VEC_safe_push (rtx, heap, scheduled_insns, insn);
+
+  /* Update dependent instructions.  */
+  for (sd_it = sd_iterator_start (insn, SD_LIST_FORW);
+       sd_iterator_cond (&sd_it, &dep);)
+    {
+      rtx next = DEP_CON (dep);
+
+      if (sched_verbose >= 4)
+	fprintf (sched_dump, ";;\t\tdep %d against %d\n", INSN_UID (insn),
+		 INSN_UID (next));
+
+      /* Resolve the dependence between INSN and NEXT.
+	 sd_resolve_dep () moves current dep to another list thus
+	 advancing the iterator.  */
+      sd_resolve_dep (sd_it);
+
+      if (!IS_SPECULATION_BRANCHY_CHECK_P (insn))
+	{
+	  resolve_dependencies (next);
+	}
+      else
+	/* Check always has only one forward dependence (to the first insn in
+	   the recovery block), therefore, this will be executed only once.  */
+	{
+	  gcc_assert (sd_lists_empty_p (insn, SD_LIST_FORW));
+	}
+    }
+}
+
+
+/* Return the head and tail pointers of ebb starting at BEG and ending
+   at END.  */
+void
+get_ebb_head_tail (basic_block beg, basic_block end, rtx *headp, rtx *tailp)
+{
+  rtx beg_head = BB_HEAD (beg);
+  rtx beg_tail = BB_END (beg);
+  rtx end_head = BB_HEAD (end);
+  rtx end_tail = BB_END (end);
+
+  /* Don't include any notes or labels at the beginning of the BEG
+     basic block, or notes at the end of the END basic blocks.  */
+
+  if (LABEL_P (beg_head))
+    beg_head = NEXT_INSN (beg_head);
 
   while (beg_head != beg_tail)
     if (NOTE_P (beg_head))
@@ -2036,9 +3124,16 @@ queue_to_ready (struct ready_list *ready)
   q_ptr = NEXT_Q (q_ptr);
 
   if (dbg_cnt (sched_insn) == false)
-    /* If debug counter is activated do not requeue insn next after
-       last_scheduled_insn.  */
-    skip_insn = next_nonnote_nondebug_insn (last_scheduled_insn);
+    {
+      /* If debug counter is activated do not requeue the first
+	 nonscheduled insn.  */
+      skip_insn = nonscheduled_insns_begin;
+      do
+	{
+	  skip_insn = next_nonnote_nondebug_insn (skip_insn);
+	}
+      while (QUEUE_INDEX (skip_insn) == QUEUE_SCHEDULED);
+    }
   else
     skip_insn = NULL_RTX;
 
@@ -2059,11 +3154,7 @@ queue_to_ready (struct ready_list *ready)
 	  && ready->n_ready - ready->n_debug > MAX_SCHED_READY_INSNS
 	  && !SCHED_GROUP_P (insn)
 	  && insn != skip_insn)
-	{
-	  if (sched_verbose >= 2)
-	    fprintf (sched_dump, "requeued because ready full\n");
-	  queue_insn (insn, 1);
-	}
+	queue_insn (insn, 1, "ready full");
       else
 	{
 	  ready_add (ready, insn, false);
@@ -2125,22 +3216,18 @@ queue_to_ready (struct ready_list *ready)
 static bool
 ok_for_early_queue_removal (rtx insn)
 {
-  int n_cycles;
-  rtx prev_insn = last_scheduled_insn;
-
   if (targetm.sched.is_costly_dependence)
     {
+      rtx prev_insn;
+      int n_cycles;
+      int i = VEC_length (rtx, scheduled_insns);
       for (n_cycles = flag_sched_stalled_insns_dep; n_cycles; n_cycles--)
 	{
-	  for ( ; prev_insn; prev_insn = PREV_INSN (prev_insn))
+	  while (i-- > 0)
 	    {
 	      int cost;
 
-	      if (prev_insn == current_sched_info->prev_head)
-		{
-		  prev_insn = NULL;
-		  break;
-		}
+	      prev_insn = VEC_index (rtx, scheduled_insns, i);
 
 	      if (!NOTE_P (prev_insn))
 		{
@@ -2162,9 +3249,8 @@ ok_for_early_queue_removal (rtx insn)
 		break;
 	    }
 
-	  if (!prev_insn)
+	  if (i == 0)
 	    break;
-	  prev_insn = PREV_INSN (prev_insn);
 	}
     }
 
@@ -2451,11 +3537,6 @@ struct choice_entry
    function max_issue.  */
 static struct choice_entry *choice_stack;
 
-/* The following variable value is number of essential insns issued on
-   the current cycle.  An insn is essential one if it changes the
-   processors state.  */
-int cycle_issued_insns;
-
 /* This holds the value of the target dfa_lookahead hook.  */
 int dfa_lookahead;
 
@@ -2609,8 +3690,8 @@ max_issue (struct ready_list *ready, int privileged_n, state_t state,
 	    {
 	      if (state_dead_lock_p (state)
 		  || insn_finishes_cycle_p (insn))
- 		/* We won't issue any more instructions in the next
- 		   choice_state.  */
+		/* We won't issue any more instructions in the next
+		   choice_state.  */
 		top->rest = 0;
 	      else
 		top->rest--;
@@ -2669,13 +3750,17 @@ choose_ready (struct ready_list *ready, bool first_cycle_insn_p,
 
   if (dbg_cnt (sched_insn) == false)
     {
-      rtx insn;
-
-      insn = next_nonnote_insn (last_scheduled_insn);
+      rtx insn = nonscheduled_insns_begin;
+      do
+	{
+	  insn = next_nonnote_insn (insn);
+	}
+      while (QUEUE_INDEX (insn) == QUEUE_SCHEDULED);
 
       if (QUEUE_INDEX (insn) == QUEUE_READY)
 	/* INSN is in the ready_list.  */
 	{
+	  nonscheduled_insns_begin = insn;
 	  ready_remove_insn (insn);
 	  *insn_ptr = insn;
 	  return 0;
@@ -2783,46 +3868,263 @@ choose_ready (struct ready_list *ready, bool first_cycle_insn_p,
 	    gcc_checking_assert (INSN_CODE (insn) >= 0
 				 || recog_memoized (insn) < 0);
 
-	    ready_try [i]
-	      = (/* INSN_CODE check can be omitted here as it is also done later
-		    in max_issue ().  */
-		 INSN_CODE (insn) < 0
-		 || (targetm.sched.first_cycle_multipass_dfa_lookahead_guard
-		     && !targetm.sched.first_cycle_multipass_dfa_lookahead_guard
-		     (insn)));
-	  }
+	    ready_try [i]
+	      = (/* INSN_CODE check can be omitted here as it is also done later
+		    in max_issue ().  */
+		 INSN_CODE (insn) < 0
+		 || (targetm.sched.first_cycle_multipass_dfa_lookahead_guard
+		     && !targetm.sched.first_cycle_multipass_dfa_lookahead_guard
+		     (insn)));
+	  }
+
+      if (max_issue (ready, 1, curr_state, first_cycle_insn_p, &index) == 0)
+	{
+	  *insn_ptr = ready_remove_first (ready);
+	  if (sched_verbose >= 4)
+	    fprintf (sched_dump, ";;\t\tChosen insn (but can't issue) : %s \n",
+                     (*current_sched_info->print_insn) (*insn_ptr, 0));
+	  return 0;
+	}
+      else
+	{
+	  if (sched_verbose >= 4)
+	    fprintf (sched_dump, ";;\t\tChosen insn : %s\n",
+		     (*current_sched_info->print_insn)
+		     (ready_element (ready, index), 0));
+
+	  *insn_ptr = ready_remove (ready, index);
+	  return 0;
+	}
+    }
+}
+
+/* This function is called when we have successfully scheduled a
+   block.  It uses the schedule stored in the scheduled_insns vector
+   to rearrange the RTL.  PREV_HEAD is used as the anchor to which we
+   append the scheduled insns; TAIL is the insn after the scheduled
+   block.  TARGET_BB is the argument passed to schedule_block.  */
+
+static void
+commit_schedule (rtx prev_head, rtx tail, basic_block *target_bb)
+{
+  unsigned int i;
+  rtx insn;
+
+  last_scheduled_insn = prev_head;
+  for (i = 0;
+       VEC_iterate (rtx, scheduled_insns, i, insn);
+       i++)
+    {
+      if (control_flow_insn_p (last_scheduled_insn)
+	  || current_sched_info->advance_target_bb (*target_bb, insn))
+	{
+	  *target_bb = current_sched_info->advance_target_bb (*target_bb, 0);
+
+	  if (sched_verbose)
+	    {
+	      rtx x;
+
+	      x = next_real_insn (last_scheduled_insn);
+	      gcc_assert (x);
+	      dump_new_block_header (1, *target_bb, x, tail);
+	    }
+
+	  last_scheduled_insn = bb_note (*target_bb);
+	}
+
+      if (current_sched_info->begin_move_insn)
+	(*current_sched_info->begin_move_insn) (insn, last_scheduled_insn);
+      move_insn (insn, last_scheduled_insn,
+		 current_sched_info->next_tail);
+      if (!DEBUG_INSN_P (insn))
+	reemit_notes (insn);
+      last_scheduled_insn = insn;
+    }
+
+  VEC_truncate (rtx, scheduled_insns, 0);
+}
+
+/* Examine all insns on the ready list and queue those which can't be
+   issued in this cycle.  TEMP_STATE is temporary scheduler state we
+   can use as scratch space.  If FIRST_CYCLE_INSN_P is true, no insns
+   have been issued for the current cycle, which means it is valid to
+   issue an asm statement.
+
+   If SHADOWS_ONLY_P is true, we eliminate all real insns and only
+   leave those for which SHADOW_P is true.  If MODULO_EPILOGUE is true,
+   we only leave insns which have an INSN_EXACT_TICK.  */
+
+static void
+prune_ready_list (state_t temp_state, bool first_cycle_insn_p,
+		  bool shadows_only_p, bool modulo_epilogue_p)
+{
+  int i, pass;
+  bool sched_group_found = false;
+  int min_cost_group = 1;
+
+  for (i = 0; i < ready.n_ready; i++)
+    {
+      rtx insn = ready_element (&ready, i);
+      if (SCHED_GROUP_P (insn))
+	{
+	  sched_group_found = true;
+	  break;
+	}
+    }
+
+  /* Make two passes if there's a SCHED_GROUP_P insn; make sure to handle
+     such an insn first and note its cost, then schedule all other insns
+     for one cycle later.  */
+  for (pass = sched_group_found ? 0 : 1; pass < 2; )
+    {
+      int n = ready.n_ready;
+      for (i = 0; i < n; i++)
+	{
+	  rtx insn = ready_element (&ready, i);
+	  int cost = 0;
+	  const char *reason = "resource conflict";
+
+	  if (DEBUG_INSN_P (insn))
+	    continue;
+
+	  if (sched_group_found && !SCHED_GROUP_P (insn))
+	    {
+	      if (pass == 0)
+		continue;
+	      cost = min_cost_group;
+	      reason = "not in sched group";
+	    }
+	  else if (modulo_epilogue_p
+		   && INSN_EXACT_TICK (insn) == INVALID_TICK)
+	    {
+	      cost = max_insn_queue_index;
+	      reason = "not an epilogue insn";
+	    }
+	  else if (shadows_only_p && !SHADOW_P (insn))
+	    {
+	      cost = 1;
+	      reason = "not a shadow";
+	    }
+	  else if (recog_memoized (insn) < 0)
+	    {
+	      if (!first_cycle_insn_p
+		  && (GET_CODE (PATTERN (insn)) == ASM_INPUT
+		      || asm_noperands (PATTERN (insn)) >= 0))
+		cost = 1;
+	      reason = "asm";
+	    }
+	  else if (sched_pressure_p)
+	    cost = 0;
+	  else
+	    {
+	      int delay_cost = 0;
+
+	      if (delay_htab)
+		{
+		  struct delay_pair *delay_entry;
+		  delay_entry
+		    = (struct delay_pair *)htab_find_with_hash (delay_htab, insn,
+								htab_hash_pointer (insn));
+		  while (delay_entry && delay_cost == 0)
+		    {
+		      delay_cost = estimate_shadow_tick (delay_entry);
+		      if (delay_cost > max_insn_queue_index)
+			delay_cost = max_insn_queue_index;
+		      delay_entry = delay_entry->next_same_i1;
+		    }
+		}
+
+	      memcpy (temp_state, curr_state, dfa_state_size);
+	      cost = state_transition (temp_state, insn);
+	      if (cost < 0)
+		cost = 0;
+	      else if (cost == 0)
+		cost = 1;
+	      if (cost < delay_cost)
+		{
+		  cost = delay_cost;
+		  reason = "shadow tick";
+		}
+	    }
+	  if (cost >= 1)
+	    {
+	      if (SCHED_GROUP_P (insn) && cost > min_cost_group)
+		min_cost_group = cost;
+	      ready_remove (&ready, i);
+	      queue_insn (insn, cost, reason);
+	      if (i + 1 < n)
+		break;
+	    }
+	}
+      if (i == n)
+	pass++;
+    }
+}
+
+/* Called when we detect that the schedule is impossible.  We examine the
+   backtrack queue to find the earliest insn that caused this condition.  */
 
-      if (max_issue (ready, 1, curr_state, first_cycle_insn_p, &index) == 0)
-	{
-	  *insn_ptr = ready_remove_first (ready);
-	  if (sched_verbose >= 4)
-	    fprintf (sched_dump, ";;\t\tChosen insn (but can't issue) : %s \n",
-                     (*current_sched_info->print_insn) (*insn_ptr, 0));
-	  return 0;
-	}
-      else
+static struct haifa_saved_data *
+verify_shadows (void)
+{
+  struct haifa_saved_data *save, *earliest_fail = NULL;
+  for (save = backtrack_queue; save; save = save->next)
+    {
+      int t;
+      struct delay_pair *pair = save->delay_pair;
+      rtx i1 = pair->i1;
+
+      for (; pair; pair = pair->next_same_i1)
 	{
-	  if (sched_verbose >= 4)
-	    fprintf (sched_dump, ";;\t\tChosen insn : %s\n",
-		     (*current_sched_info->print_insn)
-		     (ready_element (ready, index), 0));
+	  rtx i2 = pair->i2;
 
-	  *insn_ptr = ready_remove (ready, index);
-	  return 0;
+	  if (QUEUE_INDEX (i2) == QUEUE_SCHEDULED)
+	    continue;
+
+	  t = INSN_TICK (i1) + pair_delay (pair);
+	  if (t < clock_var)
+	    {
+	      if (sched_verbose >= 2)
+		fprintf (sched_dump,
+			 ";;\t\tfailed delay requirements for %d/%d (%d->%d)"
+			 ", not ready\n",
+			 INSN_UID (pair->i1), INSN_UID (pair->i2),
+			 INSN_TICK (pair->i1), INSN_EXACT_TICK (pair->i2));
+	      earliest_fail = save;
+	      break;
+	    }
+	  if (QUEUE_INDEX (i2) >= 0)
+	    {
+	      int queued_for = INSN_TICK (i2);
+
+	      if (t < queued_for)
+		{
+		  if (sched_verbose >= 2)
+		    fprintf (sched_dump,
+			     ";;\t\tfailed delay requirements for %d/%d"
+			     " (%d->%d), queued too late\n",
+			     INSN_UID (pair->i1), INSN_UID (pair->i2),
+			     INSN_TICK (pair->i1), INSN_EXACT_TICK (pair->i2));
+		  earliest_fail = save;
+		  break;
+		}
+	    }
 	}
     }
+
+  return earliest_fail;
 }
 
 /* Use forward list scheduling to rearrange insns of block pointed to by
    TARGET_BB, possibly bringing insns from subsequent blocks in the same
    region.  */
 
-void
-schedule_block (basic_block *target_bb)
+bool
+schedule_block (basic_block *target_bb, state_t init_state)
 {
   int i;
-  bool first_cycle_insn_p;
-  int can_issue_more;
+  bool success = modulo_ii == 0;
+  struct sched_block_state ls;
   state_t temp_state = NULL;  /* It is used for multipass scheduling.  */
   int sort_p, advance, start_clock_var;
 
@@ -2843,11 +4145,16 @@ schedule_block (basic_block *target_bb)
 
   haifa_recovery_bb_recently_added_p = false;
 
+  backtrack_queue = NULL;
+
   /* Debug info.  */
   if (sched_verbose)
     dump_new_block_header (0, *target_bb, head, tail);
 
-  state_reset (curr_state);
+  if (init_state == NULL)
+    state_reset (curr_state);
+  else
+    memcpy (curr_state, init_state, dfa_state_size);
 
   /* Clear the ready list.  */
   ready.first = ready.veclen - 1;
@@ -2861,7 +4168,8 @@ schedule_block (basic_block *target_bb)
     targetm.sched.init (sched_dump, sched_verbose, ready.veclen);
 
   /* We start inserting insns after PREV_HEAD.  */
-  last_scheduled_insn = prev_head;
+  last_scheduled_insn = nonscheduled_insns_begin = prev_head;
+  last_nondebug_scheduled_insn = NULL_RTX;
 
   gcc_assert ((NOTE_P (last_scheduled_insn)
 	       || DEBUG_INSN_P (last_scheduled_insn))
@@ -2906,12 +4214,12 @@ schedule_block (basic_block *target_bb)
 
       /* Delay all insns past it for 1 cycle.  If debug counter is
 	 activated make an exception for the insn right after
-	 last_scheduled_insn.  */
+	 nonscheduled_insns_begin.  */
       {
 	rtx skip_insn;
 
 	if (dbg_cnt (sched_insn) == false)
-	  skip_insn = next_nonnote_insn (last_scheduled_insn);
+	  skip_insn = next_nonnote_insn (nonscheduled_insns_begin);
 	else
 	  skip_insn = NULL_RTX;
 
@@ -2922,8 +4230,10 @@ schedule_block (basic_block *target_bb)
 	    insn = ready_remove (&ready, i);
 
 	    if (insn != skip_insn)
-	      queue_insn (insn, 1);
+	      queue_insn (insn, 1, "list truncated");
 	  }
+	if (skip_insn)
+	  ready_add (&ready, skip_insn, true);
       }
     }
 
@@ -2934,7 +4244,13 @@ schedule_block (basic_block *target_bb)
 
   advance = 0;
 
+  gcc_assert (VEC_length (rtx, scheduled_insns) == 0);
   sort_p = TRUE;
+  must_backtrack = false;
+  modulo_insns_scheduled = 0;
+
+  ls.modulo_epilogue = false;
+
   /* Loop until all the insns in BB are scheduled.  */
   while ((*current_sched_info->schedule_more_p) ())
     {
@@ -2963,78 +4279,114 @@ schedule_block (basic_block *target_bb)
 	}
       while (advance > 0);
 
-      if (sort_p)
+      if (ls.modulo_epilogue)
 	{
-	  /* Sort the ready list based on priority.  */
-	  ready_sort (&ready);
-
-	  if (sched_verbose >= 2)
+	  int stage = clock_var / modulo_ii;
+	  if (stage > modulo_last_stage * 2 + 2)
 	    {
-	      fprintf (sched_dump, ";;\t\tReady list after ready_sort:  ");
-	      debug_ready_list (&ready);
+	      if (sched_verbose >= 2)
+		fprintf (sched_dump,
+			 ";;\t\tmodulo scheduled succeeded at II %d\n",
+			 modulo_ii);
+	      success = true;
+	      goto end_schedule;
 	    }
 	}
-
-      /* We don't want md sched reorder to even see debug isns, so put
-	 them out right away.  */
-      if (ready.n_ready && DEBUG_INSN_P (ready_element (&ready, 0)))
+      else if (modulo_ii > 0)
 	{
-	  if (control_flow_insn_p (last_scheduled_insn))
+	  int stage = clock_var / modulo_ii;
+	  if (stage > modulo_max_stages)
 	    {
-	      *target_bb = current_sched_info->advance_target_bb
-		(*target_bb, 0);
-
-	      if (sched_verbose)
-		{
-		  rtx x;
-
-		  x = next_real_insn (last_scheduled_insn);
-		  gcc_assert (x);
-		  dump_new_block_header (1, *target_bb, x, tail);
-		}
-
-	      last_scheduled_insn = bb_note (*target_bb);
+	      if (sched_verbose >= 2)
+		fprintf (sched_dump,
+			 ";;\t\tfailing schedule due to excessive stages\n");
+	      goto end_schedule;
 	    }
-
-	  while (ready.n_ready && DEBUG_INSN_P (ready_element (&ready, 0)))
+	  if (modulo_n_insns == modulo_insns_scheduled
+	      && stage > modulo_last_stage)
 	    {
-	      rtx insn = ready_remove_first (&ready);
-	      gcc_assert (DEBUG_INSN_P (insn));
-	      (*current_sched_info->begin_schedule_ready) (insn,
-							   last_scheduled_insn);
-	      move_insn (insn, last_scheduled_insn,
-			 current_sched_info->next_tail);
-	      last_scheduled_insn = insn;
-	      advance = schedule_insn (insn);
-	      gcc_assert (advance == 0);
-	      if (ready.n_ready > 0)
-		ready_sort (&ready);
+	      if (sched_verbose >= 2)
+		fprintf (sched_dump,
+			 ";;\t\tfound kernel after %d stages, II %d\n",
+			 stage, modulo_ii);
+	      ls.modulo_epilogue = true;
 	    }
-
-	  if (!ready.n_ready)
-	    continue;
 	}
 
-      /* Allow the target to reorder the list, typically for
-	 better instruction bundling.  */
-      if (sort_p && targetm.sched.reorder
-	  && (ready.n_ready == 0
-	      || !SCHED_GROUP_P (ready_element (&ready, 0))))
-	can_issue_more =
-	  targetm.sched.reorder (sched_dump, sched_verbose,
-				 ready_lastpos (&ready),
-				 &ready.n_ready, clock_var);
-      else
-	can_issue_more = issue_rate;
+      prune_ready_list (temp_state, true, false, ls.modulo_epilogue);
+      if (ready.n_ready == 0)
+	continue;
+      if (must_backtrack)
+	goto do_backtrack;
 
-      first_cycle_insn_p = true;
+      ls.first_cycle_insn_p = true;
+      ls.shadows_only_p = false;
       cycle_issued_insns = 0;
+      ls.can_issue_more = issue_rate;
       for (;;)
 	{
 	  rtx insn;
 	  int cost;
-	  bool asm_p = false;
+	  bool asm_p;
+
+	  if (sort_p && ready.n_ready > 0)
+	    {
+	      /* Sort the ready list based on priority.  This must be
+		 done every iteration through the loop, as schedule_insn
+		 may have readied additional insns that will not be
+		 sorted correctly.  */
+	      ready_sort (&ready);
+
+	      if (sched_verbose >= 2)
+		{
+		  fprintf (sched_dump, ";;\t\tReady list after ready_sort:  ");
+		  debug_ready_list (&ready);
+		}
+	    }
+
+	  /* We don't want md sched reorder to even see debug isns, so put
+	     them out right away.  */
+	  if (ready.n_ready && DEBUG_INSN_P (ready_element (&ready, 0))
+	      && (*current_sched_info->schedule_more_p) ())
+	    {
+	      while (ready.n_ready && DEBUG_INSN_P (ready_element (&ready, 0)))
+		{
+		  rtx insn = ready_remove_first (&ready);
+		  gcc_assert (DEBUG_INSN_P (insn));
+		  (*current_sched_info->begin_schedule_ready) (insn);
+		  VEC_safe_push (rtx, heap, scheduled_insns, insn);
+		  last_scheduled_insn = insn;
+		  advance = schedule_insn (insn);
+		  gcc_assert (advance == 0);
+		  if (ready.n_ready > 0)
+		    ready_sort (&ready);
+		}
+	    }
+
+	  if (ls.first_cycle_insn_p && !ready.n_ready)
+	    break;
+
+	resume_after_backtrack:
+	  /* Allow the target to reorder the list, typically for
+	     better instruction bundling.  */
+	  if (sort_p
+	      && (ready.n_ready == 0
+		  || !SCHED_GROUP_P (ready_element (&ready, 0))))
+	    {
+	      if (ls.first_cycle_insn_p && targetm.sched.reorder)
+		ls.can_issue_more
+		  = targetm.sched.reorder (sched_dump, sched_verbose,
+					   ready_lastpos (&ready),
+					   &ready.n_ready, clock_var);
+	      else if (!ls.first_cycle_insn_p && targetm.sched.reorder2)
+		ls.can_issue_more
+		  = targetm.sched.reorder2 (sched_dump, sched_verbose,
+					    ready.n_ready
+					    ? ready_lastpos (&ready) : NULL,
+					    &ready.n_ready, clock_var);
+	    }
 
+	restart_choose_ready:
 	  if (sched_verbose >= 2)
 	    {
 	      fprintf (sched_dump, ";;\tReady list (t = %3d):  ",
@@ -3045,7 +4397,7 @@ schedule_block (basic_block *target_bb)
 	    }
 
 	  if (ready.n_ready == 0
-	      && can_issue_more
+	      && ls.can_issue_more
 	      && reload_completed)
 	    {
 	      /* Allow scheduling insns directly from the queue in case
@@ -3059,7 +4411,7 @@ schedule_block (basic_block *target_bb)
 	    }
 
 	  if (ready.n_ready == 0
-	      || !can_issue_more
+	      || !ls.can_issue_more
 	      || state_dead_lock_p (curr_state)
 	      || !(*current_sched_info->schedule_more_p) ())
 	    break;
@@ -3070,14 +4422,13 @@ schedule_block (basic_block *target_bb)
 	      int res;
 
 	      insn = NULL_RTX;
-	      res = choose_ready (&ready, first_cycle_insn_p, &insn);
+	      res = choose_ready (&ready, ls.first_cycle_insn_p, &insn);
 
 	      if (res < 0)
 		/* Finish cycle.  */
 		break;
 	      if (res > 0)
-		/* Restart choose_ready ().  */
-		continue;
+		goto restart_choose_ready;
 
 	      gcc_assert (insn != NULL_RTX);
 	    }
@@ -3112,172 +4463,187 @@ schedule_block (basic_block *target_bb)
 	    }
 
 	  sort_p = TRUE;
-	  memcpy (temp_state, curr_state, dfa_state_size);
-	  if (recog_memoized (insn) < 0)
-	    {
-	      asm_p = (GET_CODE (PATTERN (insn)) == ASM_INPUT
-		       || asm_noperands (PATTERN (insn)) >= 0);
-	      if (!first_cycle_insn_p && asm_p)
-		/* This is asm insn which is tried to be issued on the
-		   cycle not first.  Issue it on the next cycle.  */
-		cost = 1;
-	      else
-		/* A USE insn, or something else we don't need to
-		   understand.  We can't pass these directly to
-		   state_transition because it will trigger a
-		   fatal error for unrecognizable insns.  */
-		cost = 0;
-	    }
-	  else if (sched_pressure_p)
-	    cost = 0;
-	  else
-	    {
-	      cost = state_transition (temp_state, insn);
-	      if (cost < 0)
-		cost = 0;
-	      else if (cost == 0)
-		cost = 1;
-	    }
-
-	  if (cost >= 1)
-	    {
-	      queue_insn (insn, cost);
- 	      if (SCHED_GROUP_P (insn))
- 		{
- 		  advance = cost;
- 		  break;
- 		}
-
-	      continue;
-	    }
 
 	  if (current_sched_info->can_schedule_ready_p
 	      && ! (*current_sched_info->can_schedule_ready_p) (insn))
 	    /* We normally get here only if we don't want to move
 	       insn from the split block.  */
 	    {
-	      TODO_SPEC (insn) = (TODO_SPEC (insn) & ~SPECULATIVE) | HARD_DEP;
-	      continue;
+	      TODO_SPEC (insn) = HARD_DEP;
+	      goto restart_choose_ready;
 	    }
 
-	  /* DECISION is made.  */
-
-          if (TODO_SPEC (insn) & SPECULATIVE)
-            generate_recovery_code (insn);
-
-	  if (control_flow_insn_p (last_scheduled_insn)
-	      /* This is used to switch basic blocks by request
-		 from scheduler front-end (actually, sched-ebb.c only).
-		 This is used to process blocks with single fallthru
-		 edge.  If succeeding block has jump, it [jump] will try
-		 move at the end of current bb, thus corrupting CFG.  */
-	      || current_sched_info->advance_target_bb (*target_bb, insn))
+	  if (delay_htab)
 	    {
-	      *target_bb = current_sched_info->advance_target_bb
-		(*target_bb, 0);
-
-	      if (sched_verbose)
+	      /* If this insn is the first part of a delay-slot pair, record a
+		 backtrack point.  */
+	      struct delay_pair *delay_entry;
+	      delay_entry
+		= (struct delay_pair *)htab_find_with_hash (delay_htab, insn,
+							    htab_hash_pointer (insn));
+	      if (delay_entry)
 		{
-		  rtx x;
-
-		  x = next_real_insn (last_scheduled_insn);
-		  gcc_assert (x);
-		  dump_new_block_header (1, *target_bb, x, tail);
+		  save_backtrack_point (delay_entry, ls);
+		  if (sched_verbose >= 2)
+		    fprintf (sched_dump, ";;\t\tsaving backtrack point\n");
 		}
-
-	      last_scheduled_insn = bb_note (*target_bb);
 	    }
 
-	  /* Update counters, etc in the scheduler's front end.  */
-	  (*current_sched_info->begin_schedule_ready) (insn,
-						       last_scheduled_insn);
+	  /* DECISION is made.  */
 
-	  move_insn (insn, last_scheduled_insn, current_sched_info->next_tail);
+	  if (modulo_ii > 0 && INSN_UID (insn) < modulo_iter0_max_uid)
+	    {
+	      modulo_insns_scheduled++;
+	      modulo_last_stage = clock_var / modulo_ii;
+	    }
+          if (TODO_SPEC (insn) & SPECULATIVE)
+            generate_recovery_code (insn);
 
 	  if (targetm.sched.dispatch (NULL_RTX, IS_DISPATCH_ON))
 	    targetm.sched.dispatch_do (insn, ADD_TO_DISPATCH_WINDOW);
 
-	  reemit_notes (insn);
-	  last_scheduled_insn = insn;
+	  /* Update counters, etc in the scheduler's front end.  */
+	  (*current_sched_info->begin_schedule_ready) (insn);
+ 	  VEC_safe_push (rtx, heap, scheduled_insns, insn);
+	  gcc_assert (NONDEBUG_INSN_P (insn));
+	  last_nondebug_scheduled_insn = last_scheduled_insn = insn;
 
-	  if (memcmp (curr_state, temp_state, dfa_state_size) != 0)
-            {
-              cycle_issued_insns++;
-              memcpy (curr_state, temp_state, dfa_state_size);
-            }
+	  if (recog_memoized (insn) >= 0)
+	    {
+	      memcpy (temp_state, curr_state, dfa_state_size);
+	      cost = state_transition (curr_state, insn);
+	      if (!sched_pressure_p)
+		gcc_assert (cost < 0);
+	      if (memcmp (temp_state, curr_state, dfa_state_size) != 0)
+		cycle_issued_insns++;
+	      asm_p = false;
+	    }
+	  else
+	    asm_p = (GET_CODE (PATTERN (insn)) == ASM_INPUT
+		     || asm_noperands (PATTERN (insn)) >= 0);
 
 	  if (targetm.sched.variable_issue)
-	    can_issue_more =
+	    ls.can_issue_more =
 	      targetm.sched.variable_issue (sched_dump, sched_verbose,
-					    insn, can_issue_more);
+					    insn, ls.can_issue_more);
 	  /* A naked CLOBBER or USE generates no instruction, so do
 	     not count them against the issue rate.  */
 	  else if (GET_CODE (PATTERN (insn)) != USE
 		   && GET_CODE (PATTERN (insn)) != CLOBBER)
-	    can_issue_more--;
+	    ls.can_issue_more--;
 	  advance = schedule_insn (insn);
 
+	  if (SHADOW_P (insn))
+	    ls.shadows_only_p = true;
+
 	  /* After issuing an asm insn we should start a new cycle.  */
 	  if (advance == 0 && asm_p)
 	    advance = 1;
-	  if (advance != 0)
+
+	  if (must_backtrack)
 	    break;
 
-	  first_cycle_insn_p = false;
+	  if (advance != 0)
+	    break;
 
-	  /* Sort the ready list based on priority.  This must be
-	     redone here, as schedule_insn may have readied additional
-	     insns that will not be sorted correctly.  */
+	  ls.first_cycle_insn_p = false;
 	  if (ready.n_ready > 0)
-	    ready_sort (&ready);
+	    prune_ready_list (temp_state, false, ls.shadows_only_p,
+			      ls.modulo_epilogue);
+	}
 
-	  /* Quickly go through debug insns such that md sched
-	     reorder2 doesn't have to deal with debug insns.  */
-	  if (ready.n_ready && DEBUG_INSN_P (ready_element (&ready, 0))
-	      && (*current_sched_info->schedule_more_p) ())
+    do_backtrack:
+      if (!must_backtrack)
+	for (i = 0; i < ready.n_ready; i++)
+	  {
+	    rtx insn = ready_element (&ready, i);
+	    if (INSN_EXACT_TICK (insn) == clock_var)
+	      {
+		must_backtrack = true;
+		clock_var++;
+		break;
+	      }
+	  }
+      if (must_backtrack && modulo_ii > 0)
+	{
+	  if (modulo_backtracks_left == 0)
+	    goto end_schedule;
+	  modulo_backtracks_left--;
+	}
+      while (must_backtrack)
+	{
+	  struct haifa_saved_data *failed;
+	  rtx failed_insn;
+
+	  must_backtrack = false;
+	  failed = verify_shadows ();
+	  gcc_assert (failed);
+
+	  failed_insn = failed->delay_pair->i1;
+	  toggle_cancelled_flags (false);
+	  unschedule_insns_until (failed_insn);
+	  while (failed != backtrack_queue)
+	    free_topmost_backtrack_point (true);
+	  restore_last_backtrack_point (&ls);
+	  if (sched_verbose >= 2)
+	    fprintf (sched_dump, ";;\t\trewind to cycle %d\n", clock_var);
+	  /* Delay by at least a cycle.  This could cause additional
+	     backtracking.  */
+	  queue_insn (failed_insn, 1, "backtracked");
+	  advance = 0;
+	  if (must_backtrack)
+	    continue;
+	  if (ready.n_ready > 0)
+	    goto resume_after_backtrack;
+	  else
 	    {
-	      if (control_flow_insn_p (last_scheduled_insn))
-		{
-		  *target_bb = current_sched_info->advance_target_bb
-		    (*target_bb, 0);
-
-		  if (sched_verbose)
-		    {
-		      rtx x;
-
-		      x = next_real_insn (last_scheduled_insn);
-		      gcc_assert (x);
-		      dump_new_block_header (1, *target_bb, x, tail);
-		    }
+	      if (clock_var == 0 && ls.first_cycle_insn_p)
+		goto end_schedule;
+	      advance = 1;
+	      break;
+	    }
+	}
+    }
+  if (ls.modulo_epilogue)
+    success = true;
 
-		  last_scheduled_insn = bb_note (*target_bb);
-		}
+ end_schedule:
+  advance_one_cycle ();
+  if (modulo_ii > 0)
+    {
+      /* Once again, debug insn suckiness: they can be on the ready list
+	 even if they have unresolved dependencies.  To make our view
+	 of the world consistent, remove such "ready" insns.  */
+    restart_debug_insn_loop:
+      for (i = ready.n_ready - 1; i >= 0; i--)
+	{
+	  rtx x;
 
- 	      while (ready.n_ready && DEBUG_INSN_P (ready_element (&ready, 0)))
-		{
-		  insn = ready_remove_first (&ready);
-		  gcc_assert (DEBUG_INSN_P (insn));
-		  (*current_sched_info->begin_schedule_ready)
-		    (insn, last_scheduled_insn);
-		  move_insn (insn, last_scheduled_insn,
-			     current_sched_info->next_tail);
-		  advance = schedule_insn (insn);
-		  last_scheduled_insn = insn;
-		  gcc_assert (advance == 0);
-		  if (ready.n_ready > 0)
-		    ready_sort (&ready);
-		}
+	  x = ready_element (&ready, i);
+	  if (DEPS_LIST_FIRST (INSN_HARD_BACK_DEPS (x)) != NULL
+	      || DEPS_LIST_FIRST (INSN_SPEC_BACK_DEPS (x)) != NULL)
+	    {
+	      ready_remove (&ready, i);
+	      goto restart_debug_insn_loop;
 	    }
+	}
+      for (i = ready.n_ready - 1; i >= 0; i--)
+	{
+	  rtx x;
 
-	  if (targetm.sched.reorder2
-	      && (ready.n_ready == 0
-		  || !SCHED_GROUP_P (ready_element (&ready, 0))))
+	  x = ready_element (&ready, i);
+	  resolve_dependencies (x);
+	}
+      for (i = 0; i <= max_insn_queue_index; i++)
+	{
+	  rtx link;
+	  while ((link = insn_queue[i]) != NULL)
 	    {
-	      can_issue_more =
-		targetm.sched.reorder2 (sched_dump, sched_verbose,
-					ready.n_ready
-					? ready_lastpos (&ready) : NULL,
-					&ready.n_ready, clock_var);
+	      rtx x = XEXP (link, 0);
+	      insn_queue[i] = XEXP (link, 1);
+	      QUEUE_INDEX (x) = QUEUE_NOWHERE;
+	      free_INSN_LIST_node (link);
+	      resolve_dependencies (x);
 	    }
 	}
     }
@@ -3289,11 +4655,11 @@ schedule_block (basic_block *target_bb)
       debug_ready_list (&ready);
     }
 
-  if (current_sched_info->queue_must_finish_empty)
+  if (modulo_ii == 0 && current_sched_info->queue_must_finish_empty)
     /* Sanity check -- queue must be empty now.  Meaningless if region has
        multiple bbs.  */
     gcc_assert (!q_size && !ready.n_ready && !ready.n_debug);
-  else
+  else if (modulo_ii == 0)
     {
       /* We must maintain QUEUE_INDEX between blocks in region.  */
       for (i = ready.n_ready - 1; i >= 0; i--)
@@ -3302,7 +4668,7 @@ schedule_block (basic_block *target_bb)
 
 	  x = ready_element (&ready, i);
 	  QUEUE_INDEX (x) = QUEUE_NOWHERE;
-	  TODO_SPEC (x) = (TODO_SPEC (x) & ~SPECULATIVE) | HARD_DEP;
+	  TODO_SPEC (x) = HARD_DEP;
 	}
 
       if (q_size)
@@ -3315,14 +4681,22 @@ schedule_block (basic_block *target_bb)
 
 		x = XEXP (link, 0);
 		QUEUE_INDEX (x) = QUEUE_NOWHERE;
-		TODO_SPEC (x) = (TODO_SPEC (x) & ~SPECULATIVE) | HARD_DEP;
+		TODO_SPEC (x) = HARD_DEP;
 	      }
 	    free_INSN_LIST_list (&insn_queue[i]);
 	  }
     }
 
-  if (sched_verbose)
-    fprintf (sched_dump, ";;   total time = %d\n", clock_var);
+  if (success)
+    {
+      commit_schedule (prev_head, tail, target_bb);
+      if (sched_verbose)
+	fprintf (sched_dump, ";;   total time = %d\n", clock_var);
+    }
+  else
+    last_scheduled_insn = tail;
+
+  VEC_truncate (rtx, scheduled_insns, 0);
 
   if (!current_sched_info->queue_must_finish_empty
       || haifa_recovery_bb_recently_added_p)
@@ -3358,6 +4732,10 @@ schedule_block (basic_block *target_bb)
 
   current_sched_info->head = head;
   current_sched_info->tail = tail;
+
+  free_backtrack_queue ();
+
+  return success;
 }
 
 /* Set_priorities: compute priority of each insn in the block.  */
@@ -3482,7 +4860,8 @@ sched_init (void)
 
   init_alias_analysis ();
 
-  df_set_flags (DF_LR_RUN_DCE);
+  if (!sched_no_dce)
+    df_set_flags (DF_LR_RUN_DCE);
   df_note_add_problem ();
 
   /* More problems needed for interloop dep calculation in SMS.  */
@@ -3533,6 +4912,8 @@ haifa_sched_init (void)
   setup_sched_dump ();
   sched_init ();
 
+  scheduled_insns = VEC_alloc (rtx, heap, 0);
+
   if (spec_info != NULL)
     {
       sched_deps_info->use_deps_list = 1;
@@ -3572,6 +4953,8 @@ haifa_sched_init (void)
   nr_begin_data = nr_begin_control = nr_be_in_data = nr_be_in_control = 0;
   before_recovery = 0;
   after_recovery = 0;
+
+  modulo_ii = 0;
 }
 
 /* Finish work with the data specific to the Haifa scheduler.  */
@@ -3603,6 +4986,8 @@ haifa_sched_finish (void)
                c, nr_be_in_control);
     }
 
+  VEC_free (rtx, heap, scheduled_insns);
+
   /* Finalize h_i_d, dependency caches, and luids for the whole
      function.  Target will be finalized in md_global_finish ().  */
   sched_deps_finish ();
@@ -3643,6 +5028,17 @@ sched_finish (void)
 #endif
 }
 
+/* Free all delay_pair structures that were recorded.  */
+void
+free_delay_pairs (void)
+{
+  if (delay_htab)
+    {
+      htab_empty (delay_htab);
+      htab_empty (delay_htab_i2);
+    }
+}
+
 /* Fix INSN_TICKs of the instructions in the current block as well as
    INSN_TICKs of their dependents.
    HEAD and TAIL are the begin and the end of the current scheduled block.  */
@@ -3718,8 +5114,6 @@ fix_inter_tick (rtx head, rtx tail)
   bitmap_clear (&processed);
 }
 
-static int haifa_speculate_insn (rtx, ds_t, rtx *);
-
 /* Check if NEXT is ready to be added to the ready or queue list.
    If "yes", add it to the proper list.
    Returns:
@@ -3729,72 +5123,22 @@ static int haifa_speculate_insn (rtx, ds_t, rtx *);
 int
 try_ready (rtx next)
 {
-  ds_t old_ts, *ts;
+  ds_t old_ts, new_ts;
 
-  ts = &TODO_SPEC (next);
-  old_ts = *ts;
+  old_ts = TODO_SPEC (next);
 
-  gcc_assert (!(old_ts & ~(SPECULATIVE | HARD_DEP))
+  gcc_assert (!(old_ts & ~(SPECULATIVE | HARD_DEP | DEP_CONTROL))
 	      && ((old_ts & HARD_DEP)
-		  || (old_ts & SPECULATIVE)));
-
-  if (sd_lists_empty_p (next, SD_LIST_BACK))
-    /* NEXT has all its dependencies resolved.  */
-    {
-      /* Remove HARD_DEP bit from NEXT's status.  */
-      *ts &= ~HARD_DEP;
-
-      if (current_sched_info->flags & DO_SPECULATION)
-	/* Remove all speculative bits from NEXT's status.  */
-	*ts &= ~SPECULATIVE;
-    }
-  else
-    {
-      /* One of the NEXT's dependencies has been resolved.
-	 Recalculate NEXT's status.  */
-
-      *ts &= ~SPECULATIVE & ~HARD_DEP;
-
-      if (sd_lists_empty_p (next, SD_LIST_HARD_BACK))
-	/* Now we've got NEXT with speculative deps only.
-	   1. Look at the deps to see what we have to do.
-	   2. Check if we can do 'todo'.  */
-	{
-	  sd_iterator_def sd_it;
-	  dep_t dep;
-	  bool first_p = true;
-
-	  FOR_EACH_DEP (next, SD_LIST_BACK, sd_it, dep)
-	    {
-	      ds_t ds = DEP_STATUS (dep) & SPECULATIVE;
-
-	      if (DEBUG_INSN_P (DEP_PRO (dep))
-		  && !DEBUG_INSN_P (next))
-		continue;
-
-	      if (first_p)
-		{
-		  first_p = false;
-
-		  *ts = ds;
-		}
-	      else
-		*ts = ds_merge (*ts, ds);
-	    }
+		  || (old_ts & SPECULATIVE)
+		  || (old_ts & DEP_CONTROL)));
 
-	  if (ds_weak (*ts) < spec_info->data_weakness_cutoff)
-	    /* Too few points.  */
-	    *ts = (*ts & ~SPECULATIVE) | HARD_DEP;
-	}
-      else
-	*ts |= HARD_DEP;
-    }
+  new_ts = recompute_todo_spec (next);
 
-  if (*ts & HARD_DEP)
-    gcc_assert (*ts == old_ts
+  if (new_ts & HARD_DEP)
+    gcc_assert (new_ts == old_ts
 		&& QUEUE_INDEX (next) == QUEUE_NOWHERE);
   else if (current_sched_info->new_ready)
-    *ts = current_sched_info->new_ready (next, *ts);
+    new_ts = current_sched_info->new_ready (next, new_ts);
 
   /* * if !(old_ts & SPECULATIVE) (e.g. HARD_DEP or 0), then insn might
      have its original pattern or changed (speculative) one.  This is due
@@ -3802,29 +5146,29 @@ try_ready (rtx next)
      * But if (old_ts & SPECULATIVE), then we are pretty sure that insn
      has speculative pattern.
 
-     We can't assert (!(*ts & HARD_DEP) || *ts == old_ts) here because
+     We can't assert (!(new_ts & HARD_DEP) || new_ts == old_ts) here because
      control-speculative NEXT could have been discarded by sched-rgn.c
      (the same case as when discarded by can_schedule_ready_p ()).  */
 
-  if ((*ts & SPECULATIVE)
-      /* If (old_ts == *ts), then (old_ts & SPECULATIVE) and we don't
+  if ((new_ts & SPECULATIVE)
+      /* If (old_ts == new_ts), then (old_ts & SPECULATIVE) and we don't
 	 need to change anything.  */
-      && *ts != old_ts)
+      && new_ts != old_ts)
     {
       int res;
       rtx new_pat;
 
-      gcc_assert ((*ts & SPECULATIVE) && !(*ts & ~SPECULATIVE));
+      gcc_assert ((new_ts & SPECULATIVE) && !(new_ts & ~SPECULATIVE));
 
-      res = haifa_speculate_insn (next, *ts, &new_pat);
+      res = haifa_speculate_insn (next, new_ts, &new_pat);
 
       switch (res)
 	{
 	case -1:
 	  /* It would be nice to change DEP_STATUS of all dependences,
-	     which have ((DEP_STATUS & SPECULATIVE) == *ts) to HARD_DEP,
+	     which have ((DEP_STATUS & SPECULATIVE) == new_ts) to HARD_DEP,
 	     so we won't reanalyze anything.  */
-	  *ts = (*ts & ~SPECULATIVE) | HARD_DEP;
+	  new_ts = HARD_DEP;
 	  break;
 
 	case 0:
@@ -3840,7 +5184,8 @@ try_ready (rtx next)
 	       save it.  */
 	    ORIG_PAT (next) = PATTERN (next);
 
-	  haifa_change_pattern (next, new_pat);
+	  res = haifa_change_pattern (next, new_pat);
+	  gcc_assert (res);
 	  break;
 
 	default:
@@ -3848,14 +5193,16 @@ try_ready (rtx next)
 	}
     }
 
-  /* We need to restore pattern only if (*ts == 0), because otherwise it is
-     either correct (*ts & SPECULATIVE),
-     or we simply don't care (*ts & HARD_DEP).  */
+  /* We need to restore pattern only if (new_ts == 0), because otherwise it is
+     either correct (new_ts & SPECULATIVE),
+     or we simply don't care (new_ts & HARD_DEP).  */
 
   gcc_assert (!ORIG_PAT (next)
 	      || !IS_SPECULATION_BRANCHY_CHECK_P (next));
 
-  if (*ts & HARD_DEP)
+  TODO_SPEC (next) = new_ts;
+
+  if (new_ts & HARD_DEP)
     {
       /* We can't assert (QUEUE_INDEX (next) == QUEUE_NOWHERE) here because
 	 control-speculative NEXT could have been discarded by sched-rgn.c
@@ -3863,35 +5210,38 @@ try_ready (rtx next)
       /*gcc_assert (QUEUE_INDEX (next) == QUEUE_NOWHERE);*/
 
       change_queue_index (next, QUEUE_NOWHERE);
+
       return -1;
     }
-  else if (!(*ts & BEGIN_SPEC) && ORIG_PAT (next) && !IS_SPECULATION_CHECK_P (next))
+  else if (!(new_ts & BEGIN_SPEC)
+	   && ORIG_PAT (next) && PREDICATED_PAT (next) == NULL_RTX
+	   && !IS_SPECULATION_CHECK_P (next))
     /* We should change pattern of every previously speculative
        instruction - and we determine if NEXT was speculative by using
        ORIG_PAT field.  Except one case - speculation checks have ORIG_PAT
        pat too, so skip them.  */
     {
-      haifa_change_pattern (next, ORIG_PAT (next));
+      bool success = haifa_change_pattern (next, ORIG_PAT (next));
+      gcc_assert (success);
       ORIG_PAT (next) = 0;
     }
 
   if (sched_verbose >= 2)
     {
-      int s = TODO_SPEC (next);
-
       fprintf (sched_dump, ";;\t\tdependencies resolved: insn %s",
                (*current_sched_info->print_insn) (next, 0));
 
       if (spec_info && spec_info->dump)
         {
-          if (s & BEGIN_DATA)
+          if (new_ts & BEGIN_DATA)
             fprintf (spec_info->dump, "; data-spec;");
-          if (s & BEGIN_CONTROL)
+          if (new_ts & BEGIN_CONTROL)
             fprintf (spec_info->dump, "; control-spec;");
-          if (s & BE_IN_CONTROL)
+          if (new_ts & BE_IN_CONTROL)
             fprintf (spec_info->dump, "; in-control-spec;");
         }
-
+      if (TODO_SPEC (next) & DEP_CONTROL)
+	fprintf (sched_dump, " predicated");
       fprintf (sched_dump, "\n");
     }
 
@@ -3974,7 +5324,7 @@ change_queue_index (rtx next, int delay)
   if (delay == QUEUE_READY)
     ready_add (readyp, next, false);
   else if (delay >= 1)
-    queue_insn (next, delay);
+    queue_insn (next, delay, "change queue index");
 
   if (sched_verbose >= 2)
     {
@@ -4004,6 +5354,7 @@ sched_extend_ready_list (int new_sched_ready_n_insns)
     {
       i = 0;
       sched_ready_n_insns = 0;
+      VEC_reserve (rtx, heap, scheduled_insns, new_sched_ready_n_insns);
     }
   else
     i = sched_ready_n_insns + 1;
@@ -4866,28 +6217,33 @@ fix_recovery_deps (basic_block rec)
   add_jump_dependencies (insn, jump);
 }
 
-/* Change pattern of INSN to NEW_PAT.  */
-void
-sched_change_pattern (rtx insn, rtx new_pat)
+/* Change pattern of INSN to NEW_PAT.  Invalidate cached haifa
+   instruction data.  */
+static bool
+haifa_change_pattern (rtx insn, rtx new_pat)
 {
+  sd_iterator_def sd_it;
+  dep_t dep;
   int t;
 
   t = validate_change (insn, &PATTERN (insn), new_pat, 0);
-  gcc_assert (t);
+  if (!t)
+    return false;
   dfa_clear_single_insn_cache (insn);
-}
 
-/* Change pattern of INSN to NEW_PAT.  Invalidate cached haifa
-   instruction data.  */
-static void
-haifa_change_pattern (rtx insn, rtx new_pat)
-{
-  sched_change_pattern (insn, new_pat);
+  sd_it = sd_iterator_start (insn,
+			     SD_LIST_FORW | SD_LIST_BACK | SD_LIST_RES_BACK);
+  while (sd_iterator_cond (&sd_it, &dep))
+    {
+      DEP_COST (dep) = UNKNOWN_DEP_COST;
+      sd_iterator_next (&sd_it);
+    }
 
   /* Invalidate INSN_COST, so it'll be recalculated.  */
   INSN_COST (insn) = -1;
   /* Invalidate INSN_TICK, so it'll be recalculated.  */
   INSN_TICK (insn) = INVALID_TICK;
+  return true;
 }
 
 /* -1 - can't speculate,
@@ -5313,6 +6669,11 @@ check_cfg (rtx head, rtx tail)
 		    gcc_assert (/* Usual case.  */
                                 (EDGE_COUNT (bb->succs) > 1
                                  && !BARRIER_P (NEXT_INSN (head)))
+				/* Special cases, see cfglayout.c:
+				   fixup_reorder_chain.  */
+				|| (EDGE_COUNT (bb->succs) == 1
+				    && (!onlyjump_p (head)
+					|| returnjump_p (head)))
                                 /* Or jump to the next instruction.  */
                                 || (EDGE_COUNT (bb->succs) == 1
                                     && (BB_HEAD (EDGE_I (bb->succs, 0)->dest)
@@ -5529,6 +6890,7 @@ init_h_i_d (rtx insn)
       INSN_COST (insn) = -1;
       QUEUE_INDEX (insn) = QUEUE_NOWHERE;
       INSN_TICK (insn) = INVALID_TICK;
+      INSN_EXACT_TICK (insn) = INVALID_TICK;
       INTER_TICK (insn) = INVALID_TICK;
       TODO_SPEC (insn) = HARD_DEP;
     }
@@ -5630,9 +6992,16 @@ sched_create_empty_bb_1 (basic_block after)
 rtx
 sched_emit_insn (rtx pat)
 {
-  rtx insn = emit_insn_after (pat, last_scheduled_insn);
-  last_scheduled_insn = insn;
+  rtx insn = emit_insn_before (pat, nonscheduled_insns_begin);
   haifa_init_insn (insn);
+
+  if (current_sched_info->add_remove_insn)
+    current_sched_info->add_remove_insn (insn, 0);
+
+  (*current_sched_info->begin_schedule_ready) (insn);
+  VEC_safe_push (rtx, heap, scheduled_insns, insn);
+
+  last_scheduled_insn = insn;
   return insn;
 }
 
diff --git a/gcc/hooks.c b/gcc/hooks.c
index f859dd9..2b934c0 100644
--- a/gcc/hooks.c
+++ b/gcc/hooks.c
@@ -149,6 +149,12 @@ hook_int_const_tree_0 (const_tree a ATTRIBUTE_UNUSED)
   return 0;
 }
 
+int
+hook_int_void_0 (void)
+{
+  return 0;
+}
+
 /* ??? Used for comp_type_attributes, which ought to return bool.  */
 int
 hook_int_const_tree_const_tree_1 (const_tree a ATTRIBUTE_UNUSED, const_tree b ATTRIBUTE_UNUSED)
diff --git a/gcc/hooks.h b/gcc/hooks.h
index 7962fe8..0a017a6 100644
--- a/gcc/hooks.h
+++ b/gcc/hooks.h
@@ -69,6 +69,8 @@ extern int hook_int_const_tree_const_tree_1 (const_tree, const_tree);
 extern int hook_int_rtx_0 (rtx);
 extern int hook_int_rtx_bool_0 (rtx, bool);
 
+extern int hook_int_void_0 (void);
+
 extern tree hook_tree_const_tree_null (const_tree);
 
 extern tree hook_tree_tree_tree_null (tree, tree);
diff --git a/gcc/ifcvt.c b/gcc/ifcvt.c
index eee5cc7..4b890ce 100644
--- a/gcc/ifcvt.c
+++ b/gcc/ifcvt.c
@@ -103,7 +103,7 @@ static int cond_exec_find_if_block (ce_if_block_t *);
 static int find_if_case_1 (basic_block, edge, edge);
 static int find_if_case_2 (basic_block, edge, edge);
 static int dead_or_predicable (basic_block, basic_block, basic_block,
-			       basic_block, int);
+			       edge, int);
 static void noce_emit_move_insn (rtx, rtx);
 static rtx block_has_only_trap (basic_block);
 
@@ -304,6 +304,10 @@ cond_exec_process_insns (ce_if_block_t *ce_info ATTRIBUTE_UNUSED,
 
   for (insn = start; ; insn = NEXT_INSN (insn))
     {
+      /* dwarf2out can't cope with conditional prologues.  */
+      if (NOTE_P (insn) && NOTE_KIND (insn) == NOTE_INSN_PROLOGUE_END)
+	return FALSE;
+
       if (NOTE_P (insn) || DEBUG_INSN_P (insn))
 	goto insn_done;
 
@@ -879,7 +883,7 @@ noce_emit_move_insn (rtx x, rtx y)
 		}
 
 	      gcc_assert (start < (MEM_P (op) ? BITS_PER_UNIT : BITS_PER_WORD));
-	      store_bit_field (op, size, start, GET_MODE (x), y);
+	      store_bit_field (op, size, start, false, GET_MODE (x), y);
 	      return;
 	    }
 
@@ -933,7 +937,7 @@ noce_emit_move_insn (rtx x, rtx y)
   inner = XEXP (outer, 0);
   outmode = GET_MODE (outer);
   bitpos = SUBREG_BYTE (outer) * BITS_PER_UNIT;
-  store_bit_field (inner, GET_MODE_BITSIZE (outmode), bitpos, outmode, y);
+  store_bit_field (inner, GET_MODE_BITSIZE (outmode), bitpos, false, outmode, y);
 }
 
 /* Return sequence of instructions generated by if conversion.  This
@@ -3789,6 +3793,7 @@ find_if_case_1 (basic_block test_bb, edge then_edge, edge else_edge)
   basic_block then_bb = then_edge->dest;
   basic_block else_bb = else_edge->dest;
   basic_block new_bb;
+  rtx else_target = NULL_RTX;
   int then_bb_index;
 
   /* If we are partitioning hot/cold basic blocks, we don't want to
@@ -3838,9 +3843,16 @@ find_if_case_1 (basic_block test_bb, edge then_edge, edge else_edge)
 				    predictable_edge_p (then_edge)))))
     return FALSE;
 
+  if (else_bb == EXIT_BLOCK_PTR)
+    {
+      rtx jump = BB_END (else_edge->src);
+      gcc_assert (JUMP_P (jump));
+      else_target = JUMP_LABEL (jump);
+    }
+
   /* Registers set are dead, or are predicable.  */
   if (! dead_or_predicable (test_bb, then_bb, else_bb,
-			    single_succ (then_bb), 1))
+			    single_succ_edge (then_bb), 1))
     return FALSE;
 
   /* Conversion went ok, including moving the insns and fixing up the
@@ -3857,6 +3869,9 @@ find_if_case_1 (basic_block test_bb, edge then_edge, edge else_edge)
       redirect_edge_succ (FALLTHRU_EDGE (test_bb), else_bb);
       new_bb = 0;
     }
+  else if (else_bb == EXIT_BLOCK_PTR)
+    new_bb = force_nonfallthru_and_redirect (FALLTHRU_EDGE (test_bb),
+					     else_bb, else_target);
   else
     new_bb = redirect_edge_and_branch_force (FALLTHRU_EDGE (test_bb),
 					     else_bb);
@@ -3955,7 +3970,7 @@ find_if_case_2 (basic_block test_bb, edge then_edge, edge else_edge)
     return FALSE;
 
   /* Registers set are dead, or are predicable.  */
-  if (! dead_or_predicable (test_bb, else_bb, then_bb, else_succ->dest, 0))
+  if (! dead_or_predicable (test_bb, else_bb, then_bb, else_succ, 0))
     return FALSE;
 
   /* Conversion went ok, including moving the insns and fixing up the
@@ -3984,12 +3999,34 @@ find_if_case_2 (basic_block test_bb, edge then_edge, edge else_edge)
 
 static int
 dead_or_predicable (basic_block test_bb, basic_block merge_bb,
-		    basic_block other_bb, basic_block new_dest, int reversep)
+		    basic_block other_bb, edge dest_edge, int reversep)
 {
-  rtx head, end, jump, earliest = NULL_RTX, old_dest, new_label = NULL_RTX;
+  basic_block new_dest = dest_edge->dest;
+  rtx head, end, jump, earliest = NULL_RTX, old_dest;
   bitmap merge_set = NULL;
   /* Number of pending changes.  */
   int n_validated_changes = 0;
+  rtx new_dest_label;
+
+  jump = BB_END (dest_edge->src);
+  if (JUMP_P (jump))
+    {
+      new_dest_label = JUMP_LABEL (jump);
+      if (new_dest_label == NULL_RTX)
+	{
+	  new_dest_label = PATTERN (jump);
+	  gcc_assert (ANY_RETURN_P (new_dest_label));
+	}
+    }
+  else if (other_bb != new_dest)
+    {
+      if (new_dest == EXIT_BLOCK_PTR)
+	new_dest_label = ret_rtx;
+      else
+	new_dest_label = block_label (new_dest);
+    }
+  else
+    new_dest_label = NULL_RTX;
 
   jump = BB_END (test_bb);
 
@@ -4127,10 +4164,9 @@ dead_or_predicable (basic_block test_bb, basic_block merge_bb,
   old_dest = JUMP_LABEL (jump);
   if (other_bb != new_dest)
     {
-      new_label = block_label (new_dest);
       if (reversep
-	  ? ! invert_jump_1 (jump, new_label)
-	  : ! redirect_jump_1 (jump, new_label))
+	  ? ! invert_jump_1 (jump, new_dest_label)
+	  : ! redirect_jump_1 (jump, new_dest_label))
 	goto cancel;
     }
 
@@ -4141,7 +4177,7 @@ dead_or_predicable (basic_block test_bb, basic_block merge_bb,
 
   if (other_bb != new_dest)
     {
-      redirect_jump_2 (jump, old_dest, new_label, 0, reversep);
+      redirect_jump_2 (jump, old_dest, new_dest_label, 0, reversep);
 
       redirect_edge_succ (BRANCH_EDGE (test_bb), new_dest);
       if (reversep)
diff --git a/gcc/incpath.c b/gcc/incpath.c
index eb097c1..f5c6225 100644
--- a/gcc/incpath.c
+++ b/gcc/incpath.c
@@ -30,6 +30,9 @@
 #include "intl.h"
 #include "incpath.h"
 #include "cppdefault.h"
+#include "flags.h"
+#include "toplev.h"
+#include "diagnostic-core.h"
 
 /* Microsoft Windows does not natively support inodes.
    VMS has non-numeric inodes.  */
@@ -365,6 +368,24 @@ merge_include_chains (const char *sysroot, cpp_reader *pfile, int verbose)
 	}
       fprintf (stderr, _("End of search list.\n"));
     }
+
+#ifdef ENABLE_POISON_SYSTEM_DIRECTORIES
+  if (flag_poison_system_directories)
+    {
+	struct cpp_dir *p;
+
+	for (p = heads[QUOTE]; p; p = p->next)
+	  {
+	   if ((!strncmp (p->name, "/usr/include", 12))
+	       || (!strncmp (p->name, "/usr/local/include", 18))
+	       || (!strncmp (p->name, "/usr/X11R6/include", 18)))
+	     warning (OPT_Wpoison_system_directories,
+		      "include location \"%s\" is unsafe for "
+		      "cross-compilation",
+		      p->name);
+	  }
+    }
+#endif
 }
 
 /* Use given -I paths for #include "..." but not #include <...>, and
diff --git a/gcc/integrate.c b/gcc/integrate.c
index 7072a75..5a3ec24 100644
--- a/gcc/integrate.c
+++ b/gcc/integrate.c
@@ -55,6 +55,7 @@ along with GCC; see the file COPYING3.  If not see
 typedef struct GTY(()) initial_value_pair {
   rtx hard_reg;
   rtx pseudo;
+  bool initialized;
 } initial_value_pair;
 typedef struct GTY(()) initial_value_struct {
   int num_entries;
@@ -238,6 +239,7 @@ get_hard_reg_initial_val (enum machine_mode mode, unsigned int regno)
 {
   struct initial_value_struct *ivs;
   rtx rv;
+  int entry;
 
   rv = has_hard_reg_initial_val (mode, regno);
   if (rv)
@@ -253,17 +255,19 @@ get_hard_reg_initial_val (enum machine_mode mode, unsigned int regno)
       crtl->hard_reg_initial_vals = ivs;
     }
 
-  if (ivs->num_entries >= ivs->max_entries)
+  entry = ivs->num_entries++;
+  if (entry >= ivs->max_entries)
     {
       ivs->max_entries += 5;
       ivs->entries = GGC_RESIZEVEC (initial_value_pair, ivs->entries,
 				    ivs->max_entries);
     }
 
-  ivs->entries[ivs->num_entries].hard_reg = gen_rtx_REG (mode, regno);
-  ivs->entries[ivs->num_entries].pseudo = gen_reg_rtx (mode);
+  ivs->entries[entry].hard_reg = gen_rtx_REG (mode, regno);
+  ivs->entries[entry].pseudo = gen_reg_rtx (mode);
+  ivs->entries[entry].initialized = false;
 
-  return ivs->entries[ivs->num_entries++].pseudo;
+  return ivs->entries[entry].pseudo;
 }
 
 /* See if get_hard_reg_initial_val has been used to create a pseudo
@@ -298,7 +302,16 @@ emit_initial_value_sets (void)
 
   start_sequence ();
   for (i = 0; i < ivs->num_entries; i++)
-    emit_move_insn (ivs->entries[i].pseudo, ivs->entries[i].hard_reg);
+    {
+      if (ivs->entries[i].initialized)
+	continue;
+      ivs->entries[i].initialized = true;
+      emit_move_insn (ivs->entries[i].pseudo, ivs->entries[i].hard_reg);
+#ifdef HAVE_use_initial_val
+      if (HAVE_use_initial_val)
+	emit_insn (gen_use_initial_val (ivs->entries[i].pseudo));
+#endif
+    }
   seq = get_insns ();
   end_sequence ();
 
diff --git a/gcc/ira-build.c b/gcc/ira-build.c
index b3c1e14..017659b 100644
--- a/gcc/ira-build.c
+++ b/gcc/ira-build.c
@@ -480,6 +480,7 @@ ira_create_allocno (int regno, bool cap_p, ira_loop_tree_node_t loop_tree_node)
   ALLOCNO_HARD_REGNO (a) = -1;
   ALLOCNO_CALL_FREQ (a) = 0;
   ALLOCNO_CALLS_CROSSED_NUM (a) = 0;
+  CLEAR_HARD_REG_SET (ALLOCNO_CROSSED_CALLS_CLOBBERED_REGS (a));
 #ifdef STACK_REGS
   ALLOCNO_NO_STACK_REG_P (a) = false;
   ALLOCNO_TOTAL_NO_STACK_REG_P (a) = false;
@@ -877,6 +878,8 @@ create_cap_allocno (ira_allocno_t a)
   merge_hard_reg_conflicts (a, cap, false);
 
   ALLOCNO_CALLS_CROSSED_NUM (cap) = ALLOCNO_CALLS_CROSSED_NUM (a);
+  IOR_HARD_REG_SET (ALLOCNO_CROSSED_CALLS_CLOBBERED_REGS (cap),
+		    ALLOCNO_CROSSED_CALLS_CLOBBERED_REGS (a));
   if (internal_flag_ira_verbose > 2 && ira_dump_file != NULL)
     {
       fprintf (ira_dump_file, "    Creating cap ");
@@ -1668,6 +1671,9 @@ propagate_allocno_info (void)
 	  merge_hard_reg_conflicts (a, parent_a, true);
 	  ALLOCNO_CALLS_CROSSED_NUM (parent_a)
 	    += ALLOCNO_CALLS_CROSSED_NUM (a);
+	  IOR_HARD_REG_SET (ALLOCNO_CROSSED_CALLS_CLOBBERED_REGS (parent_a),
+			    ALLOCNO_CROSSED_CALLS_CLOBBERED_REGS (a));
+
 	  ALLOCNO_EXCESS_PRESSURE_POINTS_NUM (parent_a)
 	    += ALLOCNO_EXCESS_PRESSURE_POINTS_NUM (a);
 	  cover_class = ALLOCNO_COVER_CLASS (a);
@@ -2010,6 +2016,8 @@ propagate_some_info_from_allocno (ira_allocno_t a, ira_allocno_t from_a)
   ALLOCNO_FREQ (a) += ALLOCNO_FREQ (from_a);
   ALLOCNO_CALL_FREQ (a) += ALLOCNO_CALL_FREQ (from_a);
   ALLOCNO_CALLS_CROSSED_NUM (a) += ALLOCNO_CALLS_CROSSED_NUM (from_a);
+  IOR_HARD_REG_SET (ALLOCNO_CROSSED_CALLS_CLOBBERED_REGS (a),
+		    ALLOCNO_CROSSED_CALLS_CLOBBERED_REGS (from_a));
   ALLOCNO_EXCESS_PRESSURE_POINTS_NUM (a)
     += ALLOCNO_EXCESS_PRESSURE_POINTS_NUM (from_a);
   if (! ALLOCNO_BAD_SPILL_P (from_a))
@@ -2634,6 +2642,8 @@ copy_info_to_removed_store_destinations (int regno)
       ALLOCNO_CALL_FREQ (parent_a) += ALLOCNO_CALL_FREQ (a);
       ALLOCNO_CALLS_CROSSED_NUM (parent_a)
 	+= ALLOCNO_CALLS_CROSSED_NUM (a);
+      IOR_HARD_REG_SET (ALLOCNO_CROSSED_CALLS_CLOBBERED_REGS (parent_a),
+			ALLOCNO_CROSSED_CALLS_CLOBBERED_REGS (a));
       ALLOCNO_EXCESS_PRESSURE_POINTS_NUM (parent_a)
 	+= ALLOCNO_EXCESS_PRESSURE_POINTS_NUM (a);
       merged_p = true;
diff --git a/gcc/ira-costs.c b/gcc/ira-costs.c
index 2329613..b2cf357 100644
--- a/gcc/ira-costs.c
+++ b/gcc/ira-costs.c
@@ -724,11 +724,11 @@ record_reg_classes (int n_alts, int n_ops, rtx *ops,
 
 /* Wrapper around REGNO_OK_FOR_INDEX_P, to allow pseudo registers.  */
 static inline bool
-ok_for_index_p_nonstrict (rtx reg)
+ok_for_index_p_nonstrict (rtx reg, enum machine_mode mode)
 {
   unsigned regno = REGNO (reg);
 
-  return regno >= FIRST_PSEUDO_REGISTER || REGNO_OK_FOR_INDEX_P (regno);
+  return regno >= FIRST_PSEUDO_REGISTER || ok_for_index_p_1 (regno, mode);
 }
 
 /* A version of regno_ok_for_base_p for use here, when all
@@ -766,7 +766,7 @@ record_address_regs (enum machine_mode mode, rtx x, int context,
   enum reg_class rclass;
 
   if (context == 1)
-    rclass = INDEX_REG_CLASS;
+    rclass = index_reg_class (mode);
   else
     rclass = base_reg_class (mode, outer_code, index_code);
 
@@ -813,7 +813,8 @@ record_address_regs (enum machine_mode mode, rtx x, int context,
 	   just record registers in any non-constant operands.  We
 	   assume here, as well as in the tests below, that all
 	   addresses are in canonical form.  */
-	else if (INDEX_REG_CLASS == base_reg_class (VOIDmode, PLUS, SCRATCH))
+	else if (index_reg_class (mode)
+		 == base_reg_class (mode, PLUS, SCRATCH))
 	  {
 	    record_address_regs (mode, arg0, context, PLUS, code1, scale);
 	    if (! CONSTANT_P (arg1))
@@ -834,7 +835,7 @@ record_address_regs (enum machine_mode mode, rtx x, int context,
 	else if (code0 == REG && code1 == REG
 		 && REGNO (arg0) < FIRST_PSEUDO_REGISTER
 		 && (ok_for_base_p_nonstrict (arg0, mode, PLUS, REG)
-		     || ok_for_index_p_nonstrict (arg0)))
+		     || ok_for_index_p_nonstrict (arg0, mode)))
 	  record_address_regs (mode, arg1,
 			       ok_for_base_p_nonstrict (arg0, mode, PLUS, REG)
 			       ? 1 : 0,
@@ -842,7 +843,7 @@ record_address_regs (enum machine_mode mode, rtx x, int context,
 	else if (code0 == REG && code1 == REG
 		 && REGNO (arg1) < FIRST_PSEUDO_REGISTER
 		 && (ok_for_base_p_nonstrict (arg1, mode, PLUS, REG)
-		     || ok_for_index_p_nonstrict (arg1)))
+		     || ok_for_index_p_nonstrict (arg1, mode)))
 	  record_address_regs (mode, arg0,
 			       ok_for_base_p_nonstrict (arg1, mode, PLUS, REG)
 			       ? 1 : 0,
@@ -1295,12 +1296,16 @@ find_costs_and_classes (FILE *dump_file)
 		      /* Propagate costs to upper levels in the region
 			 tree.  */
 		      parent_a_num = ALLOCNO_NUM (parent_a);
+		      if (i >= first_moveable_pseudo && i < last_moveable_pseudo)
+			COSTS (total_allocno_costs, parent_a_num)->mem_cost = 0;
 		      for (k = 0; k < cost_classes_num; k++)
 			COSTS (total_allocno_costs, parent_a_num)->cost[k]
 			  += COSTS (total_allocno_costs, a_num)->cost[k];
 		      COSTS (total_allocno_costs, parent_a_num)->mem_cost
 			+= COSTS (total_allocno_costs, a_num)->mem_cost;
 		    }
+		  if (i >= first_moveable_pseudo && i < last_moveable_pseudo)
+		    COSTS (costs, a_num)->mem_cost = 0;
 		  for (k = 0; k < cost_classes_num; k++)
 		    temp_costs->cost[k] += COSTS (costs, a_num)->cost[k];
 		  temp_costs->mem_cost += COSTS (costs, a_num)->mem_cost;
@@ -1310,7 +1315,9 @@ find_costs_and_classes (FILE *dump_file)
 #endif
 		}
 	    }
-	  if (equiv_savings < 0)
+	  if (i >= first_moveable_pseudo && i < last_moveable_pseudo)
+	    temp_costs->mem_cost = 0;
+	  else if (equiv_savings < 0)
 	    temp_costs->mem_cost = -equiv_savings;
 	  else if (equiv_savings > 0)
 	    {
@@ -1766,17 +1773,27 @@ ira_tune_allocno_costs_and_cover_classes (void)
 	      regno = ira_class_hard_regs[cover_class][j];
 	      rclass = REGNO_REG_CLASS (regno);
 	      cost = 0;
-	      if (! ira_hard_reg_not_in_set_p (regno, mode, call_used_reg_set)
-		  || HARD_REGNO_CALL_PART_CLOBBERED (regno, mode))
-		cost += (ALLOCNO_CALL_FREQ (a)
-			 * (ira_memory_move_cost[mode][rclass][0]
+
+	      /* If regno is not clobbered by call set its cost to zero.
+		 This is to make this regno preferable choice for
+		 allocation.  */
+	      if (! ira_hard_reg_not_in_set_p
+		  (regno, mode, ALLOCNO_CROSSED_CALLS_CLOBBERED_REGS (a)))
+		{
+		  if (! ira_hard_reg_not_in_set_p (regno, mode,
+						   call_used_reg_set)
+		      || HARD_REGNO_CALL_PART_CLOBBERED (regno, mode))
+		    cost += (ALLOCNO_CALL_FREQ (a)
+			     * (ira_memory_move_cost[mode][rclass][0]
 			    + ira_memory_move_cost[mode][rclass][1]));
 #ifdef IRA_HARD_REGNO_ADD_COST_MULTIPLIER
-	      cost += ((ira_memory_move_cost[mode][rclass][0]
-			+ ira_memory_move_cost[mode][rclass][1])
-		       * ALLOCNO_FREQ (a)
-		       * IRA_HARD_REGNO_ADD_COST_MULTIPLIER (regno) / 2);
+		  cost += ((ira_memory_move_cost[mode][rclass][0]
+			    + ira_memory_move_cost[mode][rclass][1])
+			   * ALLOCNO_FREQ (a)
+			   * IRA_HARD_REGNO_ADD_COST_MULTIPLIER (regno) / 2);
 #endif
+		}
+
 	      reg_costs[j] += cost;
 	      if (min_cost > reg_costs[j])
 		min_cost = reg_costs[j];
diff --git a/gcc/ira-emit.c b/gcc/ira-emit.c
index b90adb7..18f16aa 100644
--- a/gcc/ira-emit.c
+++ b/gcc/ira-emit.c
@@ -217,8 +217,8 @@ add_to_edge_list (edge e, move_t move, bool head_p)
 
 /* Create and return new pseudo-register with the same attributes as
    ORIGINAL_REG.  */
-static rtx
-create_new_reg (rtx original_reg)
+rtx
+ira_create_new_reg (rtx original_reg)
 {
   rtx new_reg;
 
@@ -506,7 +506,7 @@ change_loop (ira_loop_tree_node_t node)
 		fprintf (ira_dump_file, "  %i vs parent %i:",
 			 ALLOCNO_HARD_REGNO (allocno),
 			 ALLOCNO_HARD_REGNO (parent_allocno));
-	      set_allocno_reg (allocno, create_new_reg (original_reg));
+	      set_allocno_reg (allocno, ira_create_new_reg (original_reg));
 	    }
 	}
     }
@@ -527,7 +527,7 @@ change_loop (ira_loop_tree_node_t node)
       if (! used_p)
 	continue;
       bitmap_set_bit (renamed_regno_bitmap, regno);
-      set_allocno_reg (allocno, create_new_reg (ALLOCNO_REG (allocno)));
+      set_allocno_reg (allocno, ira_create_new_reg (ALLOCNO_REG (allocno)));
     }
 }
 
@@ -733,7 +733,7 @@ modify_move_list (move_t list)
 		ALLOCNO_ASSIGNED_P (new_allocno) = true;
 		ALLOCNO_HARD_REGNO (new_allocno) = -1;
 		ALLOCNO_REG (new_allocno)
-		  = create_new_reg (ALLOCNO_REG (set_move->to));
+		  = ira_create_new_reg (ALLOCNO_REG (set_move->to));
 
 		/* Make it possibly conflicting with all earlier
 		   created allocnos.  Cases where temporary allocnos
diff --git a/gcc/ira-int.h b/gcc/ira-int.h
index a58f0ca..82fc34c 100644
--- a/gcc/ira-int.h
+++ b/gcc/ira-int.h
@@ -359,6 +359,8 @@ struct ira_allocno
   int call_freq;
   /* Accumulated number of the intersected calls.  */
   int calls_crossed_num;
+  /* Registers clobbered by intersected calls.  */
+  HARD_REG_SET crossed_calls_clobbered_regs;
   /* TRUE if the allocno assigned to memory was a destination of
      removed move (see ira-emit.c) at loop exit because the value of
      the corresponding pseudo-register is not changed inside the
@@ -453,6 +455,8 @@ struct ira_allocno
 #define ALLOCNO_HARD_REGNO(A) ((A)->hard_regno)
 #define ALLOCNO_CALL_FREQ(A) ((A)->call_freq)
 #define ALLOCNO_CALLS_CROSSED_NUM(A) ((A)->calls_crossed_num)
+#define ALLOCNO_CROSSED_CALLS_CLOBBERED_REGS(A) \
+  ((A)->crossed_calls_clobbered_regs)
 #define ALLOCNO_MEM_OPTIMIZED_DEST(A) ((A)->mem_optimized_dest)
 #define ALLOCNO_MEM_OPTIMIZED_DEST_P(A) ((A)->mem_optimized_dest_p)
 #define ALLOCNO_SOMEWHERE_RENAMED_P(A) ((A)->somewhere_renamed_p)
@@ -1388,3 +1392,6 @@ ira_allocate_and_set_or_copy_costs (int **vec, enum reg_class cover_class,
 	reg_costs[i] = val;
     }
 }
+
+extern rtx ira_create_new_reg (rtx);
+extern int first_moveable_pseudo, last_moveable_pseudo;
diff --git a/gcc/ira-lives.c b/gcc/ira-lives.c
index 5c5c415..65fd926 100644
--- a/gcc/ira-lives.c
+++ b/gcc/ira-lives.c
@@ -1168,6 +1168,10 @@ process_bb_node_lives (ira_loop_tree_node_t loop_tree_node)
 		  ira_object_t obj = ira_object_id_map[i];
 		  ira_allocno_t a = OBJECT_ALLOCNO (obj);
 		  int num = ALLOCNO_NUM (a);
+		  HARD_REG_SET this_call_used_reg_set;
+
+		  get_call_reg_set_usage (insn, &this_call_used_reg_set,
+					  call_used_reg_set);
 
 		  /* Don't allocate allocnos that cross setjmps or any
 		     call, if this function receives a nonlocal
@@ -1182,9 +1186,9 @@ process_bb_node_lives (ira_loop_tree_node_t loop_tree_node)
 		  if (can_throw_internal (insn))
 		    {
 		      IOR_HARD_REG_SET (OBJECT_CONFLICT_HARD_REGS (obj),
-					call_used_reg_set);
+					this_call_used_reg_set);
 		      IOR_HARD_REG_SET (OBJECT_TOTAL_CONFLICT_HARD_REGS (obj),
-					call_used_reg_set);
+					this_call_used_reg_set);
 		    }
 
 		  if (sparseset_bit_p (allocnos_processed, num))
@@ -1201,6 +1205,9 @@ process_bb_node_lives (ira_loop_tree_node_t loop_tree_node)
 		  /* Mark it as saved at the next call.  */
 		  allocno_saved_at_call[num] = last_call_num + 1;
 		  ALLOCNO_CALLS_CROSSED_NUM (a)++;
+		  IOR_HARD_REG_SET (ALLOCNO_CROSSED_CALLS_CLOBBERED_REGS (a),
+				    this_call_used_reg_set);
+
 		}
 	    }
 
diff --git a/gcc/ira.c b/gcc/ira.c
index f2b871f..be59dc2 100644
--- a/gcc/ira.c
+++ b/gcc/ira.c
@@ -321,7 +321,7 @@ along with GCC; see the file COPYING3.  If not see
 #include "integrate.h"
 #include "ggc.h"
 #include "ira-int.h"
-
+#include "dbgcnt.h"
 
 struct target_ira default_target_ira;
 struct target_ira_int default_target_ira_int;
@@ -3051,6 +3051,520 @@ build_insn_chain (void)
     print_insn_chains (dump_file);
 }
 
+/* Examine the rtx found in *LOC, which is read or written to as determined
+   by TYPE.  Return false if we find a reason why an insn containing this
+   rtx should not be moved (such as accesses to non-constant memory), true
+   otherwise.  */
+static bool
+rtx_moveable_p (rtx *loc, enum op_type type)
+{
+  const char *fmt;
+  rtx x = *loc;
+  enum rtx_code code = GET_CODE (x);
+  int i, j;
+
+  code = GET_CODE (x);
+  switch (code)
+    {
+    case CONST:
+    case CONST_INT:
+    case CONST_DOUBLE:
+    case CONST_FIXED:
+    case CONST_VECTOR:
+    case SYMBOL_REF:
+    case LABEL_REF:
+      return true;
+
+    case PC:
+      return type == OP_IN;
+
+    case CC0:
+      return false;
+
+    case REG:
+      if (x == frame_pointer_rtx)
+	return true;
+      if (HARD_REGISTER_P (x))
+	return false;
+      
+      return true;
+
+    case MEM:
+      if (type == OP_IN && MEM_READONLY_P (x))
+	return rtx_moveable_p (&XEXP (x, 0), OP_IN);
+      return false;
+
+    case SET:
+      return (rtx_moveable_p (&SET_SRC (x), OP_IN)
+	      && rtx_moveable_p (&SET_DEST (x), OP_OUT));
+
+    case STRICT_LOW_PART:
+      return rtx_moveable_p (&XEXP (x, 0), OP_OUT);
+
+    case ZERO_EXTRACT:
+    case SIGN_EXTRACT:
+      return (rtx_moveable_p (&XEXP (x, 0), type)
+	      && rtx_moveable_p (&XEXP (x, 1), OP_IN)
+	      && rtx_moveable_p (&XEXP (x, 2), OP_IN));
+
+    case CLOBBER:
+      return rtx_moveable_p (&SET_DEST (x), OP_OUT);
+
+    default:
+      break;
+    }
+
+  fmt = GET_RTX_FORMAT (code);
+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)
+    {
+      if (fmt[i] == 'e')
+	{
+	  if (!rtx_moveable_p (&XEXP (x, i), type))
+	    return false;
+	}
+      else if (fmt[i] == 'E')
+	for (j = XVECLEN (x, i) - 1; j >= 0; j--)
+	  {
+	    if (!rtx_moveable_p (&XVECEXP (x, i, j), type))
+	      return false;
+	  }
+    }
+  return true;
+}
+
+/* A wrapper around dominated_by_p, which uses the information in UID_LUID
+   to give dominance relationships between two insns I1 and I2.  */
+static bool
+insn_dominated_by_p (rtx i1, rtx i2, int *uid_luid)
+{
+  basic_block bb1 = BLOCK_FOR_INSN (i1);
+  basic_block bb2 = BLOCK_FOR_INSN (i2);
+
+  if (bb1 == bb2)
+    return uid_luid[INSN_UID (i2)] < uid_luid[INSN_UID (i1)];
+  return dominated_by_p (CDI_DOMINATORS, bb1, bb2);
+}
+
+/* Record the range of register numbers added by find_moveable_pseudos.  */
+int first_moveable_pseudo, last_moveable_pseudo;
+
+/* These two vectors hold data for every register added by
+   find_movable_pseudos, with index 0 holding data for the
+   first_moveable_pseudo.  */
+/* The original home register.  */
+static VEC (rtx, heap) *pseudo_replaced_reg;
+/* The move instruction we added to move the value to its original home
+   register.  */
+static VEC (rtx, heap) *pseudo_move_insn;
+
+/* Look for instances where we have an instruction that is known to increase
+   register pressure, and whose result is not used immediately.  If it is
+   possible to move the instruction downwards to just before its first use,
+   split its lifetime into two ranges.  We create a new pseudo to compute the
+   value, and emit a move instruction just before the first use.  If, after
+   register allocation, the new pseudo remains unallocated, the function
+   move_unallocated_pseudos then deletes the move instruction and places
+   the computation just before the first use.
+
+   Such a move is safe and profitable if all the input registers remain live
+   and unchanged between the original computation and its first use.  In such
+   a situation, the computation is known to increase register pressure, and
+   moving it is known to at least not worsen it.
+
+   We restrict moves to only those cases where a register remains unallocated,
+   in order to avoid interfering too much with the instruction schedule.  As
+   an exception, we may move insns which only modify their input register
+   (typically induction variables), as this increases the freedom for our
+   intended transformation, and does not limit the second instruction
+   scheduler pass.  */
+   
+static void
+find_moveable_pseudos (void)
+{
+  unsigned i;
+  int max_regs = max_reg_num ();
+  int max_uid = get_max_uid ();
+  basic_block bb;
+  int *uid_luid = XNEWVEC (int, max_uid);
+  rtx *closest_uses = XNEWVEC (rtx, max_regs);
+  /* A set of registers which are live but not modified throughout a block.  */
+  bitmap_head *bb_transp_live = XNEWVEC (bitmap_head, last_basic_block);
+  /* A set of registers which only exist in a given basic block.  */
+  bitmap_head *bb_local = XNEWVEC (bitmap_head, last_basic_block);
+  /* A set of registers which are set once, in an instruction that can be
+     moved freely downwards, but are otherwise transparent to a block.  */
+  bitmap_head *bb_moveable_reg_sets = XNEWVEC (bitmap_head, last_basic_block);
+  bitmap_head live, used, set, interesting, unusable_as_input;
+  bitmap_iterator bi;
+  bitmap_initialize (&interesting, 0);
+
+  first_moveable_pseudo = max_regs;
+  VEC_free (rtx, heap, pseudo_move_insn);
+  VEC_free (rtx, heap, pseudo_replaced_reg);
+  VEC_safe_grow (rtx, heap, pseudo_move_insn, max_regs);
+  VEC_safe_grow (rtx, heap, pseudo_replaced_reg, max_regs);
+
+  df_analyze ();
+  calculate_dominance_info (CDI_DOMINATORS);
+
+  i = 0;
+  bitmap_initialize (&live, 0);
+  bitmap_initialize (&used, 0);
+  bitmap_initialize (&set, 0);
+  bitmap_initialize (&unusable_as_input, 0);
+  FOR_EACH_BB (bb)
+    {
+      rtx insn;
+      bitmap transp = bb_transp_live + bb->index;
+      bitmap moveable = bb_moveable_reg_sets + bb->index;
+      bitmap local = bb_local + bb->index;
+
+      bitmap_initialize (local, 0);
+      bitmap_initialize (transp, 0);
+      bitmap_initialize (moveable, 0);
+      bitmap_copy (&live, df_get_live_out (bb));
+      bitmap_and_into (&live, df_get_live_in (bb));
+      bitmap_copy (transp, &live);
+      bitmap_clear (moveable);
+      bitmap_clear (&live);
+      bitmap_clear (&used);
+      bitmap_clear (&set);
+      FOR_BB_INSNS (bb, insn)
+	if (NONDEBUG_INSN_P (insn))
+	  {
+	    df_ref *u_rec, *d_rec;
+
+	    uid_luid[INSN_UID (insn)] = i++;
+	    
+	    u_rec = DF_INSN_USES (insn);
+	    d_rec = DF_INSN_DEFS (insn);
+	    if (d_rec[0] != NULL && d_rec[1] == NULL
+		&& u_rec[0] != NULL && u_rec[1] == NULL
+		&& DF_REF_REGNO (*u_rec) == DF_REF_REGNO (*d_rec)
+		&& !bitmap_bit_p (&set, DF_REF_REGNO (*u_rec))
+		&& rtx_moveable_p (&PATTERN (insn), OP_IN))
+	      {
+		unsigned regno = DF_REF_REGNO (*u_rec);
+		bitmap_set_bit (moveable, regno);
+		bitmap_set_bit (&set, regno);
+		bitmap_set_bit (&used, regno);
+		bitmap_clear_bit (transp, regno);
+		continue;
+	      }
+	    while (*u_rec)
+	      {
+		unsigned regno = DF_REF_REGNO (*u_rec);
+		bitmap_set_bit (&used, regno);
+		if (bitmap_clear_bit (moveable, regno))
+		  bitmap_clear_bit (transp, regno);
+		u_rec++;
+	      }
+
+	    while (*d_rec)
+	      {
+		unsigned regno = DF_REF_REGNO (*d_rec);
+		bitmap_set_bit (&set, regno);
+		bitmap_clear_bit (transp, regno);
+		bitmap_clear_bit (moveable, regno);
+		d_rec++;
+	      }
+	  }
+    }
+
+  bitmap_clear (&live);
+  bitmap_clear (&used);
+  bitmap_clear (&set);
+
+  FOR_EACH_BB (bb)
+    {
+      bitmap local = bb_local + bb->index;
+      rtx insn;
+
+      FOR_BB_INSNS (bb, insn)
+	if (NONDEBUG_INSN_P (insn))
+	  {
+	    rtx def_insn, closest_use, note;
+	    df_ref *def_rec, def, use;
+	    unsigned regno;
+	    bool all_dominated, all_local;
+	    enum machine_mode mode;
+
+	    def_rec = DF_INSN_DEFS (insn);
+	    /* There must be exactly one def in this insn.  */
+	    def = *def_rec;
+	    if (!def || def_rec[1] || !single_set (insn))
+	      continue;
+	    /* This must be the only definition of the reg.  We also limit
+	       which modes we deal with so that we can assume we can generate
+	       move instructions.  */
+	    regno = DF_REF_REGNO (def);
+	    mode = GET_MODE (DF_REF_REG (def));
+	    if (DF_REG_DEF_COUNT (regno) != 1
+		|| !DF_REF_INSN_INFO (def)
+		|| HARD_REGISTER_NUM_P (regno)
+		|| (!INTEGRAL_MODE_P (mode) && !FLOAT_MODE_P (mode)))
+	      continue;
+	    def_insn = DF_REF_INSN (def);
+
+	    for (note = REG_NOTES (def_insn); note; note = XEXP (note, 1))
+	      if (REG_NOTE_KIND (note) == REG_EQUIV && MEM_P (XEXP (note, 0)))
+		break;
+		
+	    if (note)
+	      {
+		if (dump_file)
+		  fprintf (dump_file, "Ignoring reg %d, has equiv memory\n",
+			   regno);
+		bitmap_set_bit (&unusable_as_input, regno);
+		continue;
+	      }
+
+	    use = DF_REG_USE_CHAIN (regno);
+	    all_dominated = true;
+	    all_local = true;
+	    closest_use = NULL_RTX;
+	    for (; use; use = DF_REF_NEXT_REG (use))
+	      {
+		rtx insn;
+		if (!DF_REF_INSN_INFO (use))
+		  {
+		    all_dominated = false;
+		    all_local = false;
+		    break;
+		  }
+		insn = DF_REF_INSN (use);
+		if (DEBUG_INSN_P (insn))
+		  continue;
+		if (BLOCK_FOR_INSN (insn) != BLOCK_FOR_INSN (def_insn))
+		  all_local = false;
+		if (!insn_dominated_by_p (insn, def_insn, uid_luid))
+		  all_dominated = false;
+		if (closest_use != insn && closest_use != const0_rtx)
+		  {
+		    if (closest_use == NULL_RTX)
+		      closest_use = insn;
+		    else if (insn_dominated_by_p (closest_use, insn, uid_luid))
+		      closest_use = insn;
+		    else if (!insn_dominated_by_p (insn, closest_use, uid_luid))
+		      closest_use = const0_rtx;
+		  }
+	      }
+	    if (!all_dominated)
+	      {
+		if (dump_file)
+		  fprintf (dump_file, "Reg %d not all uses dominated by set\n",
+			   regno);
+		continue;
+	      }
+	    if (all_local)
+	      bitmap_set_bit (local, regno);
+	    if (closest_use == const0_rtx || closest_use == NULL
+		|| next_nonnote_nondebug_insn (def_insn) == closest_use)
+	      {
+		if (dump_file)
+		  fprintf (dump_file, "Reg %d uninteresting%s\n", regno,
+			   closest_use == const0_rtx || closest_use == NULL
+			   ? " (no unique first use)" : "");
+		continue;
+	      }
+#ifdef HAVE_cc0
+	    if (reg_referenced_p (cc0_rtx, PATTERN (closest_use)))
+	      {
+		if (dump_file)
+		  fprintf (dump_file, "Reg %d: closest user uses cc0\n",
+			   regno);
+		continue;
+	      }
+#endif
+	    bitmap_set_bit (&interesting, regno);
+	    closest_uses[regno] = closest_use;
+
+	    if (dump_file && (all_local || all_dominated))
+	      {
+		fprintf (dump_file, "Reg %u:", regno);
+		if (all_local)
+		  fprintf (dump_file, " local to bb %d", bb->index);
+		if (all_dominated)
+		  fprintf (dump_file, " def dominates all uses");
+		if (closest_use != const0_rtx)
+		  fprintf (dump_file, " has unique first use");
+		fputs ("\n", dump_file);
+	      }
+	  }
+    }
+
+  EXECUTE_IF_SET_IN_BITMAP (&interesting, 0, i, bi)
+    {
+      df_ref def = DF_REG_DEF_CHAIN (i);
+      rtx def_insn = DF_REF_INSN (def);
+      basic_block def_block = BLOCK_FOR_INSN (def_insn);
+      bitmap def_bb_local = bb_local + def_block->index;
+      bitmap def_bb_moveable = bb_moveable_reg_sets + def_block->index;
+      bitmap def_bb_transp = bb_transp_live + def_block->index;
+      bool local_to_bb_p = bitmap_bit_p (def_bb_local, i);
+      rtx use_insn = closest_uses[i];
+      df_ref *def_insn_use_rec = DF_INSN_USES (def_insn);
+      bool all_ok = true;
+      bool all_transp = true;
+
+      if (!REG_P (DF_REF_REG (def)))
+	continue;
+
+      if (!local_to_bb_p)
+	{
+	  if (dump_file)
+	    fprintf (dump_file, "Reg %u not local to one basic block\n",
+		     i);
+	  continue;
+	}
+      if (reg_equiv_init[i] != NULL_RTX)
+	{
+	  if (dump_file)
+	    fprintf (dump_file, "Ignoring reg %u with equiv init insn\n",
+		     i);
+	  continue;
+	}
+      if (!rtx_moveable_p (&PATTERN (def_insn), OP_IN))
+	{
+	  if (dump_file)
+	    fprintf (dump_file, "Found def insn %d for %d to be not moveable\n",
+		     INSN_UID (def_insn), i);
+	  continue;
+	}
+      if (dump_file)
+	fprintf (dump_file, "Examining insn %d, def for %d\n",
+		 INSN_UID (def_insn), i);
+      while (*def_insn_use_rec != NULL)
+	{
+	  df_ref use = *def_insn_use_rec;
+	  unsigned regno = DF_REF_REGNO (use);
+	  if (bitmap_bit_p (&unusable_as_input, regno))
+	    {
+	      all_ok = false;
+	      if (dump_file)
+		fprintf (dump_file, "  found unusable input reg %u.\n", regno);
+	      break;
+	    }
+	  if (!bitmap_bit_p (def_bb_transp, regno))
+	    {
+	      if (bitmap_bit_p (def_bb_moveable, regno)
+		  && !control_flow_insn_p (use_insn)
+#ifdef HAVE_cc0
+		  && !sets_cc0_p (use_insn)
+#endif
+		  )
+		{
+		  if (modified_between_p (DF_REF_REG (use), def_insn, use_insn))
+		    {
+		      rtx x = NEXT_INSN (def_insn);
+		      while (!modified_in_p (DF_REF_REG (use), x))
+			{
+			  gcc_assert (x != use_insn);
+			  x = NEXT_INSN (x);
+			}
+		      if (dump_file)
+			fprintf (dump_file, "  input reg %u modified but insn %d moveable\n",
+				 regno, INSN_UID (x));
+		      emit_insn_after (PATTERN (x), use_insn);
+		      set_insn_deleted (x);
+		    }
+		  else
+		    {
+		      if (dump_file)
+			fprintf (dump_file, "  input reg %u modified between def and use\n",
+				 regno);
+		      all_transp = false;
+		    }
+		}
+	      else
+		all_transp = false;
+	    }
+
+	  def_insn_use_rec++;
+	}
+      if (!all_ok)
+	continue;
+      if (!dbg_cnt (ira_move))
+	break;
+      if (dump_file)
+	fprintf (dump_file, "  all ok%s\n", all_transp ? " and transp" : "");
+
+      if (all_transp)
+	{
+	  rtx def_reg = DF_REF_REG (def);
+	  rtx newreg = ira_create_new_reg (def_reg);
+	  if (validate_change (def_insn, DF_REF_LOC (def), newreg, 0))
+	    {
+	      unsigned nregno = REGNO (newreg);
+	      rtx move = emit_insn_before (gen_move_insn (def_reg, newreg),
+					   use_insn);
+	      nregno -= max_regs;
+	      VEC_replace (rtx, pseudo_move_insn, nregno, move);
+	      VEC_replace (rtx, pseudo_replaced_reg, nregno, def_reg);
+	    }
+	}
+    }
+  
+  FOR_EACH_BB (bb)
+    {
+      bitmap_clear (bb_local + bb->index);
+      bitmap_clear (bb_transp_live + bb->index);
+      bitmap_clear (bb_moveable_reg_sets + bb->index);
+    }
+  bitmap_clear (&interesting);
+  bitmap_clear (&unusable_as_input);
+  free (uid_luid);
+  free (closest_uses);
+  free (bb_local);
+  free (bb_transp_live);
+  free (bb_moveable_reg_sets);
+
+  last_moveable_pseudo = max_reg_num ();
+
+  fix_reg_equiv_init();
+  regstat_free_n_sets_and_refs ();
+  regstat_free_ri ();
+  regstat_init_n_sets_and_refs ();
+  regstat_compute_ri ();
+}
+
+/* Perform the second half of the transformation started in
+   find_moveable_pseudos.  We look for instances where the newly introduced
+   pseudo remains unallocated, and remove it by moving the definition to
+   just before its use, replacing the move instruction generated by
+   find_moveable_pseudos.  */
+static void
+move_unallocated_pseudos (void)
+{
+  int i;
+  for (i = first_moveable_pseudo; i < last_moveable_pseudo; i++)
+    if (reg_renumber[i] < 0)
+      {
+	df_ref def = DF_REG_DEF_CHAIN (i);
+	int idx = i - first_moveable_pseudo;
+	rtx other_reg = VEC_index (rtx, pseudo_replaced_reg, idx);
+	rtx def_insn = DF_REF_INSN (def);
+	rtx move_insn = VEC_index (rtx, pseudo_move_insn, idx);
+	rtx set;
+	rtx newinsn = emit_insn_after (PATTERN (def_insn), move_insn);
+	int success;
+
+	if (dump_file)
+	  fprintf (dump_file, "moving def of %d (insn %d now) ",
+		   REGNO (other_reg), INSN_UID (def_insn));
+
+	set = single_set (newinsn);
+	success = validate_change (newinsn, &SET_DEST (set), other_reg, 0);
+	gcc_assert (success);
+	if (dump_file)
+	  fprintf (dump_file, " %d) rather than keep unallocated replacement %d\n",
+		   INSN_UID (newinsn), i);
+	delete_insn (move_insn);
+	delete_insn (def_insn);
+	SET_REG_N_REFS (i, 0);
+      }
+}
+
 /* Allocate memory for reg_equiv_memory_loc.  */
 static void
 init_reg_equiv_memory_loc (void)
@@ -3064,6 +3578,89 @@ init_reg_equiv_memory_loc (void)
   reg_equiv_memory_loc = VEC_address (rtx, reg_equiv_memory_loc_vec);
 }
 
+/* Look for CALL_INSNS with a SET in their CALL_INSN_FUNCTION_USAGE.  Such
+   a SET indicates that a function returns one of its arguments.  If we
+   find such a case, we can look backwards from the call to see if the
+   argument hard register is set from a pseudo, and make that pseudo not
+   live across the call by inserting a copy from the return register after
+   the call.  If the pseudo is still allocated a call-saved register,
+   copy propagation will later eliminate the unnecessary copy.  */
+
+static void
+create_sets_for_returned_args (void)
+{
+  basic_block bb;
+  bitmap_head regs_live;
+
+  bitmap_initialize (&regs_live, 0);
+
+  FOR_EACH_BB (bb)
+    {
+      rtx insn;
+      bitmap_copy (&regs_live, DF_LR_OUT (bb));
+      df_simulate_initialize_backwards (bb, &regs_live);
+      FOR_BB_INSNS_REVERSE (bb, insn)
+	{
+	  if (CALL_P (insn))
+	    {
+	      rtx exp = CALL_INSN_FUNCTION_USAGE (insn);
+	      while (exp != NULL)
+		{
+		  rtx x = XEXP (exp, 0);
+		  if (GET_CODE (x) == SET)
+		    {
+		      exp = x;
+		      break;
+		    }
+		  exp = XEXP (exp, 1);
+		}
+	      if (exp != NULL)
+		{
+		  rtx reg = SET_SRC (exp);
+		  rtx exp_dest = SET_DEST (exp);
+		  rtx prev = PREV_INSN (insn);
+		  while (prev && !(INSN_P (prev)
+				   && BLOCK_FOR_INSN (prev) != bb))
+		    {
+		      if (NONDEBUG_INSN_P (prev))
+			{
+			  rtx set = single_set (prev);
+			  if (set && rtx_equal_p (SET_DEST (set), reg))
+			    {
+			      rtx src = SET_SRC (set);
+			      if (!REG_P (src) || HARD_REGISTER_P (src)
+				  || !bitmap_bit_p (&regs_live, REGNO (src)))
+				break;
+			      if (modified_between_p (src, prev, insn))
+				break;
+			      emit_insn_after (gen_move_insn (src, exp_dest),
+					       insn);
+			      break;
+			    }
+			  if (set && rtx_equal_p (SET_SRC (set), reg))
+			    {
+			      rtx dest = SET_DEST (set);
+			      if (!REG_P (dest) || HARD_REGISTER_P (dest)
+				  || !bitmap_bit_p (&regs_live, REGNO (dest)))
+				break;
+			      if (modified_between_p (dest, prev, insn))
+				break;
+			      emit_insn_after (gen_move_insn (dest, exp_dest),
+					       insn);
+			      break;
+			    }
+			  if (reg_overlap_mentioned_p (reg, PATTERN (prev)))
+			    break;
+			}
+		      prev = PREV_INSN (prev);
+		    }
+		}
+	    }
+	  df_simulate_one_insn_backwards (bb, insn, &regs_live);
+	}
+    }
+}
+
 /* All natural loops.  */
 struct loops ira_loops;
 
@@ -3113,6 +3710,9 @@ ira (FILE *f)
 #endif
   df_analyze ();
   df_clear_flags (DF_NO_INSN_RESCAN);
+
+  create_sets_for_returned_args ();
+
   regstat_init_n_sets_and_refs ();
   regstat_compute_ri ();
 
@@ -3156,6 +3756,8 @@ ira (FILE *f)
 	}
     }
 
+  find_moveable_pseudos ();
+
   max_regno_before_ira = allocated_reg_info_size = max_reg_num ();
   ira_setup_eliminable_regset ();
 
@@ -3265,6 +3867,7 @@ ira (FILE *f)
 	      max_regno * sizeof (struct ira_spilled_reg_stack_slot));
     }
 
+  move_unallocated_pseudos ();
   timevar_pop (TV_IRA);
 
   timevar_push (TV_RELOAD);
diff --git a/gcc/java/Make-lang.in b/gcc/java/Make-lang.in
index f5852b4..4c33338 100644
--- a/gcc/java/Make-lang.in
+++ b/gcc/java/Make-lang.in
@@ -333,7 +333,7 @@ java/jcf-io.o: java/jcf-io.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \
 java/jcf-path.o: java/jcf-path.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \
   java/jcf.h
 	$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \
-	  -DLIBGCJ_ZIP_FILE='"$(datadir)/java/libgcj-$(version).jar"' \
+	  -DLIBGCJ_ZIP_FILE='"$(jardir)/java/libgcj-$(version).jar"' \
 	  -DDEFAULT_TARGET_VERSION=\"$(version)\" \
 	  $(srcdir)/java/jcf-path.c $(OUTPUT_OPTION)
 
diff --git a/gcc/jump.c b/gcc/jump.c
index 9721fe1..b722946 100644
--- a/gcc/jump.c
+++ b/gcc/jump.c
@@ -29,7 +29,8 @@ along with GCC; see the file COPYING3.  If not see
    JUMP_LABEL internal field.  With this we can detect labels that
    become unused because of the deletion of all the jumps that
    formerly used them.  The JUMP_LABEL info is sometimes looked
-   at by later passes.
+   at by later passes.  For return insns, it contains either a
+   RETURN or a SIMPLE_RETURN rtx.
 
    The subroutines redirect_jump and invert_jump are used
    from other passes as well.  */
@@ -741,10 +742,10 @@ condjump_p (const_rtx insn)
     return (GET_CODE (x) == IF_THEN_ELSE
 	    && ((GET_CODE (XEXP (x, 2)) == PC
 		 && (GET_CODE (XEXP (x, 1)) == LABEL_REF
-		     || GET_CODE (XEXP (x, 1)) == RETURN))
+		     || ANY_RETURN_P (XEXP (x, 1))))
 		|| (GET_CODE (XEXP (x, 1)) == PC
 		    && (GET_CODE (XEXP (x, 2)) == LABEL_REF
-			|| GET_CODE (XEXP (x, 2)) == RETURN))));
+			|| ANY_RETURN_P (XEXP (x, 2))))));
 }
 
 /* Return nonzero if INSN is a (possibly) conditional jump inside a
@@ -773,11 +774,11 @@ condjump_in_parallel_p (const_rtx insn)
     return 0;
   if (XEXP (SET_SRC (x), 2) == pc_rtx
       && (GET_CODE (XEXP (SET_SRC (x), 1)) == LABEL_REF
-	  || GET_CODE (XEXP (SET_SRC (x), 1)) == RETURN))
+	  || ANY_RETURN_P (XEXP (SET_SRC (x), 1)) == RETURN))
     return 1;
   if (XEXP (SET_SRC (x), 1) == pc_rtx
       && (GET_CODE (XEXP (SET_SRC (x), 2)) == LABEL_REF
-	  || GET_CODE (XEXP (SET_SRC (x), 2)) == RETURN))
+	  || ANY_RETURN_P (XEXP (SET_SRC (x), 2))))
     return 1;
   return 0;
 }
@@ -839,8 +840,9 @@ any_condjump_p (const_rtx insn)
   a = GET_CODE (XEXP (SET_SRC (x), 1));
   b = GET_CODE (XEXP (SET_SRC (x), 2));
 
-  return ((b == PC && (a == LABEL_REF || a == RETURN))
-	  || (a == PC && (b == LABEL_REF || b == RETURN)));
+  return ((b == PC && (a == LABEL_REF || a == RETURN || a == SIMPLE_RETURN))
+	  || (a == PC
+	      && (b == LABEL_REF || b == RETURN || b == SIMPLE_RETURN)));
 }
 
 /* Return the label of a conditional jump.  */
@@ -877,6 +879,7 @@ returnjump_p_1 (rtx *loc, void *data ATTRIBUTE_UNUSED)
   switch (GET_CODE (x))
     {
     case RETURN:
+    case SIMPLE_RETURN:
     case EH_RETURN:
       return true;
 
@@ -1199,7 +1202,7 @@ delete_related_insns (rtx insn)
   /* If deleting a jump, decrement the count of the label,
      and delete the label if it is now unused.  */
 
-  if (JUMP_P (insn) && JUMP_LABEL (insn))
+  if (JUMP_P (insn) && JUMP_LABEL (insn) && !ANY_RETURN_P (JUMP_LABEL (insn)))
     {
       rtx lab = JUMP_LABEL (insn), lab_next;
 
@@ -1330,6 +1333,18 @@ delete_for_peephole (rtx from, rtx to)
      is also an unconditional jump in that case.  */
 }
 
+/* A helper function for redirect_exp_1; examines its input X and returns
+   either a LABEL_REF around a label, or a RETURN if X was NULL.  */
+static rtx
+redirect_target (rtx x)
+{
+  if (x == NULL_RTX)
+    return ret_rtx;
+  if (!ANY_RETURN_P (x))
+    return gen_rtx_LABEL_REF (Pmode, x);
+  return x;
+}
+
 /* Throughout LOC, redirect OLABEL to NLABEL.  Treat null OLABEL or
    NLABEL as a return.  Accrue modifications into the change group.  */
 
@@ -1341,37 +1356,19 @@ redirect_exp_1 (rtx *loc, rtx olabel, rtx nlabel, rtx insn)
   int i;
   const char *fmt;
 
-  if (code == LABEL_REF)
+  if ((code == LABEL_REF && XEXP (x, 0) == olabel)
+      || x == olabel)
     {
-      if (XEXP (x, 0) == olabel)
-	{
-	  rtx n;
-	  if (nlabel)
-	    n = gen_rtx_LABEL_REF (Pmode, nlabel);
-	  else
-	    n = gen_rtx_RETURN (VOIDmode);
-
-	  validate_change (insn, loc, n, 1);
-	  return;
-	}
-    }
-  else if (code == RETURN && olabel == 0)
-    {
-      if (nlabel)
-	x = gen_rtx_LABEL_REF (Pmode, nlabel);
-      else
-	x = gen_rtx_RETURN (VOIDmode);
-      if (loc == &PATTERN (insn))
-	x = gen_rtx_SET (VOIDmode, pc_rtx, x);
-      validate_change (insn, loc, x, 1);
+      validate_change (insn, loc, redirect_target (nlabel), 1);
       return;
     }
 
-  if (code == SET && nlabel == 0 && SET_DEST (x) == pc_rtx
+  if (code == SET && SET_DEST (x) == pc_rtx
+      && ANY_RETURN_P (nlabel)
       && GET_CODE (SET_SRC (x)) == LABEL_REF
       && XEXP (SET_SRC (x), 0) == olabel)
     {
-      validate_change (insn, loc, gen_rtx_RETURN (VOIDmode), 1);
+      validate_change (insn, loc, nlabel, 1);
       return;
     }
 
@@ -1408,6 +1405,7 @@ redirect_jump_1 (rtx jump, rtx nlabel)
   int ochanges = num_validated_changes ();
   rtx *loc, asmop;
 
+  gcc_assert (nlabel);
   asmop = extract_asm_operands (PATTERN (jump));
   if (asmop)
     {
@@ -1429,17 +1427,20 @@ redirect_jump_1 (rtx jump, rtx nlabel)
    jump target label is unused as a result, it and the code following
    it may be deleted.
 
-   If NLABEL is zero, we are to turn the jump into a (possibly conditional)
-   RETURN insn.
+   Normally, NLABEL will be a label, but it may also be a RETURN or
+   SIMPLE_RETURN rtx; in that case we are to turn the jump into a
+   (possibly conditional) return insn.
 
    The return value will be 1 if the change was made, 0 if it wasn't
-   (this can only occur for NLABEL == 0).  */
+   (this can only occur when trying to produce return insns).  */
 
 int
 redirect_jump (rtx jump, rtx nlabel, int delete_unused)
 {
   rtx olabel = JUMP_LABEL (jump);
 
+  gcc_assert (nlabel != NULL_RTX);
+
   if (nlabel == olabel)
     return 1;
 
@@ -1451,7 +1452,7 @@ redirect_jump (rtx jump, rtx nlabel, int delete_unused)
 }
 
 /* Fix up JUMP_LABEL and label ref counts after OLABEL has been replaced with
-   NLABEL in JUMP.
+   NEW_DEST in JUMP.
    If DELETE_UNUSED is positive, delete related insn to OLABEL if its ref
    count has dropped to zero.  */
 void
@@ -1467,13 +1468,14 @@ redirect_jump_2 (rtx jump, rtx olabel, rtx nlabel, int delete_unused,
      about this.  */
   gcc_assert (delete_unused >= 0);
   JUMP_LABEL (jump) = nlabel;
-  if (nlabel)
+  if (nlabel && !ANY_RETURN_P (nlabel))
     ++LABEL_NUSES (nlabel);
 
   /* Update labels in any REG_EQUAL note.  */
   if ((note = find_reg_note (jump, REG_EQUAL, NULL_RTX)) != NULL_RTX)
     {
-      if (!nlabel || (invert && !invert_exp_1 (XEXP (note, 0), jump)))
+      if (ANY_RETURN_P (nlabel)
+	  || (invert && !invert_exp_1 (XEXP (note, 0), jump)))
 	remove_note (jump, note);
       else
 	{
@@ -1482,7 +1484,8 @@ redirect_jump_2 (rtx jump, rtx olabel, rtx nlabel, int delete_unused,
 	}
     }
 
-  if (olabel && --LABEL_NUSES (olabel) == 0 && delete_unused > 0
+  if (olabel && !ANY_RETURN_P (olabel)
+      && --LABEL_NUSES (olabel) == 0 && delete_unused > 0
       /* Undefined labels will remain outside the insn stream.  */
       && INSN_UID (olabel))
     delete_related_insns (olabel);
diff --git a/gcc/libfuncs.h b/gcc/libfuncs.h
index b01aed3..9ff17aa 100644
--- a/gcc/libfuncs.h
+++ b/gcc/libfuncs.h
@@ -40,6 +40,10 @@ enum libfunc_index
   LTI_profile_function_entry,
   LTI_profile_function_exit,
 
+  LTI_profile_call_entry,
+  LTI_profile_call_inside,
+  LTI_profile_call_exit,
+
   LTI_synchronize,
 
   LTI_gcov_flush,
@@ -94,6 +98,10 @@ extern struct target_libfuncs *this_target_libfuncs;
 #define profile_function_entry_libfunc	(libfunc_table[LTI_profile_function_entry])
 #define profile_function_exit_libfunc	(libfunc_table[LTI_profile_function_exit])
 
+#define profile_call_entry_libfunc	(libfunc_table[LTI_profile_call_entry])
+#define profile_call_inside_libfunc	(libfunc_table[LTI_profile_call_inside])
+#define profile_call_exit_libfunc	(libfunc_table[LTI_profile_call_exit])
+
 #define synchronize_libfunc	(libfunc_table[LTI_synchronize])
 
 #define gcov_flush_libfunc	(libfunc_table[LTI_gcov_flush])
diff --git a/gcc/libgcc2.h b/gcc/libgcc2.h
index 4e6fd23..bb4e6f3 100644
--- a/gcc/libgcc2.h
+++ b/gcc/libgcc2.h
@@ -193,8 +193,13 @@ typedef int shift_count_type __attribute__((mode (__libgcc_shift_count__)));
 #define UHWtype	UDItype
 #define DWtype	TItype
 #define UDWtype	UTItype
+#ifdef LIBGCC2_GNU_PREFIX
+#define __NW(a,b)	__gnu_ ## a ## di ## b
+#define __NDW(a,b)	__gnu_ ## a ## ti ## b
+#else
 #define __NW(a,b)	__ ## a ## di ## b
 #define __NDW(a,b)	__ ## a ## ti ## b
+#endif
 #define COMPAT_SIMODE_TRAPPING_ARITHMETIC
 #elif LIBGCC2_UNITS_PER_WORD == 4
 #define W_TYPE_SIZE (4 * BITS_PER_UNIT)
@@ -204,8 +209,13 @@ typedef int shift_count_type __attribute__((mode (__libgcc_shift_count__)));
 #define UHWtype	USItype
 #define DWtype	DItype
 #define UDWtype	UDItype
+#ifdef LIBGCC2_GNU_PREFIX
+#define __NW(a,b)	__gnu_ ## a ## si ## b
+#define __NDW(a,b)	__gnu_ ## a ## di ## b
+#else
 #define __NW(a,b)	__ ## a ## si ## b
 #define __NDW(a,b)	__ ## a ## di ## b
+#endif
 #elif LIBGCC2_UNITS_PER_WORD == 2
 #define W_TYPE_SIZE (2 * BITS_PER_UNIT)
 #define Wtype	HItype
@@ -214,8 +224,13 @@ typedef int shift_count_type __attribute__((mode (__libgcc_shift_count__)));
 #define UHWtype	UHItype
 #define DWtype	SItype
 #define UDWtype	USItype
+#ifdef LIBGCC2_GNU_PREFIX
+#define __NW(a,b)	__gnu_ ## a ## hi ## b
+#define __NDW(a,b)	__gnu_ ## a ## si ## b
+#else
 #define __NW(a,b)	__ ## a ## hi ## b
 #define __NDW(a,b)	__ ## a ## si ## b
+#endif
 #else
 #define W_TYPE_SIZE BITS_PER_UNIT
 #define Wtype	QItype
@@ -224,10 +239,20 @@ typedef int shift_count_type __attribute__((mode (__libgcc_shift_count__)));
 #define UHWtype	UQItype
 #define DWtype	HItype
 #define UDWtype	UHItype
+#ifdef LIBGCC2_GNU_PREFIX
+#define __NW(a,b)	__gnu_ ## a ## qi ## b
+#define __NDW(a,b)	__gnu_ ## a ## hi ## b
+#else
 #define __NW(a,b)	__ ## a ## qi ## b
 #define __NDW(a,b)	__ ## a ## hi ## b
 #endif
+#endif
 
+#ifdef LIBGCC2_GNU_PREFIX
+#define __N(a)	__gnu_ ## a
+#else
+#define __N(a)	__ ## a
+#endif
 #define Wtype_MAX ((Wtype)(((UWtype)1 << (W_TYPE_SIZE - 1)) - 1))
 #define Wtype_MIN (- Wtype_MAX - 1)
 
@@ -298,6 +323,50 @@ typedef int shift_count_type __attribute__((mode (__libgcc_shift_count__)));
 #define __popcountDI2	__NDW(popcount,2)
 #define __parityDI2	__NDW(parity,2)
 
+#define __clz_tab		__N(clz_tab)
+#define __bswapsi2		__N(bswapsi2)
+#define __bswapdi2		__N(bswapdi2)
+#define __udiv_w_sdiv		__N(udiv_w_sdiv)
+#define __clear_cache		__N(clear_cache)
+#define __enable_execute_stack	__N(enable_execute_stack)
+
+#ifndef __powisf2
+#define __powisf2		__N(powisf2)
+#endif
+#ifndef __powidf2
+#define __powidf2		__N(powidf2)
+#endif
+#ifndef __powitf2
+#define __powitf2		__N(powitf2)
+#endif
+#ifndef __powixf2
+#define __powixf2		__N(powixf2)
+#endif
+#ifndef __mulsc3
+#define __mulsc3		__N(mulsc3)
+#endif
+#ifndef __muldc3
+#define __muldc3		__N(muldc3)
+#endif
+#ifndef __mulxc3
+#define __mulxc3		__N(mulxc3)
+#endif
+#ifndef __multc3
+#define __multc3		__N(multc3)
+#endif
+#ifndef __divsc3
+#define __divsc3		__N(divsc3)
+#endif
+#ifndef __divdc3
+#define __divdc3		__N(divdc3)
+#endif
+#ifndef __divxc3
+#define __divxc3		__N(divxc3)
+#endif
+#ifndef __divtc3
+#define __divtc3		__N(divtc3)
+#endif
+
 extern DWtype __muldi3 (DWtype, DWtype);
 extern DWtype __divdi3 (DWtype, DWtype);
 extern UDWtype __udivdi3 (UDWtype, UDWtype);
@@ -347,6 +416,12 @@ extern DWtype __mulvDI3 (DWtype, DWtype);
 extern DWtype __negvDI2 (DWtype);
 
 #ifdef COMPAT_SIMODE_TRAPPING_ARITHMETIC
+#define __absvsi2	__N(absvsi2)
+#define __negvsi2	__N(negvsi2)
+#define __addvsi3	__N(addvsi3)
+#define __subvsi3	__N(subvsi3)
+#define __mulvsi3	__N(mulvsi3)
+
 extern SItype __absvsi2 (SItype);
 extern SItype __addvsi3 (SItype, SItype);
 extern SItype __subvsi3 (SItype, SItype);
diff --git a/gcc/lists.c b/gcc/lists.c
index 4628e68..a962d3e 100644
--- a/gcc/lists.c
+++ b/gcc/lists.c
@@ -164,6 +164,37 @@ free_INSN_LIST_list (rtx *listp)
   free_list (listp, &unused_insn_list);
 }
 
+/* Make a copy of the INSN_LIST list LINK and return it.  */
+rtx
+copy_INSN_LIST (rtx link)
+{
+  rtx new_queue;
+  rtx *pqueue = &new_queue;
+
+  for (; link; link = XEXP (link, 1))
+    {
+      rtx x = XEXP (link, 0);
+      rtx newlink = alloc_INSN_LIST (x, NULL);
+      *pqueue = newlink;
+      pqueue = &XEXP (newlink, 1);
+    }
+  *pqueue = NULL_RTX;
+  return new_queue;
+}
+
+/* Duplicate the INSN_LIST elements of COPY and prepend them to OLD.  */
+rtx
+concat_INSN_LIST (rtx copy, rtx old)
+{
+  rtx new_rtx = old;
+  for (; copy ; copy = XEXP (copy, 1))
+    {
+      new_rtx = alloc_INSN_LIST (XEXP (copy, 0), new_rtx);
+      PUT_REG_NOTE_KIND (new_rtx, REG_NOTE_KIND (copy));
+    }
+  return new_rtx;
+}
+
 /* This function will free up an individual EXPR_LIST node.  */
 void
 free_EXPR_LIST_node (rtx ptr)
diff --git a/gcc/lto/lto-lang.c b/gcc/lto/lto-lang.c
index 5f157d6..aa49dac 100644
--- a/gcc/lto/lto-lang.c
+++ b/gcc/lto/lto-lang.c
@@ -47,6 +47,7 @@ static tree handle_nothrow_attribute (tree *, tree, tree, int, bool *);
 static tree handle_sentinel_attribute (tree *, tree, tree, int, bool *);
 static tree handle_type_generic_attribute (tree *, tree, tree, int, bool *);
 static tree handle_format_attribute (tree *, tree, tree, int, bool *);
+static tree handle_fnspec_attribute (tree *, tree, tree, int, bool *);
 static tree handle_format_arg_attribute (tree *, tree, tree, int, bool *);
 
 /* Table of machine-independent attributes supported in GIMPLE.  */
@@ -74,6 +75,8 @@ const struct attribute_spec lto_attribute_table[] =
 			      handle_sentinel_attribute },
   { "type generic",           0, 0, false, true, true,
 			      handle_type_generic_attribute },
+  { "fn spec",	 	      1, 1, false, true, true,
+			      handle_fnspec_attribute },
   { NULL,                     0, 0, false, false, false, NULL }
 };
 
@@ -94,11 +97,13 @@ enum built_in_attribute
 {
 #define DEF_ATTR_NULL_TREE(ENUM) ENUM,
 #define DEF_ATTR_INT(ENUM, VALUE) ENUM,
+#define DEF_ATTR_STRING(ENUM, VALUE) ENUM,
 #define DEF_ATTR_IDENT(ENUM, STRING) ENUM,
 #define DEF_ATTR_TREE_LIST(ENUM, PURPOSE, VALUE, CHAIN) ENUM,
 #include "builtin-attrs.def"
 #undef DEF_ATTR_NULL_TREE
 #undef DEF_ATTR_INT
+#undef DEF_ATTR_STRING
 #undef DEF_ATTR_IDENT
 #undef DEF_ATTR_TREE_LIST
   ATTR_LAST
@@ -438,6 +443,20 @@ handle_format_arg_attribute (tree * ARG_UNUSED (node), tree ARG_UNUSED (name),
 }
 
 
+/* Handle a "fn spec" attribute; arguments as in
+   struct attribute_spec.handler.  */
+
+static tree
+handle_fnspec_attribute (tree *node ATTRIBUTE_UNUSED, tree ARG_UNUSED (name),
+			 tree args, int ARG_UNUSED (flags),
+			 bool *no_add_attrs ATTRIBUTE_UNUSED)
+{
+  gcc_assert (args
+	      && TREE_CODE (TREE_VALUE (args)) == STRING_CST
+	      && !TREE_CHAIN (args));
+  return NULL_TREE;
+}
+
 /* Cribbed from c-common.c.  */
 
 static void
@@ -524,6 +543,8 @@ lto_init_attributes (void)
   built_in_attributes[(int) ENUM] = NULL_TREE;
 #define DEF_ATTR_INT(ENUM, VALUE)				\
   built_in_attributes[(int) ENUM] = build_int_cst (NULL_TREE, VALUE);
+#define DEF_ATTR_STRING(ENUM, VALUE)				\
+  built_in_attributes[(int) ENUM] = build_string (strlen (VALUE), VALUE);
 #define DEF_ATTR_IDENT(ENUM, STRING)				\
   built_in_attributes[(int) ENUM] = get_identifier (STRING);
 #define DEF_ATTR_TREE_LIST(ENUM, PURPOSE, VALUE, CHAIN)	\
diff --git a/gcc/mode-switching.c b/gcc/mode-switching.c
index d4c7b24..09a6ebc 100644
--- a/gcc/mode-switching.c
+++ b/gcc/mode-switching.c
@@ -33,6 +33,7 @@ along with GCC; see the file COPYING3.  If not see
 #include "output.h"
 #include "tm_p.h"
 #include "function.h"
+#include "integrate.h"
 #include "tree-pass.h"
 #include "timevar.h"
 #include "df.h"
@@ -751,6 +752,7 @@ rest_of_handle_mode_switching (void)
 {
 #ifdef OPTIMIZE_MODE_SWITCHING
   optimize_mode_switching ();
+  emit_initial_value_sets ();
 #endif /* OPTIMIZE_MODE_SWITCHING */
   return 0;
 }
diff --git a/gcc/modulo-sched.c b/gcc/modulo-sched.c
index 2ae267c..2700c79 100644
--- a/gcc/modulo-sched.c
+++ b/gcc/modulo-sched.c
@@ -1,5 +1,5 @@
 /* Swing Modulo Scheduling implementation.
-   Copyright (C) 2004, 2005, 2006, 2007, 2008, 2009, 2010
+   Copyright (C) 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011
    Free Software Foundation, Inc.
    Contributed by Ayal Zaks and Mustafa Hagog <zaks,mustafa@il.ibm.com>
 
@@ -245,9 +245,7 @@ sms_print_insn (const_rtx insn, int aligned ATTRIBUTE_UNUSED)
 
 static void
 compute_jump_reg_dependencies (rtx insn ATTRIBUTE_UNUSED,
-			       regset cond_exec ATTRIBUTE_UNUSED,
-			       regset used ATTRIBUTE_UNUSED,
-			       regset set ATTRIBUTE_UNUSED)
+			       regset used ATTRIBUTE_UNUSED)
 {
 }
 
@@ -275,7 +273,8 @@ static struct haifa_sched_info sms_sched_info =
   NULL, NULL,
   0, 0,
 
-  NULL, NULL, NULL,
+  NULL, NULL, NULL, NULL,
+  NULL, NULL,
   0
 };
 
diff --git a/gcc/optabs.c b/gcc/optabs.c
index 6be8db0..3665ac1 100644
--- a/gcc/optabs.c
+++ b/gcc/optabs.c
@@ -4169,11 +4169,13 @@ prepare_cmp_insn (rtx x, rtx y, enum rtx_code comparison, rtx size,
 	 result against 1 in the biased case, and zero in the unbiased
 	 case. For unsigned comparisons always compare against 1 after
 	 biasing the unbiased result by adding 1. This gives us a way to
-	 represent LTU. */
+	 represent LTU.
+	 The comparisons in the fixed-point helper library are always
+	 biased.  */
       x = result;
       y = const1_rtx;
 
-      if (!TARGET_LIB_INT_CMP_BIASED)
+      if (!TARGET_LIB_INT_CMP_BIASED && !ALL_FIXED_POINT_MODE_P (mode))
 	{
 	  if (unsignedp)
 	    x = plus_constant (result, 1);
@@ -5454,13 +5456,22 @@ gen_libfunc (optab optable, const char *opname, int suffix, enum machine_mode mo
   unsigned opname_len = strlen (opname);
   const char *mname = GET_MODE_NAME (mode);
   unsigned mname_len = strlen (mname);
-  char *libfunc_name = XALLOCAVEC (char, 2 + opname_len + mname_len + 1 + 1);
+  int prefix_len = targetm.libfunc_gnu_prefix ? 6 : 2;
+  int len = prefix_len + opname_len + mname_len + 1 + 1;
+  char *libfunc_name = XALLOCAVEC (char, len);
   char *p;
   const char *q;
 
   p = libfunc_name;
   *p++ = '_';
   *p++ = '_';
+  if (targetm.libfunc_gnu_prefix)
+    {
+      *p++ = 'g';
+      *p++ = 'n';
+      *p++ = 'u';
+      *p++ = '_';
+    }
   for (q = opname; *q; )
     *p++ = *q++;
   for (q = mname; *q; q++)
@@ -5664,6 +5675,7 @@ gen_interclass_conv_libfunc (convert_optab tab,
 
   const char *fname, *tname;
   const char *q;
+  int prefix_len = targetm.libfunc_gnu_prefix ? 6 : 2;
   char *libfunc_name, *suffix;
   char *nondec_name, *dec_name, *nondec_suffix, *dec_suffix;
   char *p;
@@ -5674,11 +5686,19 @@ gen_interclass_conv_libfunc (convert_optab tab,
 
   mname_len = strlen (GET_MODE_NAME (tmode)) + strlen (GET_MODE_NAME (fmode));
 
-  nondec_name = XALLOCAVEC (char, 2 + opname_len + mname_len + 1 + 1);
+  nondec_name = XALLOCAVEC (char, prefix_len + opname_len + mname_len + 1 + 1);
   nondec_name[0] = '_';
   nondec_name[1] = '_';
-  memcpy (&nondec_name[2], opname, opname_len);
-  nondec_suffix = nondec_name + opname_len + 2;
+  if (targetm.libfunc_gnu_prefix)
+    {
+      nondec_name[2] = 'g';
+      nondec_name[3] = 'n';
+      nondec_name[4] = 'u';
+      nondec_name[5] = '_';
+    }
+
+  memcpy (&nondec_name[prefix_len], opname, opname_len);
+  nondec_suffix = nondec_name + opname_len + prefix_len;
 
   dec_name = XALLOCAVEC (char, 2 + dec_len + opname_len + mname_len + 1 + 1);
   dec_name[0] = '_';
@@ -5789,6 +5809,7 @@ gen_intraclass_conv_libfunc (convert_optab tab, const char *opname,
 
   const char *fname, *tname;
   const char *q;
+  int prefix_len = targetm.libfunc_gnu_prefix ? 6 : 2;
   char *nondec_name, *dec_name, *nondec_suffix, *dec_suffix;
   char *libfunc_name, *suffix;
   char *p;
@@ -5802,8 +5823,15 @@ gen_intraclass_conv_libfunc (convert_optab tab, const char *opname,
   nondec_name = XALLOCAVEC (char, 2 + opname_len + mname_len + 1 + 1);
   nondec_name[0] = '_';
   nondec_name[1] = '_';
-  memcpy (&nondec_name[2], opname, opname_len);
-  nondec_suffix = nondec_name + opname_len + 2;
+  if (targetm.libfunc_gnu_prefix)
+    {
+      nondec_name[2] = 'g';
+      nondec_name[3] = 'n';
+      nondec_name[4] = 'u';
+      nondec_name[5] = '_';
+    }
+  memcpy (&nondec_name[prefix_len], opname, opname_len);
+  nondec_suffix = nondec_name + opname_len + prefix_len;
 
   dec_name = XALLOCAVEC (char, 2 + dec_len + opname_len + mname_len + 1 + 1);
   dec_name[0] = '_';
@@ -6533,8 +6561,16 @@ init_optabs (void)
 
   /* Explicitly initialize the bswap libfuncs since we need them to be
      valid for things other than word_mode.  */
-  set_optab_libfunc (bswap_optab, SImode, "__bswapsi2");
-  set_optab_libfunc (bswap_optab, DImode, "__bswapdi2");
+  if (targetm.libfunc_gnu_prefix)
+    {
+      set_optab_libfunc (bswap_optab, SImode, "__gnu_bswapsi2");
+      set_optab_libfunc (bswap_optab, DImode, "__gnu_bswapdi2");
+    }
+  else
+    {
+      set_optab_libfunc (bswap_optab, SImode, "__bswapsi2");
+      set_optab_libfunc (bswap_optab, DImode, "__bswapdi2");
+    }
 
   /* Use cabs for double complex abs, since systems generally have cabs.
      Don't define any libcall for float complex, so that cabs will be used.  */
@@ -6565,6 +6601,14 @@ init_optabs (void)
   profile_function_exit_libfunc
     = init_one_libfunc ("__cyg_profile_func_exit");
 
+  /* For call entry/exit instrumentation.  */
+  profile_call_entry_libfunc
+    = init_one_libfunc ("__cyg_profile_call_enter");
+  profile_call_inside_libfunc
+    = init_one_libfunc ("__cyg_profile_call_inside");
+  profile_call_exit_libfunc
+    = init_one_libfunc ("__cyg_profile_call_exit");
+
   gcov_flush_libfunc = init_one_libfunc ("__gcov_flush");
 
   /* Allow the target to add more libcalls or rename some, etc.  */
diff --git a/gcc/opts.c b/gcc/opts.c
index 9f93356..5960543 100644
--- a/gcc/opts.c
+++ b/gcc/opts.c
@@ -481,6 +481,7 @@ static const struct default_options default_options_table[] =
     { OPT_LEVELS_2_PLUS, OPT_fstrict_overflow, NULL, 1 },
     { OPT_LEVELS_2_PLUS, OPT_freorder_blocks, NULL, 1 },
     { OPT_LEVELS_2_PLUS, OPT_freorder_functions, NULL, 1 },
+    { OPT_LEVELS_2_PLUS, OPT_ftree_if_to_switch_conversion, NULL, 1 },
     { OPT_LEVELS_2_PLUS, OPT_ftree_vrp, NULL, 1 },
     { OPT_LEVELS_2_PLUS, OPT_ftree_builtin_call_dce, NULL, 1 },
     { OPT_LEVELS_2_PLUS, OPT_ftree_pre, NULL, 1 },
@@ -492,6 +493,7 @@ static const struct default_options default_options_table[] =
     { OPT_LEVELS_2_PLUS, OPT_falign_jumps, NULL, 1 },
     { OPT_LEVELS_2_PLUS, OPT_falign_labels, NULL, 1 },
     { OPT_LEVELS_2_PLUS, OPT_falign_functions, NULL, 1 },
+    { OPT_LEVELS_2_PLUS, OPT_fextension_elimination, NULL, 1 },
 
     /* -O3 optimizations.  */
     { OPT_LEVELS_3_PLUS, OPT_ftree_loop_distribute_patterns, NULL, 1 },
@@ -503,6 +505,7 @@ static const struct default_options default_options_table[] =
     { OPT_LEVELS_3_PLUS, OPT_fgcse_after_reload, NULL, 1 },
     { OPT_LEVELS_3_PLUS, OPT_ftree_vectorize, NULL, 1 },
     { OPT_LEVELS_3_PLUS, OPT_fipa_cp_clone, NULL, 1 },
+    { OPT_LEVELS_3_PLUS, OPT_ftree_pre_partial_partial, NULL, 1 },
 
     /* -Ofast adds optimizations to -O3.  */
     { OPT_LEVELS_FAST, OPT_ffast_math, NULL, 1 },
@@ -1696,6 +1699,10 @@ common_handle_option (struct gcc_options *opts,
       /* No-op. Used by the driver and passed to us because it starts with f.*/
       break;
 
+    case OPT_feglibc_:
+      /* This is a no-op at the moment.  */
+      break;
+
     default:
       /* If the flag was handled in a standard way, assume the lack of
 	 processing here is intentional.  */
diff --git a/gcc/params.def b/gcc/params.def
index e5a82e9..c29bda3 100644
--- a/gcc/params.def
+++ b/gcc/params.def
@@ -175,6 +175,13 @@ DEFPARAM(PARAM_MAX_PENDING_LIST_LENGTH,
 	 "The maximum length of scheduling's pending operations list",
 	 32, 0, 0)
 
+/* This parameter limits the number of backtracking attempts when using the
+   haifa scheduler for modulo scheduling.  */
+DEFPARAM(PARAM_MAX_MODULO_BACKTRACK_ATTEMPTS,
+	 "max-modulo-backtrack-attempts",
+	 "The maximum number of backtrack attempts the scheduler should make when modulo scheduling a loop",
+	 40, 0, 0)
+
 DEFPARAM(PARAM_LARGE_FUNCTION_INSNS,
 	 "large-function-insns",
 	 "The size of function body to be considered large",
@@ -604,6 +611,11 @@ DEFPARAM(PARAM_SCHED_SPEC_PROB_CUTOFF,
          "The minimal probability of speculation success (in percents), so that speculative insn will be scheduled.",
          40, 0, 100)
 
+DEFPARAM(PARAM_SCHED_STATE_EDGE_PROB_CUTOFF,
+         "sched-state-edge-prob-cutoff",
+         "The minimum probability an edge must have for the scheduler to save its state across it.",
+         10, 0, 100)
+
 DEFPARAM(PARAM_SELSCHED_MAX_LOOKAHEAD,
          "selsched-max-lookahead",
          "The maximum size of the lookahead window of selective scheduling",
@@ -883,6 +895,11 @@ DEFPARAM (CXX_MAX_NAMESPACES_FOR_DIAGNOSTIC_HELP,
 	  "name lookup fails",
 	  1000, 0, 0)
 
+DEFPARAM (PARAM_IF_TO_SWITCH_THRESHOLD,
+	  "if-to-switch-threshold",
+	  "Threshold for converting an if-chain into a switch",
+	  3, 0, 0)
+
 /*
 Local variables:
 mode:c
diff --git a/gcc/passes.c b/gcc/passes.c
index a33a6af..c07c2bf 100644
--- a/gcc/passes.c
+++ b/gcc/passes.c
@@ -767,6 +767,7 @@ init_optimization_passes (void)
 	  NEXT_PASS (pass_cd_dce);
 	  NEXT_PASS (pass_early_ipa_sra);
 	  NEXT_PASS (pass_tail_recursion);
+	  NEXT_PASS (pass_if_to_switch);
 	  NEXT_PASS (pass_convert_switch);
           NEXT_PASS (pass_cleanup_eh);
           NEXT_PASS (pass_profile);
@@ -822,6 +823,7 @@ init_optimization_passes (void)
       NEXT_PASS (pass_rename_ssa_copies);
       NEXT_PASS (pass_complete_unrolli);
       NEXT_PASS (pass_ccp);
+      NEXT_PASS (pass_promote_indices);
       NEXT_PASS (pass_forwprop);
       NEXT_PASS (pass_call_cdce);
       /* pass_build_alias is a dummy pass that ensures that we
@@ -829,10 +831,12 @@ init_optimization_passes (void)
 	 alias information also rewrites no longer addressed
 	 locals into SSA form if possible.  */
       NEXT_PASS (pass_build_alias);
+      NEXT_PASS (pass_remove_local_statics);
       NEXT_PASS (pass_return_slot);
       NEXT_PASS (pass_phiprop);
       NEXT_PASS (pass_fre);
       NEXT_PASS (pass_copy_prop);
+      NEXT_PASS (pass_if_to_switch);
       NEXT_PASS (pass_merge_phi);
       NEXT_PASS (pass_vrp);
       NEXT_PASS (pass_dce);
@@ -996,6 +1000,7 @@ init_optimization_passes (void)
       NEXT_PASS (pass_initialize_regs);
       NEXT_PASS (pass_ud_rtl_dce);
       NEXT_PASS (pass_combine);
+      NEXT_PASS (pass_ee);
       NEXT_PASS (pass_if_after_combine);
       NEXT_PASS (pass_partition_blocks);
       NEXT_PASS (pass_regmove);
@@ -1014,6 +1019,7 @@ init_optimization_passes (void)
 	  struct opt_pass **p = &pass_postreload.pass.sub;
 	  NEXT_PASS (pass_postreload_cse);
 	  NEXT_PASS (pass_gcse2);
+	  NEXT_PASS (pass_cprop_hardreg);
 	  NEXT_PASS (pass_split_after_reload);
 	  NEXT_PASS (pass_implicit_zee);
 	  NEXT_PASS (pass_compare_elim_after_reload);
@@ -1024,13 +1030,14 @@ init_optimization_passes (void)
 	  NEXT_PASS (pass_peephole2);
 	  NEXT_PASS (pass_if_after_reload);
 	  NEXT_PASS (pass_regrename);
-	  NEXT_PASS (pass_cprop_hardreg);
+	  NEXT_PASS (pass_cprop_hardreg2);
 	  NEXT_PASS (pass_fast_rtl_dce);
 	  NEXT_PASS (pass_reorder_blocks);
 	  NEXT_PASS (pass_branch_target_load_optimize2);
 	  NEXT_PASS (pass_leaf_regs);
 	  NEXT_PASS (pass_split_before_sched2);
 	  NEXT_PASS (pass_sched2);
+	  NEXT_PASS (pass_peephole2);
 	  NEXT_PASS (pass_stack_regs);
 	    {
 	      struct opt_pass **p = &pass_stack_regs.pass.sub;
diff --git a/gcc/pointer-set.c b/gcc/pointer-set.c
index b57c404..d2d2078 100644
--- a/gcc/pointer-set.c
+++ b/gcc/pointer-set.c
@@ -181,6 +181,23 @@ void pointer_set_traverse (const struct pointer_set_t *pset,
       break;
 }
 
+/* Return the number of elements in PSET.  */
+
+size_t
+pointer_set_n_elements (struct pointer_set_t *pset)
+{
+  return pset->n_elements;
+}
+
+/* Remove all entries from PSET.  */
+
+void
+pointer_set_clear (struct pointer_set_t *pset)
+{
+  pset->n_elements = 0;
+  memset (pset->slots, 0, sizeof (pset->slots[0]) * pset->n_slots);
+}
+
 
 /* A pointer map is represented the same way as a pointer_set, so
    the hash code is based on the address of the key, rather than
@@ -301,3 +318,20 @@ void pointer_map_traverse (const struct pointer_map_t *pmap,
     if (pmap->keys[i] && !fn (pmap->keys[i], &pmap->values[i], data))
       break;
 }
+
+/* Return the number of elements in PMAP.  */
+
+size_t
+pointer_map_n_elements (struct pointer_map_t *pmap)
+{
+  return pmap->n_elements;
+}
+
+/* Remove all entries from PMAP.  */
+
+void pointer_map_clear (struct pointer_map_t *pmap)
+{
+  pmap->n_elements = 0;
+  memset (pmap->keys, 0, sizeof (pmap->keys[0]) * pmap->n_slots);
+  memset (pmap->values, 0, sizeof (pmap->values[0]) * pmap->n_slots);
+}
diff --git a/gcc/pointer-set.h b/gcc/pointer-set.h
index f6b085c..75630df 100644
--- a/gcc/pointer-set.h
+++ b/gcc/pointer-set.h
@@ -29,6 +29,8 @@ int pointer_set_insert (struct pointer_set_t *pset, const void *p);
 void pointer_set_traverse (const struct pointer_set_t *,
 			   bool (*) (const void *, void *),
 			   void *);
+size_t pointer_set_n_elements (struct pointer_set_t *);
+void pointer_set_clear (struct pointer_set_t *);
 
 struct pointer_map_t;
 struct pointer_map_t *pointer_map_create (void);
@@ -38,5 +40,7 @@ void **pointer_map_contains (const struct pointer_map_t *pmap, const void *p);
 void **pointer_map_insert (struct pointer_map_t *pmap, const void *p);
 void pointer_map_traverse (const struct pointer_map_t *,
 			   bool (*) (const void *, void **, void *), void *);
+size_t pointer_map_n_elements (struct pointer_map_t *);
+void pointer_map_clear (struct pointer_map_t *);
 
 #endif  /* POINTER_SET_H  */
diff --git a/gcc/postreload.c b/gcc/postreload.c
index a423410..538d86f 100644
--- a/gcc/postreload.c
+++ b/gcc/postreload.c
@@ -46,6 +46,7 @@ along with GCC; see the file COPYING3.  If not see
 #include "target.h"
 #include "timevar.h"
 #include "tree-pass.h"
+#include "addresses.h"
 #include "df.h"
 #include "dbgcnt.h"
 
@@ -1122,6 +1123,7 @@ reload_combine_recognize_pattern (rtx insn)
       && reg_state[regno].use_index < RELOAD_COMBINE_MAX_USES
       && last_label_ruid < reg_state[regno].use_ruid)
     {
+      enum reg_class index_regs = index_reg_class (VOIDmode);
       rtx base = XEXP (src, 1);
       rtx prev = prev_nonnote_nondebug_insn (insn);
       rtx prev_set = prev ? single_set (prev) : NULL_RTX;
@@ -1134,8 +1136,8 @@ reload_combine_recognize_pattern (rtx insn)
 	 register+register that we want to use to substitute uses of REG
 	 (typically in MEMs) with.  First check REG and BASE for being
 	 index registers; we can use them even if they are not dead.  */
-      if (TEST_HARD_REG_BIT (reg_class_contents[INDEX_REG_CLASS], regno)
-	  || TEST_HARD_REG_BIT (reg_class_contents[INDEX_REG_CLASS],
+      if (TEST_HARD_REG_BIT (reg_class_contents[index_regs], regno)
+	  || TEST_HARD_REG_BIT (reg_class_contents[index_regs],
 				REGNO (base)))
 	{
 	  index_reg = reg;
@@ -1149,7 +1151,7 @@ reload_combine_recognize_pattern (rtx insn)
 	     two registers.  */
 	  for (i = first_index_reg; i <= last_index_reg; i++)
 	    {
-	      if (TEST_HARD_REG_BIT (reg_class_contents[INDEX_REG_CLASS], i)
+	      if (TEST_HARD_REG_BIT (reg_class_contents[index_regs], i)
 		  && reg_state[i].use_index == RELOAD_COMBINE_MAX_USES
 		  && reg_state[i].store_ruid <= reg_state[regno].use_ruid
 		  && (call_used_regs[i] || df_regs_ever_live_p (i))
@@ -1237,15 +1239,17 @@ reload_combine (void)
   unsigned int r;
   int min_labelno, n_labels;
   HARD_REG_SET ever_live_at_start, *label_live;
+  enum reg_class index_regs;
 
   /* To avoid wasting too much time later searching for an index register,
      determine the minimum and maximum index register numbers.  */
-  if (INDEX_REG_CLASS == NO_REGS)
+  index_regs = index_reg_class (VOIDmode);
+  if (index_regs == NO_REGS)
     last_index_reg = -1;
   else if (first_index_reg == -1 && last_index_reg == 0)
     {
       for (r = 0; r < FIRST_PSEUDO_REGISTER; r++)
-	if (TEST_HARD_REG_BIT (reg_class_contents[INDEX_REG_CLASS], r))
+	if (TEST_HARD_REG_BIT (reg_class_contents[index_regs], r))
 	  {
 	    if (first_index_reg == -1)
 	      first_index_reg = r;
@@ -1344,8 +1348,10 @@ reload_combine (void)
 	  for (link = CALL_INSN_FUNCTION_USAGE (insn); link;
 	       link = XEXP (link, 1))
 	    {
-	      rtx usage_rtx = XEXP (XEXP (link, 0), 0);
-	      if (REG_P (usage_rtx))
+	      rtx setuse = XEXP (link, 0);
+	      rtx usage_rtx = XEXP (setuse, 0);
+	      if ((GET_CODE (setuse) == USE || GET_CODE (setuse) == CLOBBER)
+		  && REG_P (usage_rtx))
 	        {
 		  unsigned int i;
 		  unsigned int start_reg = REGNO (usage_rtx);
diff --git a/gcc/print-rtl.c b/gcc/print-rtl.c
index 2a6a198..7fc4b48 100644
--- a/gcc/print-rtl.c
+++ b/gcc/print-rtl.c
@@ -313,9 +313,16 @@ print_rtx (const_rtx in_rtx)
 	      }
 	  }
 	else if (i == 8 && JUMP_P (in_rtx) && JUMP_LABEL (in_rtx) != NULL)
-	  /* Output the JUMP_LABEL reference.  */
-	  fprintf (outfile, "\n%s%*s -> %d", print_rtx_head, indent * 2, "",
-		   INSN_UID (JUMP_LABEL (in_rtx)));
+	  {
+	    /* Output the JUMP_LABEL reference.  */
+	    fprintf (outfile, "\n%s%*s -> ", print_rtx_head, indent * 2, "");
+	    if (GET_CODE (JUMP_LABEL (in_rtx)) == RETURN)
+	      fprintf (outfile, "return");
+	    else if (GET_CODE (JUMP_LABEL (in_rtx)) == SIMPLE_RETURN)
+	      fprintf (outfile, "simple_return");
+	    else
+	      fprintf (outfile, "%d", INSN_UID (JUMP_LABEL (in_rtx)));
+	  }
 	else if (i == 0 && GET_CODE (in_rtx) == VALUE)
 	  {
 #ifndef GENERATOR_FILE
diff --git a/gcc/reg-notes.def b/gcc/reg-notes.def
index 329cd67..2702bc3 100644
--- a/gcc/reg-notes.def
+++ b/gcc/reg-notes.def
@@ -92,6 +92,7 @@ REG_NOTE (LABEL_OPERAND)
    respectively.  */
 REG_NOTE (DEP_OUTPUT)
 REG_NOTE (DEP_ANTI)
+REG_NOTE (DEP_CONTROL)
 
 /* REG_BR_PROB is attached to JUMP_INSNs and CALL_INSNs.  It has an
    integer value.  For jumps, it is the probability that this is a
diff --git a/gcc/regcprop.c b/gcc/regcprop.c
index 0f0dfb3..049bdce 100644
--- a/gcc/regcprop.c
+++ b/gcc/regcprop.c
@@ -39,6 +39,7 @@
 #include "timevar.h"
 #include "tree-pass.h"
 #include "df.h"
+#include "dce.h"
 
 /* The following code does forward propagation of hard register copies.
    The object is to eliminate as many dependencies as possible, so that
@@ -254,18 +255,27 @@ kill_clobbered_value (rtx x, const_rtx set, void *data)
     kill_value (x, vd);
 }
 
+/* A structure passed as data to kill_set_value through note_stores.  */
+struct kill_set_value_data
+{
+  struct value_data *vd;
+  rtx ignore_set_reg;
+};
+  
 /* Called through note_stores.  If X is set, not clobbered, kill its
    current value and install it as the root of its own value list.  */
 
 static void
 kill_set_value (rtx x, const_rtx set, void *data)
 {
-  struct value_data *const vd = (struct value_data *) data;
+  struct kill_set_value_data *ksvd = (struct kill_set_value_data *) data;
+  if (rtx_equal_p (x, ksvd->ignore_set_reg))
+    return;
   if (GET_CODE (set) != CLOBBER)
     {
-      kill_value (x, vd);
+      kill_value (x, ksvd->vd);
       if (REG_P (x))
-	set_value_regno (REGNO (x), GET_MODE (x), vd);
+	set_value_regno (REGNO (x), GET_MODE (x), ksvd->vd);
     }
 }
 
@@ -585,14 +595,14 @@ replace_oldest_value_addr (rtx *loc, enum reg_class cl,
 	    int index_op;
 	    unsigned regno0 = REGNO (op0), regno1 = REGNO (op1);
 
-	    if (REGNO_OK_FOR_INDEX_P (regno1)
+	    if (regno_ok_for_index_p (regno1, mode)
 		&& regno_ok_for_base_p (regno0, mode, PLUS, REG))
 	      index_op = 1;
-	    else if (REGNO_OK_FOR_INDEX_P (regno0)
+	    else if (regno_ok_for_index_p (regno0, mode)
 		     && regno_ok_for_base_p (regno1, mode, PLUS, REG))
 	      index_op = 0;
 	    else if (regno_ok_for_base_p (regno0, mode, PLUS, REG)
-		     || REGNO_OK_FOR_INDEX_P (regno1))
+		     || regno_ok_for_index_p (regno1, mode))
 	      index_op = 1;
 	    else if (regno_ok_for_base_p (regno1, mode, PLUS, REG))
 	      index_op = 0;
@@ -617,8 +627,8 @@ replace_oldest_value_addr (rtx *loc, enum reg_class cl,
 	  }
 
 	if (locI)
-	  changed |= replace_oldest_value_addr (locI, INDEX_REG_CLASS, mode,
-						insn, vd);
+	  changed |= replace_oldest_value_addr (locI, index_reg_class(mode), 
+						mode, insn, vd);
 	if (locB)
 	  changed |= replace_oldest_value_addr (locB,
 						base_reg_class (mode, PLUS,
@@ -743,6 +753,7 @@ copyprop_hardreg_forward_1 (basic_block bb, struct value_data *vd)
       rtx set;
       bool replaced[MAX_RECOG_OPERANDS];
       bool changed = false;
+      struct kill_set_value_data ksvd;
 
       if (!NONDEBUG_INSN_P (insn))
 	{
@@ -955,14 +966,39 @@ copyprop_hardreg_forward_1 (basic_block bb, struct value_data *vd)
 	    note_uses (&PATTERN (insn), cprop_find_used_regs, vd);
 	}
 
+      ksvd.vd = vd;
+      ksvd.ignore_set_reg = NULL_RTX;
+
       /* Clobber call-clobbered registers.  */
       if (CALL_P (insn))
-	for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
-	  if (TEST_HARD_REG_BIT (regs_invalidated_by_call, i))
-	    kill_value_regno (i, 1, vd);
+	{
+	  int set_regno = INVALID_REGNUM;
+	  int set_nregs = 0;
+	  rtx exp;
+	  for (exp = CALL_INSN_FUNCTION_USAGE (insn); exp; exp = XEXP (exp, 1))
+	    {
+	      rtx x = XEXP (exp, 0);
+	      if (GET_CODE (x) == SET)
+		{
+		  rtx dest = SET_DEST (x);
+		  kill_value (dest, vd);
+		  set_value_regno (REGNO (dest), GET_MODE (dest), vd);
+		  copy_value (dest, SET_SRC (x), vd);
+		  ksvd.ignore_set_reg = dest;
+		  set_regno = REGNO (dest);
+		  set_nregs
+		    = hard_regno_nregs[set_regno][GET_MODE (dest)];
+		  break;
+		}
+	    }
+	  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
+	    if (TEST_HARD_REG_BIT (regs_invalidated_by_call, i)
+		&& (i < set_regno || i >= set_regno + set_nregs))
+	      kill_value_regno (i, 1, vd);
+	}
 
       /* Notice stores.  */
-      note_stores (PATTERN (insn), kill_set_value, vd);
+      note_stores (PATTERN (insn), kill_set_value, &ksvd);
 
       /* Notice copies.  */
       if (set && REG_P (SET_DEST (set)) && REG_P (SET_SRC (set)))
@@ -1166,12 +1202,28 @@ validate_value_data (struct value_data *vd)
 }
 #endif
 
+/* An early cprop pass, intended to make it easier for prepare_shrink_wrap
+   to move register moves downwards through the CFG.  */
 static bool
 gate_handle_cprop (void)
 {
-  return (optimize > 0 && (flag_cprop_registers));
+#ifdef HAVE_simple_return
+  return (optimize > 0 && flag_cprop_registers && HAVE_simple_return);
+#else
+  return 0;
+#endif
 }
 
+static unsigned int
+early_copyprop_hardreg_forward (void)
+{
+  unsigned int retval;
+
+  split_all_insns ();
+  retval = copyprop_hardreg_forward ();
+  run_fast_dce ();
+  return retval;
+}
 
 struct rtl_opt_pass pass_cprop_hardreg =
 {
@@ -1179,6 +1231,33 @@ struct rtl_opt_pass pass_cprop_hardreg =
   RTL_PASS,
   "cprop_hardreg",                      /* name */
   gate_handle_cprop,                    /* gate */
+  early_copyprop_hardreg_forward,       /* execute */
+  NULL,                                 /* sub */
+  NULL,                                 /* next */
+  0,                                    /* static_pass_number */
+  TV_CPROP_REGISTERS,                   /* tv_id */
+  0,                                    /* properties_required */
+  0,                                    /* properties_provided */
+  0,                                    /* properties_destroyed */
+  0,                                    /* todo_flags_start */
+  TODO_dump_func | TODO_df_finish
+  | TODO_verify_rtl_sharing		/* todo_flags_finish */
+ }
+};
+
+static bool
+gate_handle_cprop2 (void)
+{
+  return (optimize > 0 && (flag_cprop_registers));
+}
+
+
+struct rtl_opt_pass pass_cprop_hardreg2 =
+{
+ {
+  RTL_PASS,
+  "cprop_hardreg2",                     /* name */
+  gate_handle_cprop2,                   /* gate */
   copyprop_hardreg_forward,             /* execute */
   NULL,                                 /* sub */
   NULL,                                 /* next */
diff --git a/gcc/regrename.c b/gcc/regrename.c
index c2292ef..a99ee2a 100644
--- a/gcc/regrename.c
+++ b/gcc/regrename.c
@@ -868,14 +868,14 @@ scan_rtx_address (rtx insn, rtx *loc, enum reg_class cl,
 	    int index_op;
 	    unsigned regno0 = REGNO (op0), regno1 = REGNO (op1);
 
-	    if (REGNO_OK_FOR_INDEX_P (regno1)
+	    if (regno_ok_for_index_p (regno1, mode)
 		&& regno_ok_for_base_p (regno0, mode, PLUS, REG))
 	      index_op = 1;
-	    else if (REGNO_OK_FOR_INDEX_P (regno0)
+	    else if (regno_ok_for_index_p (regno0, mode)
 		     && regno_ok_for_base_p (regno1, mode, PLUS, REG))
 	      index_op = 0;
 	    else if (regno_ok_for_base_p (regno0, mode, PLUS, REG)
-		     || REGNO_OK_FOR_INDEX_P (regno1))
+		     || regno_ok_for_index_p (regno1, mode))
 	      index_op = 1;
 	    else if (regno_ok_for_base_p (regno1, mode, PLUS, REG))
 	      index_op = 0;
@@ -900,7 +900,7 @@ scan_rtx_address (rtx insn, rtx *loc, enum reg_class cl,
 	  }
 
 	if (locI)
-	  scan_rtx_address (insn, locI, INDEX_REG_CLASS, action, mode);
+	  scan_rtx_address (insn, locI, index_reg_class (mode), action, mode);
 	if (locB)
 	  scan_rtx_address (insn, locB, base_reg_class (mode, PLUS, index_code),
 			    action, mode);
diff --git a/gcc/regs.h b/gcc/regs.h
index a19a15d..67817d8 100644
--- a/gcc/regs.h
+++ b/gcc/regs.h
@@ -397,4 +397,8 @@ overlaps_hard_reg_set_p (const HARD_REG_SET regs, enum machine_mode mode,
   return false;
 }
 
+/* Get registers used by given function call instruction.  */
+void get_call_reg_set_usage (rtx insn, HARD_REG_SET *reg_set,
+			     HARD_REG_SET default_set);
+
 #endif /* GCC_REGS_H */
diff --git a/gcc/reload.c b/gcc/reload.c
index 8b57a99..53817bf 100644
--- a/gcc/reload.c
+++ b/gcc/reload.c
@@ -5086,7 +5086,8 @@ find_reloads_address (enum machine_mode mode, rtx *memrefloc, rtx ad,
 
       if (double_reg_address_ok
 	  && regno_ok_for_base_p (REGNO (XEXP (ad, 0)), mode,
-				  PLUS, CONST_INT))
+				  PLUS, CONST_INT)
+	  && index_reg_class (mode) != NO_REGS)
 	{
 	  /* Unshare the sum as well.  */
 	  *loc = ad = copy_rtx (ad);
@@ -5094,8 +5095,8 @@ find_reloads_address (enum machine_mode mode, rtx *memrefloc, rtx ad,
 	  /* Reload the displacement into an index reg.
 	     We assume the frame pointer or arg pointer is a base reg.  */
 	  find_reloads_address_part (XEXP (ad, 1), &XEXP (ad, 1),
-				     INDEX_REG_CLASS, GET_MODE (ad), opnum,
-				     type, ind_levels);
+				     index_reg_class (mode), GET_MODE (ad),
+				     opnum, type, ind_levels);
 	  return 0;
 	}
       else
@@ -5489,13 +5490,13 @@ find_reloads_address_1 (enum machine_mode mode, rtx x, int context,
 #define REG_OK_FOR_CONTEXT(CONTEXT, REGNO, MODE, OUTER, INDEX)		\
   ((CONTEXT) == 0							\
    ? regno_ok_for_base_p (REGNO, MODE, OUTER, INDEX)			\
-   : REGNO_OK_FOR_INDEX_P (REGNO))
+   : regno_ok_for_index_p (REGNO, MODE))
 
   enum reg_class context_reg_class;
   RTX_CODE code = GET_CODE (x);
 
   if (context == 1)
-    context_reg_class = INDEX_REG_CLASS;
+    context_reg_class = index_reg_class (mode);
   else
     context_reg_class = base_reg_class (mode, outer_code, index_code);
 
@@ -5587,17 +5588,17 @@ find_reloads_address_1 (enum machine_mode mode, rtx x, int context,
 
 	else if (code0 == REG && code1 == REG)
 	  {
-	    if (REGNO_OK_FOR_INDEX_P (REGNO (op1))
+	    if (regno_ok_for_index_p (REGNO (op1), mode)
 		&& regno_ok_for_base_p (REGNO (op0), mode, PLUS, REG))
 	      return 0;
-	    else if (REGNO_OK_FOR_INDEX_P (REGNO (op0))
+	    else if (regno_ok_for_index_p (REGNO (op0), mode)
 		     && regno_ok_for_base_p (REGNO (op1), mode, PLUS, REG))
 	      return 0;
 	    else if (regno_ok_for_base_p (REGNO (op0), mode, PLUS, REG))
 	      find_reloads_address_1 (mode, orig_op1, 1, PLUS, SCRATCH,
 				      &XEXP (x, 1), opnum, type, ind_levels,
 				      insn);
-	    else if (REGNO_OK_FOR_INDEX_P (REGNO (op1)))
+	    else if (regno_ok_for_index_p (REGNO (op1), mode))
 	      find_reloads_address_1 (mode, orig_op0, 0, PLUS, REG,
 				      &XEXP (x, 0), opnum, type, ind_levels,
 				      insn);
@@ -5605,7 +5606,7 @@ find_reloads_address_1 (enum machine_mode mode, rtx x, int context,
 	      find_reloads_address_1 (mode, orig_op0, 1, PLUS, SCRATCH,
 				      &XEXP (x, 0), opnum, type, ind_levels,
 				      insn);
-	    else if (REGNO_OK_FOR_INDEX_P (REGNO (op0)))
+	    else if (regno_ok_for_index_p (REGNO (op0), mode))
 	      find_reloads_address_1 (mode, orig_op1, 0, PLUS, REG,
 				      &XEXP (x, 1), opnum, type, ind_levels,
 				      insn);
@@ -5675,7 +5676,7 @@ find_reloads_address_1 (enum machine_mode mode, rtx x, int context,
 	   need to live longer than a TYPE reload normally would, so be
 	   conservative and class it as RELOAD_OTHER.  */
 	if ((REG_P (XEXP (op1, 1))
-	     && !REGNO_OK_FOR_INDEX_P (REGNO (XEXP (op1, 1))))
+	     && !regno_ok_for_index_p (REGNO (XEXP (op1, 1)), mode))
 	    || GET_CODE (XEXP (op1, 1)) == PLUS)
 	  find_reloads_address_1 (mode, XEXP (op1, 1), 1, code, SCRATCH,
 				  &XEXP (op1, 1), opnum, RELOAD_OTHER,
diff --git a/gcc/reload1.c b/gcc/reload1.c
index 84cbfca..cb52987 100644
--- a/gcc/reload1.c
+++ b/gcc/reload1.c
@@ -2352,6 +2352,19 @@ set_label_offsets (rtx x, rtx insn, int initial_p)
 
       if (! offsets_known_at[CODE_LABEL_NUMBER (x) - first_label_num])
 	{
+	  if (x == insn)
+	    {
+	      basic_block bb;
+
+	      bb = BLOCK_FOR_INSN (insn);
+
+	      /* If the label is the target of a non-local GOTO, we must use
+	         the initial elimination offsets.  */
+	      if (bb && BB_HEAD (bb) == insn
+		  && (bb->flags & BB_NON_LOCAL_GOTO_TARGET))
+		initial_p = true;
+	    }
+	  
 	  for (i = 0; i < NUM_ELIMINABLE_REGS; i++)
 	    offsets_at[CODE_LABEL_NUMBER (x) - first_label_num][i]
 	      = (initial_p ? reg_eliminate[i].initial_offset
diff --git a/gcc/reorg.c b/gcc/reorg.c
index 2677b48..7dc20ae 100644
--- a/gcc/reorg.c
+++ b/gcc/reorg.c
@@ -161,8 +161,11 @@ static rtx *unfilled_firstobj;
 #define unfilled_slots_next	\
   ((rtx *) obstack_next_free (&unfilled_slots_obstack))
 
-/* Points to the label before the end of the function.  */
-static rtx end_of_function_label;
+/* Points to the label before the end of the function, or before a
+   return insn.  */
+static rtx function_return_label;
+/* Likewise for a simple_return.  */
+static rtx function_simple_return_label;
 
 /* Mapping between INSN_UID's and position in the code since INSN_UID's do
    not always monotonically increase.  */
@@ -175,7 +178,7 @@ static int stop_search_p (rtx, int);
 static int resource_conflicts_p (struct resources *, struct resources *);
 static int insn_references_resource_p (rtx, struct resources *, bool);
 static int insn_sets_resource_p (rtx, struct resources *, bool);
-static rtx find_end_label (void);
+static rtx find_end_label (rtx);
 static rtx emit_delay_sequence (rtx, rtx, int);
 static rtx add_to_delay_list (rtx, rtx);
 static rtx delete_from_delay_slot (rtx);
@@ -195,8 +198,8 @@ static int check_annul_list_true_false (int, rtx);
 static rtx steal_delay_list_from_target (rtx, rtx, rtx, rtx,
 					 struct resources *,
 					 struct resources *,
-					 struct resources *,
-					 int, int *, int *, rtx *);
+					 struct resources *, rtx,
+					 int, int, int *, int *, rtx *);
 static rtx steal_delay_list_from_fallthrough (rtx, rtx, rtx, rtx,
 					      struct resources *,
 					      struct resources *,
@@ -212,7 +215,7 @@ static void fix_reg_dead_note (rtx, rtx);
 static void update_reg_unused_notes (rtx, rtx);
 static void fill_simple_delay_slots (int);
 static rtx fill_slots_from_thread (rtx, rtx, rtx, rtx,
-				   int, int, int, int,
+				   int, int, int, int, int,
 				   int *, rtx);
 static void fill_eager_delay_slots (void);
 static void relax_delay_slots (rtx);
@@ -220,6 +223,15 @@ static void relax_delay_slots (rtx);
 static void make_return_insns (rtx);
 #endif
 
+/* Return true iff INSN is a simplejump, or any kind of return insn.  */
+
+static bool
+simplejump_or_return_p (rtx insn)
+{
+  return (JUMP_P (insn)
+	  && (simplejump_p (insn) || ANY_RETURN_P (PATTERN (insn))));
+}
+
 /* Return TRUE if this insn should stop the search for insn to fill delay
    slots.  LABELS_P indicates that labels should terminate the search.
    In all cases, jumps terminate the search.  */
@@ -335,23 +347,29 @@ insn_sets_resource_p (rtx insn, struct resources *res,
 
    ??? There may be a problem with the current implementation.  Suppose
    we start with a bare RETURN insn and call find_end_label.  It may set
-   end_of_function_label just before the RETURN.  Suppose the machinery
+   function_return_label just before the RETURN.  Suppose the machinery
    is able to fill the delay slot of the RETURN insn afterwards.  Then
-   end_of_function_label is no longer valid according to the property
+   function_return_label is no longer valid according to the property
    described above and find_end_label will still return it unmodified.
    Note that this is probably mitigated by the following observation:
-   once end_of_function_label is made, it is very likely the target of
+   once function_return_label is made, it is very likely the target of
    a jump, so filling the delay slot of the RETURN will be much more
    difficult.  */
 
 static rtx
-find_end_label (void)
+find_end_label (rtx kind)
 {
   rtx insn;
+  rtx *plabel;
+
+  if (kind == ret_rtx)
+    plabel = &function_return_label;
+  else
+    plabel = &function_simple_return_label;
 
   /* If we found one previously, return it.  */
-  if (end_of_function_label)
-    return end_of_function_label;
+  if (*plabel)
+    return *plabel;
 
   /* Otherwise, see if there is a label at the end of the function.  If there
      is, it must be that RETURN insns aren't needed, so that is our return
@@ -366,44 +384,44 @@ find_end_label (void)
 
   /* When a target threads its epilogue we might already have a
      suitable return insn.  If so put a label before it for the
-     end_of_function_label.  */
+     function_return_label.  */
   if (BARRIER_P (insn)
       && JUMP_P (PREV_INSN (insn))
-      && GET_CODE (PATTERN (PREV_INSN (insn))) == RETURN)
+      && PATTERN (PREV_INSN (insn)) == kind)
     {
       rtx temp = PREV_INSN (PREV_INSN (insn));
-      end_of_function_label = gen_label_rtx ();
-      LABEL_NUSES (end_of_function_label) = 0;
+      rtx label = gen_label_rtx ();
+      LABEL_NUSES (label) = 0;
 
       /* Put the label before an USE insns that may precede the RETURN insn.  */
       while (GET_CODE (temp) == USE)
 	temp = PREV_INSN (temp);
 
-      emit_label_after (end_of_function_label, temp);
+      emit_label_after (label, temp);
+      *plabel = label;
     }
 
   else if (LABEL_P (insn))
-    end_of_function_label = insn;
+    *plabel = insn;
   else
     {
-      end_of_function_label = gen_label_rtx ();
-      LABEL_NUSES (end_of_function_label) = 0;
+      rtx label = gen_label_rtx ();
+      LABEL_NUSES (label) = 0;
       /* If the basic block reorder pass moves the return insn to
 	 some other place try to locate it again and put our
-	 end_of_function_label there.  */
-      while (insn && ! (JUMP_P (insn)
-		        && (GET_CODE (PATTERN (insn)) == RETURN)))
+	 function_return_label there.  */
+      while (insn && ! (JUMP_P (insn) && (PATTERN (insn) == kind)))
 	insn = PREV_INSN (insn);
       if (insn)
 	{
 	  insn = PREV_INSN (insn);
 
-	  /* Put the label before an USE insns that may proceed the
+	  /* Put the label before an USE insns that may precede the
 	     RETURN insn.  */
 	  while (GET_CODE (insn) == USE)
 	    insn = PREV_INSN (insn);
 
-	  emit_label_after (end_of_function_label, insn);
+	  emit_label_after (label, insn);
 	}
       else
 	{
@@ -413,19 +431,16 @@ find_end_label (void)
 	      && ! HAVE_return
 #endif
 	      )
-	    {
-	      /* The RETURN insn has its delay slot filled so we cannot
-		 emit the label just before it.  Since we already have
-		 an epilogue and cannot emit a new RETURN, we cannot
-		 emit the label at all.  */
-	      end_of_function_label = NULL_RTX;
-	      return end_of_function_label;
-	    }
+	    /* The RETURN insn has its delay slot filled so we cannot
+	       emit the label just before it.  Since we already have
+	       an epilogue and cannot emit a new RETURN, we cannot
+	       emit the label at all.  */
+	    return NULL_RTX;
 #endif /* HAVE_epilogue */
 
 	  /* Otherwise, make a new label and emit a RETURN and BARRIER,
 	     if needed.  */
-	  emit_label (end_of_function_label);
+	  emit_label (label);
 #ifdef HAVE_return
 	  /* We don't bother trying to create a return insn if the
 	     epilogue has filled delay-slots; we would have to try and
@@ -437,19 +452,21 @@ find_end_label (void)
 	      /* The return we make may have delay slots too.  */
 	      rtx insn = gen_return ();
 	      insn = emit_jump_insn (insn);
+	      JUMP_LABEL (insn) = ret_rtx;
 	      emit_barrier ();
 	      if (num_delay_slots (insn) > 0)
 		obstack_ptr_grow (&unfilled_slots_obstack, insn);
 	    }
 #endif
 	}
+      *plabel = label;
     }
 
   /* Show one additional use for this label so it won't go away until
      we are done.  */
-  ++LABEL_NUSES (end_of_function_label);
+  ++LABEL_NUSES (*plabel);
 
-  return end_of_function_label;
+  return *plabel;
 }
 
 /* Put INSN and LIST together in a SEQUENCE rtx of LENGTH, and replace
@@ -529,6 +546,9 @@ emit_delay_sequence (rtx insn, rtx list, int length)
       PREV_INSN (tem) = XVECEXP (seq, 0, i - 1);
       NEXT_INSN (XVECEXP (seq, 0, i - 1)) = tem;
 
+      if (LABEL_P (tem))
+        continue;
+
       /* SPARC assembler, for instance, emit warning when debug info is output
          into the delay slot.  */
       if (INSN_LOCATOR (tem) && !INSN_LOCATOR (seq_insn))
@@ -623,7 +643,7 @@ delete_from_delay_slot (rtx insn)
      PREV_INSN (NEXT_INSN (TRIAL)) != TRIAL.  */
 
   for (trial = insn;
-       PREV_INSN (NEXT_INSN (trial)) == trial;
+       PREV_INSN (NEXT_INSN (trial)) == trial || LABEL_P (trial);
        trial = NEXT_INSN (trial))
     ;
 
@@ -658,6 +678,9 @@ delete_from_delay_slot (rtx insn)
   else if (INSN_P (trial))
     INSN_ANNULLED_BRANCH_P (trial) = 0;
 
+  if (LABEL_P (insn))
+    return trial;
+
   INSN_FROM_TARGET_P (insn) = 0;
 
   /* Show we need to fill this insn again.  */
@@ -797,10 +820,8 @@ optimize_skip (rtx insn)
   if ((next_trial == next_active_insn (JUMP_LABEL (insn))
        && ! (next_trial == 0 && crtl->epilogue_delay_list != 0))
       || (next_trial != 0
-	  && JUMP_P (next_trial)
-	  && JUMP_LABEL (insn) == JUMP_LABEL (next_trial)
-	  && (simplejump_p (next_trial)
-	      || GET_CODE (PATTERN (next_trial)) == RETURN)))
+	  && simplejump_or_return_p (next_trial)
+	  && JUMP_LABEL (insn) == JUMP_LABEL (next_trial)))
     {
       if (eligible_for_annul_false (insn, 0, trial, flags))
 	{
@@ -819,13 +840,11 @@ optimize_skip (rtx insn)
 	 branch, thread our jump to the target of that branch.  Don't
 	 change this into a RETURN here, because it may not accept what
 	 we have in the delay slot.  We'll fix this up later.  */
-      if (next_trial && JUMP_P (next_trial)
-	  && (simplejump_p (next_trial)
-	      || GET_CODE (PATTERN (next_trial)) == RETURN))
+      if (next_trial && simplejump_or_return_p (next_trial))
 	{
 	  rtx target_label = JUMP_LABEL (next_trial);
-	  if (target_label == 0)
-	    target_label = find_end_label ();
+	  if (ANY_RETURN_P (target_label))
+	    target_label = find_end_label (target_label);
 
 	  if (target_label)
 	    {
@@ -866,7 +885,7 @@ get_jump_flags (rtx insn, rtx label)
   if (JUMP_P (insn)
       && (condjump_p (insn) || condjump_in_parallel_p (insn))
       && INSN_UID (insn) <= max_uid
-      && label != 0
+      && label != 0 && !ANY_RETURN_P (label)
       && INSN_UID (label) <= max_uid)
     flags
       = (uid_to_ruid[INSN_UID (label)] > uid_to_ruid[INSN_UID (insn)])
@@ -1038,7 +1057,7 @@ get_branch_condition (rtx insn, rtx target)
     pat = XVECEXP (pat, 0, 0);
 
   if (GET_CODE (pat) == RETURN)
-    return target == 0 ? const_true_rtx : 0;
+    return ANY_RETURN_P (target) ? const_true_rtx : 0;
 
   else if (GET_CODE (pat) != SET || SET_DEST (pat) != pc_rtx)
     return 0;
@@ -1186,6 +1205,55 @@ check_annul_list_true_false (int annul_true_p, rtx delay_list)
 
   return 1;
 }
+
+/* TRIAL is an insn from a thread.  See if we can find a duplicate of trial in
+   the OPPOSITE_THREAD that we can hoist to before OPPOSITE_THREAD.  */
+
+static bool
+has_opposite_duplicate (rtx trial, rtx opposite_thread, rtx *duplicate)
+{
+  rtx pat;
+  rtx scan, prev;
+  struct resources prev_needed, prev_set;
+
+  pat = PATTERN (trial);
+
+  /* Initialize prev_needed and prev_set.  */
+  CLEAR_RESOURCE (&prev_needed);
+  CLEAR_RESOURCE (&prev_set);
+
+  /* Be conservative with respect to cc.  */
+  prev_set.cc = 1;
+
+  for ((prev = NULL_RTX), (scan = opposite_thread); !stop_search_p (scan, 1);
+       (prev = scan), (scan = next_nonnote_insn (scan)))
+    {
+      if (prev != NULL_RTX)
+        {
+          /* Mark any register set or referenced by a previous insn in
+             prev_set and prev_needed.  */
+          mark_set_resources (prev, &prev_set, 0, MARK_SRC_DEST_CALL);
+          mark_referenced_resources (prev, &prev_needed, true);
+        }
+
+      /* We're looking for a duplicate of trial.  */
+      if (!rtx_equal_p (pat, PATTERN (scan)))
+        continue;
+
+      /* If the duplicate conflicts with any previous insn, give up.  Testing
+         for anti-dependence, output dependence and true dependence.  */
+      if (insn_sets_resource_p (scan, &prev_needed, true)
+          || insn_sets_resource_p (scan, &prev_set, true)
+          || insn_references_resource_p (scan, &prev_set, true))
+        break;
+
+      *duplicate = scan;
+      return true;
+    }
+
+  return false;
+}
+
 
 /* INSN branches to an insn whose pattern SEQ is a SEQUENCE.  Given that
    the condition tested by INSN is CONDITION and the resources shown in
@@ -1212,6 +1280,7 @@ steal_delay_list_from_target (rtx insn, rtx condition, rtx seq,
 			      rtx delay_list, struct resources *sets,
 			      struct resources *needed,
 			      struct resources *other_needed,
+			      rtx other_thread, int own_opposite_thread,
 			      int slots_to_fill, int *pslots_filled,
 			      int *pannul_p, rtx *pnew_thread)
 {
@@ -1223,6 +1292,7 @@ steal_delay_list_from_target (rtx insn, rtx condition, rtx seq,
   int used_annul = 0;
   int i;
   struct resources cc_set;
+  rtx duplicate;
 
   /* We can't do anything if there are more delay slots in SEQ than we
      can handle, or if we don't know that it will be a taken branch.
@@ -1252,7 +1322,8 @@ steal_delay_list_from_target (rtx insn, rtx condition, rtx seq,
 
   if (XVECLEN (seq, 0) - 1 > slots_remaining
       || ! condition_dominates_p (condition, XVECEXP (seq, 0, 0))
-      || ! single_set (XVECEXP (seq, 0, 0)))
+      || ! (single_set (XVECEXP (seq, 0, 0))
+            || GET_CODE (PATTERN (XVECEXP (seq, 0, 0))) == RETURN))
     return delay_list;
 
 #ifdef MD_CAN_REDIRECT_BRANCH
@@ -1291,9 +1362,14 @@ steal_delay_list_from_target (rtx insn, rtx condition, rtx seq,
 	 based on jumping to the new label.  */
       flags = get_jump_flags (insn, JUMP_LABEL (XVECEXP (seq, 0, 0)));
 
+      duplicate = NULL_RTX;
+
       if (! must_annul
 	  && ((condition == const_true_rtx
-	       || (! insn_sets_resource_p (trial, other_needed, false)
+	       || ((! insn_sets_resource_p (trial, other_needed, false)
+                    || (own_opposite_thread
+                        && has_opposite_duplicate (trial, other_thread,
+                                                   &duplicate)))
 		   && ! may_trap_or_fault_p (PATTERN (trial)))))
 	  ? eligible_for_delay (insn, total_slots_filled, trial, flags)
 	  : (must_annul || (delay_list == NULL && new_delay_list == NULL))
@@ -1310,6 +1386,9 @@ steal_delay_list_from_target (rtx insn, rtx condition, rtx seq,
 	  new_delay_list = add_to_delay_list (temp, new_delay_list);
 	  total_slots_filled++;
 
+          if (!must_annul && duplicate != NULL_RTX)
+            delete_related_insns (duplicate);
+
 	  if (--slots_remaining == 0)
 	    break;
 	}
@@ -1318,7 +1397,11 @@ steal_delay_list_from_target (rtx insn, rtx condition, rtx seq,
     }
 
   /* Show the place to which we will be branching.  */
-  *pnew_thread = next_active_insn (JUMP_LABEL (XVECEXP (seq, 0, 0)));
+  temp = JUMP_LABEL (XVECEXP (seq, 0, 0));
+  if (ANY_RETURN_P (temp))
+    *pnew_thread = temp;
+  else
+    *pnew_thread = next_active_insn (temp);
 
   /* Add any new insns to the delay list and update the count of the
      number of slots filled.  */
@@ -1358,8 +1441,7 @@ steal_delay_list_from_fallthrough (rtx insn, rtx condition, rtx seq,
   /* We can't do anything if SEQ's delay insn isn't an
      unconditional branch.  */
 
-  if (! simplejump_p (XVECEXP (seq, 0, 0))
-      && GET_CODE (PATTERN (XVECEXP (seq, 0, 0))) != RETURN)
+  if (! simplejump_or_return_p (XVECEXP (seq, 0, 0)))
     return delay_list;
 
   for (i = 1; i < XVECLEN (seq, 0); i++)
@@ -1522,6 +1604,9 @@ try_merge_delay_insns (rtx insn, rtx thread)
 	{
 	  rtx dtrial = XVECEXP (pat, 0, i);
 
+          if (LABEL_P (dtrial) || DELETED_NOTE_P (dtrial))
+            return;
+
 	  if (! insn_references_resource_p (dtrial, &set, true)
 	      && ! insn_sets_resource_p (dtrial, &set, true)
 	      && ! insn_sets_resource_p (dtrial, &needed, true)
@@ -1615,17 +1700,22 @@ static rtx
 redundant_insn (rtx insn, rtx target, rtx delay_list)
 {
   rtx target_main = target;
-  rtx ipat = PATTERN (insn);
+  rtx ipat;
   rtx trial, pat;
   struct resources needed, set;
   int i;
   unsigned insns_to_search;
 
+  if (LABEL_P (insn) || DELETED_NOTE_P (insn))
+    return NULL_RTX;
+
   /* If INSN has any REG_UNUSED notes, it can't match anything since we
      are allowed to not actually assign to such a register.  */
   if (find_reg_note (insn, REG_UNUSED, NULL_RTX) != 0)
     return 0;
 
+  ipat = PATTERN (insn);
+
   /* Scan backwards looking for a match.  */
   for (trial = PREV_INSN (target),
 	 insns_to_search = MAX_DELAY_SLOT_INSN_SEARCH;
@@ -1772,6 +1862,9 @@ redundant_insn (rtx insn, rtx target, rtx delay_list)
 	    {
 	      rtx candidate = XVECEXP (pat, 0, i);
 
+	      if (LABEL_P (candidate) || DELETED_NOTE_P (candidate))
+		return NULL_RTX;
+
 	      /* If an insn will be annulled if the branch is false, it isn't
 		 considered as a possible duplicate insn.  */
 	      if (rtx_equal_p (PATTERN (candidate), ipat)
@@ -1780,6 +1873,7 @@ redundant_insn (rtx insn, rtx target, rtx delay_list)
 		{
 		  /* Show that this insn will be used in the sequel.  */
 		  INSN_FROM_TARGET_P (candidate) = 0;
+		  incr_ticks_for_insn (candidate);
 		  return candidate;
 		}
 
@@ -1827,7 +1921,7 @@ own_thread_p (rtx thread, rtx label, int allow_fallthrough)
   rtx insn;
 
   /* We don't own the function end.  */
-  if (thread == 0)
+  if (ANY_RETURN_P (thread))
     return 0;
 
   /* Get the first active insn, or THREAD, if it is an active insn.  */
@@ -2005,6 +2099,17 @@ get_label_before (rtx insn)
   return label;
 }
 
+/* Determine whether a delay list contains a label or not.  */
+
+static int
+delay_list_has_label (rtx list)
+{
+  for (;list != NULL_RTX; list = XEXP (list, 1))
+    if (LABEL_P (XEXP (list, 0)))
+      return 1;
+  return 0;
+}
+
 /* Scan a function looking for insns that need a delay slot and find insns to
    put into the delay slot.
 
@@ -2245,7 +2350,8 @@ fill_simple_delay_slots (int non_jumps_p)
 	  && (!JUMP_P (insn)
 	      || ((condjump_p (insn) || condjump_in_parallel_p (insn))
 		  && ! simplejump_p (insn)
-		  && JUMP_LABEL (insn) != 0)))
+		  && JUMP_LABEL (insn) != 0
+		  && !ANY_RETURN_P (JUMP_LABEL (insn)))))
 	{
 	  /* Invariant: If insn is a JUMP_INSN, the insn's jump
 	     label.  Otherwise, zero.  */
@@ -2270,7 +2376,7 @@ fill_simple_delay_slots (int non_jumps_p)
 		target = JUMP_LABEL (insn);
 	    }
 
-	  if (target == 0)
+	  if (target == 0 || ANY_RETURN_P (target))
 	    for (trial = next_nonnote_insn (insn); !stop_search_p (trial, 1);
 		 trial = next_trial)
 	      {
@@ -2346,6 +2452,7 @@ fill_simple_delay_slots (int non_jumps_p)
 	      && JUMP_P (trial)
 	      && simplejump_p (trial)
 	      && (target == 0 || JUMP_LABEL (trial) == target)
+	      && !ANY_RETURN_P (JUMP_LABEL (trial))
 	      && (next_trial = next_active_insn (JUMP_LABEL (trial))) != 0
 	      && ! (NONJUMP_INSN_P (next_trial)
 		    && GET_CODE (PATTERN (next_trial)) == SEQUENCE)
@@ -2368,7 +2475,7 @@ fill_simple_delay_slots (int non_jumps_p)
 	      if (new_label != 0)
 		new_label = get_label_before (new_label);
 	      else
-		new_label = find_end_label ();
+		new_label = find_end_label (simple_return_rtx);
 
 	      if (new_label)
 	        {
@@ -2396,12 +2503,15 @@ fill_simple_delay_slots (int non_jumps_p)
 				    NULL, 1, 1,
 				    own_thread_p (JUMP_LABEL (insn),
 						  JUMP_LABEL (insn), 0),
+                                    0,
 				    slots_to_fill, &slots_filled,
 				    delay_list);
 
       if (delay_list)
 	unfilled_slots_base[i]
-	  = emit_delay_sequence (insn, delay_list, slots_filled);
+	  = emit_delay_sequence (insn, delay_list,
+                                 (slots_filled
+                                  + delay_list_has_label (delay_list)));
 
       if (slots_to_fill == slots_filled)
 	unfilled_slots_base[i] = 0;
@@ -2500,7 +2610,8 @@ fill_simple_delay_slots (int non_jumps_p)
 
 /* Follow any unconditional jump at LABEL;
    return the ultimate label reached by any such chain of jumps.
-   Return null if the chain ultimately leads to a return instruction.
+   Return a suitable return rtx if the chain ultimately leads to a
+   return instruction.
    If LABEL is not followed by a jump, return LABEL.
    If the chain loops or we can't find end, return LABEL,
    since that tells caller to avoid changing the insn.  */
@@ -2515,6 +2626,7 @@ follow_jumps (rtx label)
 
   for (depth = 0;
        (depth < 10
+	&& !ANY_RETURN_P (value)
 	&& (insn = next_active_insn (value)) != 0
 	&& JUMP_P (insn)
 	&& ((JUMP_LABEL (insn) != 0 && any_uncondjump_p (insn)
@@ -2524,24 +2636,50 @@ follow_jumps (rtx label)
 	&& BARRIER_P (next));
        depth++)
     {
-      rtx tem;
+      rtx this_label = JUMP_LABEL (insn);
 
       /* If we have found a cycle, make the insn jump to itself.  */
-      if (JUMP_LABEL (insn) == label)
+      if (this_label == label)
 	return label;
 
-      tem = next_active_insn (JUMP_LABEL (insn));
-      if (tem && (GET_CODE (PATTERN (tem)) == ADDR_VEC
+      if (!ANY_RETURN_P (this_label))
+	{
+	  rtx tem = next_active_insn (this_label);
+	  if (tem
+	      && (GET_CODE (PATTERN (tem)) == ADDR_VEC
 		  || GET_CODE (PATTERN (tem)) == ADDR_DIFF_VEC))
-	break;
+	    break;
+	}
 
-      value = JUMP_LABEL (insn);
+      value = this_label;
     }
   if (depth == 10)
     return label;
   return value;
 }
 
+/* Update LABEL_NUSES of labels in INSN and its notes with UPDATE.  */
+
+static void
+update_label_uses (rtx insn, int update)
+{
+  rtx note;
+
+  for (note = REG_NOTES (insn); note != NULL_RTX; note = XEXP (note, 1))
+    if (REG_NOTE_KIND (note) == REG_LABEL_OPERAND
+        || REG_NOTE_KIND (note) == REG_LABEL_TARGET)
+      {
+        /* REG_LABEL_OPERAND could be NOTE_INSN_DELETED_LABEL too.  */
+        if (LABEL_P (XEXP (note, 0)))
+          LABEL_NUSES (XEXP (note, 0)) += update;
+        else
+          gcc_assert (REG_NOTE_KIND (note) == REG_LABEL_OPERAND);
+      }
+
+  if (JUMP_P (insn) && JUMP_LABEL (insn) && LABEL_P (JUMP_LABEL (insn)))
+    LABEL_NUSES (JUMP_LABEL (insn)) += update;
+}
+
 /* Try to find insns to place in delay slots.
 
    INSN is the jump needing SLOTS_TO_FILL delay slots.  It tests CONDITION
@@ -2569,8 +2707,8 @@ follow_jumps (rtx label)
 static rtx
 fill_slots_from_thread (rtx insn, rtx condition, rtx thread,
 			rtx opposite_thread, int likely, int thread_if_true,
-			int own_thread, int slots_to_fill,
-			int *pslots_filled, rtx delay_list)
+			int own_thread, int own_opposite_thread,
+			int slots_to_fill, int *pslots_filled, rtx delay_list)
 {
   rtx new_thread;
   struct resources opposite_needed, set, needed;
@@ -2578,6 +2716,8 @@ fill_slots_from_thread (rtx insn, rtx condition, rtx thread,
   int lose = 0;
   int must_annul = 0;
   int flags;
+  rtx duplicate;
+  int align_insns = targetm.target_align.align_insns ();
 
   /* Validate our arguments.  */
   gcc_assert(condition != const_true_rtx || thread_if_true);
@@ -2622,6 +2762,8 @@ fill_slots_from_thread (rtx insn, rtx condition, rtx thread,
        trial = next_nonnote_insn (trial))
     {
       rtx pat, old_trial;
+      rtx label;
+      bool insert_label = false;
 
       /* If we have passed a label, we no longer own this thread.  */
       if (LABEL_P (trial))
@@ -2674,8 +2816,13 @@ fill_slots_from_thread (rtx insn, rtx condition, rtx thread,
 	      continue;
 	    }
 
-	  /* There are two ways we can win:  If TRIAL doesn't set anything
-	     needed at the opposite thread and can't trap, or if it can
+          label = prev_nonnote_insn (opposite_thread);
+          if (label != NULL_RTX && !LABEL_P (label))
+            label = NULL_RTX;
+
+	  /* There are three ways we can win:  If TRIAL doesn't set anything
+	     needed at the opposite thread and can't trap, or if it has a
+	     duplicate in the opposite thread and can't trap, or if it can
 	     go into an annulled delay slot.  */
 	  if (!must_annul
 	      && (condition == const_true_rtx
@@ -2692,6 +2839,38 @@ fill_slots_from_thread (rtx insn, rtx condition, rtx thread,
 	      if (eligible_for_delay (insn, *pslots_filled, trial, flags))
 		goto winner;
 	    }
+          /* In case TRIAL sets a reg needed at the opposite thread, it's
+             possible that the insn needing that reg is a duplicate of TRIAL, in
+             which case we can remove the duplicate in the fallthrough thread,
+             and use TRIAL for the delay slot.
+             A special case is if we don't own the fallthrough thread.  In that
+             case, we also need to move the label of the fallthrough thread into
+             the delay slot.  That is only safe, if the label aligment is not
+             bigger than the insn aligment.  Otherwise, the assembler might
+             insert a nop in the delay slot to guarantee the label alignment.
+          */
+	  else if (!must_annul
+                   && condition != const_true_rtx
+                   && insn_sets_resource_p (trial, &opposite_needed, true)
+                   && !may_trap_or_fault_p (pat)
+                   && thread_if_true
+                   && (own_opposite_thread ||
+                       (label != NULL_RTX && align_insns != 0
+                        && (label_to_alignment (label) <= align_insns)))
+                   && eligible_for_delay (insn, *pslots_filled, trial, flags)
+                   && has_opposite_duplicate (trial, opposite_thread,
+                                              &duplicate))
+            {
+              if (!own_opposite_thread)
+                insert_label = true;
+
+              update_block (duplicate, opposite_thread);
+              update_label_uses (duplicate, +1);
+              delete_related_insns (duplicate);
+              update_label_uses (duplicate, -1);
+
+              goto winner;
+            }
 	  else if (0
 #ifdef ANNUL_IFTRUE_SLOTS
 		   || ! thread_if_true
@@ -2730,8 +2909,6 @@ fill_slots_from_thread (rtx insn, rtx condition, rtx thread,
 		     starting point of this thread.  */
 		  if (own_thread)
 		    {
-		      rtx note;
-
 		      update_block (trial, thread);
 		      if (trial == thread)
 			{
@@ -2743,48 +2920,36 @@ fill_slots_from_thread (rtx insn, rtx condition, rtx thread,
 		      /* We are moving this insn, not deleting it.  We must
 			 temporarily increment the use count on any referenced
 			 label lest it be deleted by delete_related_insns.  */
-		      for (note = REG_NOTES (trial);
-			   note != NULL_RTX;
-			   note = XEXP (note, 1))
-			if (REG_NOTE_KIND (note) == REG_LABEL_OPERAND
-			    || REG_NOTE_KIND (note) == REG_LABEL_TARGET)
-			  {
-			    /* REG_LABEL_OPERAND could be
-			       NOTE_INSN_DELETED_LABEL too.  */
-			    if (LABEL_P (XEXP (note, 0)))
-			      LABEL_NUSES (XEXP (note, 0))++;
-			    else
-			      gcc_assert (REG_NOTE_KIND (note)
-					  == REG_LABEL_OPERAND);
-			  }
-		      if (JUMP_P (trial) && JUMP_LABEL (trial))
-			LABEL_NUSES (JUMP_LABEL (trial))++;
-
+		      update_label_uses (trial, +1);
 		      delete_related_insns (trial);
-
-		      for (note = REG_NOTES (trial);
-			   note != NULL_RTX;
-			   note = XEXP (note, 1))
-			if (REG_NOTE_KIND (note) == REG_LABEL_OPERAND
-			    || REG_NOTE_KIND (note) == REG_LABEL_TARGET)
-			  {
-			    /* REG_LABEL_OPERAND could be
-			       NOTE_INSN_DELETED_LABEL too.  */
-			    if (LABEL_P (XEXP (note, 0)))
-			      LABEL_NUSES (XEXP (note, 0))--;
-			    else
-			      gcc_assert (REG_NOTE_KIND (note)
-					  == REG_LABEL_OPERAND);
-			  }
-		      if (JUMP_P (trial) && JUMP_LABEL (trial))
-			LABEL_NUSES (JUMP_LABEL (trial))--;
+		      update_label_uses (trial, -1);
 		    }
 		  else
 		    new_thread = next_active_insn (trial);
 
-		  temp = own_thread ? trial : copy_rtx (trial);
-		  if (thread_if_true)
-		    INSN_FROM_TARGET_P (temp) = 1;
+                  if (insert_label)
+                    {
+                      remove_insn (label);
+                      INSN_DELETED_P (label) = 1;
+
+                      /* Add label to delay list.  */
+                      delay_list = add_to_delay_list (label, delay_list);
+
+                      /* If !own_thread and we use copy_rtx (trial) here, the
+                         caching mechanism of mark_target_live_regs gets
+                         confused.  It assumes that each target uid has a unique
+                         bb.  If trial is a target, and we add the copy after
+                         the label in the delay slot, the copy is a new target
+                         with the same uid, but in a different bb.  Instead we
+                         use duplicate, also if own_thread.  */
+                      temp = duplicate;
+                    }
+                  else
+                    {
+                      temp = own_thread ? trial : copy_rtx (trial);
+                      if (thread_if_true)
+                        INSN_FROM_TARGET_P (temp) = 1;
+                    }
 
 		  delay_list = add_to_delay_list (temp, delay_list);
 
@@ -2874,7 +3039,8 @@ fill_slots_from_thread (rtx insn, rtx condition, rtx thread,
 	  delay_list
 	    = steal_delay_list_from_target (insn, condition, PATTERN (trial),
 					    delay_list, &set, &needed,
-					    &opposite_needed, slots_to_fill,
+					    &opposite_needed, opposite_thread,
+					    own_opposite_thread, slots_to_fill,
 					    pslots_filled, &must_annul,
 					    &new_thread);
 	  /* If we owned the thread and are told that it branched
@@ -2898,6 +3064,7 @@ fill_slots_from_thread (rtx insn, rtx condition, rtx thread,
      arithmetic insn after the jump insn and put the arithmetic insn in the
      delay slot.  If we can't do this, return.  */
   if (delay_list == 0 && likely && new_thread
+      && !ANY_RETURN_P (new_thread)
       && NONJUMP_INSN_P (new_thread)
       && GET_CODE (PATTERN (new_thread)) != ASM_INPUT
       && asm_noperands (PATTERN (new_thread)) < 0)
@@ -2982,16 +3149,14 @@ fill_slots_from_thread (rtx insn, rtx condition, rtx thread,
 
       gcc_assert (thread_if_true);
 
-      if (new_thread && JUMP_P (new_thread)
-	  && (simplejump_p (new_thread)
-	      || GET_CODE (PATTERN (new_thread)) == RETURN)
+      if (new_thread && simplejump_or_return_p (new_thread)
 	  && redirect_with_delay_list_safe_p (insn,
 					      JUMP_LABEL (new_thread),
 					      delay_list))
 	new_thread = follow_jumps (JUMP_LABEL (new_thread));
 
-      if (new_thread == 0)
-	label = find_end_label ();
+      if (ANY_RETURN_P (new_thread))
+	label = find_end_label (new_thread);
       else if (LABEL_P (new_thread))
 	label = new_thread;
       else
@@ -3088,7 +3253,7 @@ fill_eager_delay_slots (void)
 	  delay_list
 	    = fill_slots_from_thread (insn, condition, insn_at_target,
 				      fallthrough_insn, prediction == 2, 1,
-				      own_target,
+				      own_target, own_fallthrough,
 				      slots_to_fill, &slots_filled, delay_list);
 
 	  if (delay_list == 0 && own_fallthrough)
@@ -3096,14 +3261,15 @@ fill_eager_delay_slots (void)
 	      /* Even though we didn't find anything for delay slots,
 		 we might have found a redundant insn which we deleted
 		 from the thread that was filled.  So we have to recompute
-		 the next insn at the target.  */
+		 (a) the insn at the target, and (b) whether we own it.  */
 	      target_label = JUMP_LABEL (insn);
 	      insn_at_target = next_active_insn (target_label);
+	      own_target = own_thread_p (target_label, target_label, 0);
 
 	      delay_list
 		= fill_slots_from_thread (insn, condition, fallthrough_insn,
 					  insn_at_target, 0, 0,
-					  own_fallthrough,
+					  own_fallthrough, own_target,
 					  slots_to_fill, &slots_filled,
 					  delay_list);
 	    }
@@ -3114,22 +3280,32 @@ fill_eager_delay_slots (void)
 	    delay_list
 	      = fill_slots_from_thread (insn, condition, fallthrough_insn,
 					insn_at_target, 0, 0,
-					own_fallthrough,
+					own_fallthrough, own_target,
 					slots_to_fill, &slots_filled,
 					delay_list);
 
 	  if (delay_list == 0)
-	    delay_list
-	      = fill_slots_from_thread (insn, condition, insn_at_target,
-					next_active_insn (insn), 0, 1,
-					own_target,
-					slots_to_fill, &slots_filled,
-					delay_list);
+	    {
+	      /* In case we found a redundant insn which we deleted from the
+		 fallthrough thread, we have to recompute (a) the insn at the
+		 fallthrough, and (b) whether we own it.  */
+	      fallthrough_insn = next_active_insn (insn);
+	      own_fallthrough = own_thread_p (NEXT_INSN (insn), NULL_RTX, 1);
+
+	      delay_list
+		= fill_slots_from_thread (insn, condition, insn_at_target,
+					  next_active_insn (insn), 0, 1,
+					  own_target, own_fallthrough,
+					  slots_to_fill, &slots_filled,
+					  delay_list);
+	    }
 	}
 
       if (delay_list)
 	unfilled_slots_base[i]
-	  = emit_delay_sequence (insn, delay_list, slots_filled);
+	  = emit_delay_sequence (insn, delay_list,
+                                 (slots_filled
+                                  + delay_list_has_label (delay_list)));
 
       if (slots_to_fill == slots_filled)
 	unfilled_slots_base[i] = 0;
@@ -3314,6 +3490,19 @@ delete_jump (rtx insn)
     delete_computation (insn);
 }
 
+/* Returns first real insn in SEQ.  */
+
+static rtx
+first_real_insn_in_seq (rtx seq)
+{
+  rtx pat = PATTERN (seq);
+  rtx first = XVECEXP (pat, 0, 1);
+  if (DELETED_NOTE_P (first) || LABEL_P (first))
+    first = XVECEXP (pat, 0, 2);
+  gcc_assert (INSN_P (first));
+  return first;
+}
+
 
 /* Once we have tried two ways to fill a delay slot, make a pass over the
    code to try to improve the results and to do such things as more jump
@@ -3337,11 +3526,12 @@ relax_delay_slots (rtx first)
 	 group of consecutive labels.  */
       if (JUMP_P (insn)
 	  && (condjump_p (insn) || condjump_in_parallel_p (insn))
-	  && (target_label = JUMP_LABEL (insn)) != 0)
+	  && (target_label = JUMP_LABEL (insn)) != 0
+	  && !ANY_RETURN_P (target_label))
 	{
 	  target_label = skip_consecutive_labels (follow_jumps (target_label));
-	  if (target_label == 0)
-	    target_label = find_end_label ();
+	  if (ANY_RETURN_P (target_label))
+	    target_label = find_end_label (target_label);
 
 	  if (target_label && next_active_insn (target_label) == next
 	      && ! condjump_in_parallel_p (insn))
@@ -3356,9 +3546,8 @@ relax_delay_slots (rtx first)
 	  /* See if this jump conditionally branches around an unconditional
 	     jump.  If so, invert this jump and point it to the target of the
 	     second jump.  */
-	  if (next && JUMP_P (next)
+	  if (next && simplejump_or_return_p (next)
 	      && any_condjump_p (insn)
-	      && (simplejump_p (next) || GET_CODE (PATTERN (next)) == RETURN)
 	      && target_label
 	      && next_active_insn (target_label) == next_active_insn (next)
 	      && no_labels_between_p (insn, next))
@@ -3373,7 +3562,7 @@ relax_delay_slots (rtx first)
 		 invert_jump fails.  */
 
 	      ++LABEL_NUSES (target_label);
-	      if (label)
+	      if (label && LABEL_P (label))
 		++LABEL_NUSES (label);
 
 	      if (invert_jump (insn, label, 1))
@@ -3382,7 +3571,7 @@ relax_delay_slots (rtx first)
 		  next = insn;
 		}
 
-	      if (label)
+	      if (label && LABEL_P (label))
 		--LABEL_NUSES (label);
 
 	      if (--LABEL_NUSES (target_label) == 0)
@@ -3400,8 +3589,7 @@ relax_delay_slots (rtx first)
 	 Don't do this if we expect the conditional branch to be true, because
 	 we would then be making the more common case longer.  */
 
-      if (JUMP_P (insn)
-	  && (simplejump_p (insn) || GET_CODE (PATTERN (insn)) == RETURN)
+      if (simplejump_or_return_p (insn)
 	  && (other = prev_active_insn (insn)) != 0
 	  && any_condjump_p (other)
 	  && no_labels_between_p (other, insn)
@@ -3424,6 +3612,17 @@ relax_delay_slots (rtx first)
       pat = PATTERN (insn);
       delay_insn = XVECEXP (pat, 0, 0);
 
+      /* Removed unused label from delay slot.  */
+      if (LABEL_P (XVECEXP (pat, 0, 1)))
+        {
+          if (LABEL_NUSES (XVECEXP (pat, 0, 1)) == 0)
+            {
+              delete_from_delay_slot (XVECEXP (pat, 0, 1));
+              next = prev_active_insn (next);
+            }
+          continue;
+        }
+
       /* See if the first insn in the delay slot is redundant with some
 	 previous insn.  Remove it from the delay slot if so; then set up
 	 to reprocess this insn.  */
@@ -3442,10 +3641,10 @@ relax_delay_slots (rtx first)
 	 Only do so if optimizing for size since this results in slower, but
 	 smaller code.  */
       if (optimize_function_for_size_p (cfun)
-	  && GET_CODE (PATTERN (delay_insn)) == RETURN
+	  && ANY_RETURN_P (PATTERN (delay_insn))
 	  && next
 	  && JUMP_P (next)
-	  && GET_CODE (PATTERN (next)) == RETURN)
+	  && PATTERN (next) == PATTERN (delay_insn))
 	{
 	  rtx after;
 	  int i;
@@ -3484,14 +3683,16 @@ relax_delay_slots (rtx first)
 	continue;
 
       target_label = JUMP_LABEL (delay_insn);
+      if (target_label && ANY_RETURN_P (target_label))
+	continue;
 
       if (target_label)
 	{
 	  /* If this jump goes to another unconditional jump, thread it, but
 	     don't convert a jump into a RETURN here.  */
 	  trial = skip_consecutive_labels (follow_jumps (target_label));
-	  if (trial == 0)
-	    trial = find_end_label ();
+	  if (ANY_RETURN_P (trial))
+	    trial = find_end_label (trial);
 
 	  if (trial && trial != target_label
 	      && redirect_with_delay_slots_safe_p (delay_insn, trial, insn))
@@ -3514,7 +3715,7 @@ relax_delay_slots (rtx first)
 		 later incorrectly compute register live/death info.  */
 	      rtx tmp = next_active_insn (trial);
 	      if (tmp == 0)
-		tmp = find_end_label ();
+		tmp = find_end_label (simple_return_rtx);
 
 	      if (tmp)
 	        {
@@ -3534,14 +3735,12 @@ relax_delay_slots (rtx first)
 	     delay list and that insn is redundant, thread the jump.  */
 	  if (trial && GET_CODE (PATTERN (trial)) == SEQUENCE
 	      && XVECLEN (PATTERN (trial), 0) == 2
-	      && JUMP_P (XVECEXP (PATTERN (trial), 0, 0))
-	      && (simplejump_p (XVECEXP (PATTERN (trial), 0, 0))
-		  || GET_CODE (PATTERN (XVECEXP (PATTERN (trial), 0, 0))) == RETURN)
+	      && simplejump_or_return_p (XVECEXP (PATTERN (trial), 0, 0))
 	      && redundant_insn (XVECEXP (PATTERN (trial), 0, 1), insn, 0))
 	    {
 	      target_label = JUMP_LABEL (XVECEXP (PATTERN (trial), 0, 0));
-	      if (target_label == 0)
-		target_label = find_end_label ();
+	      if (ANY_RETURN_P (target_label))
+		target_label = find_end_label (target_label);
 
 	      if (target_label
 	          && redirect_with_delay_slots_safe_p (delay_insn, target_label,
@@ -3621,16 +3820,15 @@ relax_delay_slots (rtx first)
 	 a RETURN here.  */
       if (! INSN_ANNULLED_BRANCH_P (delay_insn)
 	  && any_condjump_p (delay_insn)
-	  && next && JUMP_P (next)
-	  && (simplejump_p (next) || GET_CODE (PATTERN (next)) == RETURN)
+	  && next && simplejump_or_return_p (next)
 	  && next_active_insn (target_label) == next_active_insn (next)
 	  && no_labels_between_p (insn, next))
 	{
 	  rtx label = JUMP_LABEL (next);
 	  rtx old_label = JUMP_LABEL (delay_insn);
 
-	  if (label == 0)
-	    label = find_end_label ();
+	  if (ANY_RETURN_P (label))
+	    label = find_end_label (label);
 
 	  /* find_end_label can generate a new label. Check this first.  */
 	  if (label
@@ -3668,10 +3866,10 @@ relax_delay_slots (rtx first)
 
       /* If we own the thread opposite the way this insn branches, see if we
 	 can merge its delay slots with following insns.  */
-      if (INSN_FROM_TARGET_P (XVECEXP (pat, 0, 1))
+      if (INSN_FROM_TARGET_P (first_real_insn_in_seq (insn))
 	  && own_thread_p (NEXT_INSN (insn), 0, 1))
 	try_merge_delay_insns (insn, next);
-      else if (! INSN_FROM_TARGET_P (XVECEXP (pat, 0, 1))
+      else if (! INSN_FROM_TARGET_P (first_real_insn_in_seq (insn))
 	       && own_thread_p (target_label, target_label, 0))
 	try_merge_delay_insns (insn, next_active_insn (target_label));
 
@@ -3691,7 +3889,8 @@ static void
 make_return_insns (rtx first)
 {
   rtx insn, jump_insn, pat;
-  rtx real_return_label = end_of_function_label;
+  rtx real_return_label = function_return_label;
+  rtx real_simple_return_label = function_simple_return_label;
   int slots, i;
 
 #ifdef DELAY_SLOTS_FOR_EPILOGUE
@@ -3709,15 +3908,22 @@ make_return_insns (rtx first)
      made for END_OF_FUNCTION_LABEL.  If so, set up anything we can't change
      into a RETURN to jump to it.  */
   for (insn = first; insn; insn = NEXT_INSN (insn))
-    if (JUMP_P (insn) && GET_CODE (PATTERN (insn)) == RETURN)
+    if (JUMP_P (insn) && ANY_RETURN_P (PATTERN (insn)))
       {
-	real_return_label = get_label_before (insn);
+	rtx t = get_label_before (insn);
+	if (PATTERN (insn) == ret_rtx)
+	  real_return_label = t;
+	else
+	  real_simple_return_label = t;
 	break;
       }
 
   /* Show an extra usage of REAL_RETURN_LABEL so it won't go away if it
      was equal to END_OF_FUNCTION_LABEL.  */
-  LABEL_NUSES (real_return_label)++;
+  if (real_return_label)
+    LABEL_NUSES (real_return_label)++;
+  if (real_simple_return_label)
+    LABEL_NUSES (real_simple_return_label)++;
 
   /* Clear the list of insns to fill so we can use it.  */
   obstack_free (&unfilled_slots_obstack, unfilled_firstobj);
@@ -3725,13 +3931,27 @@ make_return_insns (rtx first)
   for (insn = first; insn; insn = NEXT_INSN (insn))
     {
       int flags;
+      rtx kind, real_label;
 
       /* Only look at filled JUMP_INSNs that go to the end of function
 	 label.  */
       if (!NONJUMP_INSN_P (insn)
 	  || GET_CODE (PATTERN (insn)) != SEQUENCE
-	  || !JUMP_P (XVECEXP (PATTERN (insn), 0, 0))
-	  || JUMP_LABEL (XVECEXP (PATTERN (insn), 0, 0)) != end_of_function_label)
+	  || !JUMP_P (XVECEXP (PATTERN (insn), 0, 0)))
+	continue;
+
+      if (JUMP_LABEL (XVECEXP (PATTERN (insn), 0, 0)) == function_return_label)
+	{
+	  kind = ret_rtx;
+	  real_label = real_return_label;
+	}
+      else if (JUMP_LABEL (XVECEXP (PATTERN (insn), 0, 0))
+	       == function_simple_return_label)
+	{
+	  kind = simple_return_rtx;
+	  real_label = real_simple_return_label;
+	}
+      else
 	continue;
 
       pat = PATTERN (insn);
@@ -3739,14 +3959,12 @@ make_return_insns (rtx first)
 
       /* If we can't make the jump into a RETURN, try to redirect it to the best
 	 RETURN and go on to the next insn.  */
-      if (! reorg_redirect_jump (jump_insn, NULL_RTX))
+      if (! reorg_redirect_jump (jump_insn, kind))
 	{
 	  /* Make sure redirecting the jump will not invalidate the delay
 	     slot insns.  */
-	  if (redirect_with_delay_slots_safe_p (jump_insn,
-						real_return_label,
-						insn))
-	    reorg_redirect_jump (jump_insn, real_return_label);
+	  if (redirect_with_delay_slots_safe_p (jump_insn, real_label, insn))
+	    reorg_redirect_jump (jump_insn, real_label);
 	  continue;
 	}
 
@@ -3786,7 +4004,7 @@ make_return_insns (rtx first)
 	 RETURN, delete the SEQUENCE and output the individual insns,
 	 followed by the RETURN.  Then set things up so we try to find
 	 insns for its delay slots, if it needs some.  */
-      if (GET_CODE (PATTERN (jump_insn)) == RETURN)
+      if (ANY_RETURN_P (PATTERN (jump_insn)))
 	{
 	  rtx prev = PREV_INSN (insn);
 
@@ -3803,13 +4021,16 @@ make_return_insns (rtx first)
       else
 	/* It is probably more efficient to keep this with its current
 	   delay slot as a branch to a RETURN.  */
-	reorg_redirect_jump (jump_insn, real_return_label);
+	reorg_redirect_jump (jump_insn, real_label);
     }
 
   /* Now delete REAL_RETURN_LABEL if we never used it.  Then try to fill any
      new delay slots we have created.  */
-  if (--LABEL_NUSES (real_return_label) == 0)
+  if (real_return_label != NULL_RTX && --LABEL_NUSES (real_return_label) == 0)
     delete_related_insns (real_return_label);
+  if (real_simple_return_label != NULL_RTX
+      && --LABEL_NUSES (real_simple_return_label) == 0)
+    delete_related_insns (real_simple_return_label);
 
   fill_simple_delay_slots (1);
   fill_simple_delay_slots (0);
@@ -3877,7 +4098,7 @@ dbr_schedule (rtx first)
   init_resource_info (epilogue_insn);
 
   /* Show we haven't computed an end-of-function label yet.  */
-  end_of_function_label = 0;
+  function_return_label = function_simple_return_label = NULL_RTX;
 
   /* Initialize the statistics for this function.  */
   memset (num_insns_needing_delays, 0, sizeof num_insns_needing_delays);
@@ -3899,11 +4120,23 @@ dbr_schedule (rtx first)
   /* If we made an end of function label, indicate that it is now
      safe to delete it by undoing our prior adjustment to LABEL_NUSES.
      If it is now unused, delete it.  */
-  if (end_of_function_label && --LABEL_NUSES (end_of_function_label) == 0)
-    delete_related_insns (end_of_function_label);
-
+  if (function_return_label && --LABEL_NUSES (function_return_label) == 0)
+    delete_related_insns (function_return_label);
+  if (function_simple_return_label
+      && --LABEL_NUSES (function_simple_return_label) == 0)
+    delete_related_insns (function_simple_return_label);
+
+#if defined HAVE_return || defined HAVE_simple_return
+  if (
 #ifdef HAVE_return
-  if (HAVE_return && end_of_function_label != 0)
+      (HAVE_return && function_return_label != 0)
+#else
+      0
+#endif
+#ifdef HAVE_simple_return
+      || (HAVE_simple_return && function_simple_return_label != 0)
+#endif
+      )
     make_return_insns (first);
 #endif
 
diff --git a/gcc/resource.c b/gcc/resource.c
index 1ee87c0..33e8048 100644
--- a/gcc/resource.c
+++ b/gcc/resource.c
@@ -47,6 +47,7 @@ struct target_info
   struct target_info *next;	/* Next info for same hash bucket.  */
   HARD_REG_SET live_regs;	/* Registers live at target.  */
   int block;			/* Basic block number containing target.  */
+  rtx label;			/* Label corresponding to block.  */
   int bb_tick;			/* Generation count of basic block info.  */
 };
 
@@ -79,7 +80,7 @@ static HARD_REG_SET current_live_regs;
 static HARD_REG_SET pending_dead_regs;
 
 static void update_live_status (rtx, const_rtx, void *);
-static int find_basic_block (rtx, int);
+static int find_basic_block (rtx, int, rtx *);
 static rtx next_insn_no_annul (rtx);
 static rtx find_dead_or_set_registers (rtx, struct resources*,
 				       rtx*, int, struct resources,
@@ -133,8 +134,11 @@ update_live_status (rtx dest, const_rtx x, void *data ATTRIBUTE_UNUSED)
    correct.  */
 
 static int
-find_basic_block (rtx insn, int search_limit)
+find_basic_block (rtx insn, int search_limit, rtx *label)
 {
+  if (label)
+    *label = NULL_RTX;
+
   /* Scan backwards to the previous BARRIER.  Then see if we can find a
      label that starts a basic block.  Return the basic block number.  */
   for (insn = prev_nonnote_insn (insn);
@@ -156,7 +160,11 @@ find_basic_block (rtx insn, int search_limit)
        insn && LABEL_P (insn);
        insn = next_nonnote_insn (insn))
     if (BLOCK_FOR_INSN (insn))
-      return BLOCK_FOR_INSN (insn)->index;
+      {
+        if (label)
+          *label = insn;
+        return BLOCK_FOR_INSN (insn)->index;
+      }
 
   return -1;
 }
@@ -495,6 +503,8 @@ find_dead_or_set_registers (rtx target, struct resources *res,
 		  || GET_CODE (PATTERN (this_jump_insn)) == RETURN)
 		{
 		  next = JUMP_LABEL (this_jump_insn);
+		  if (next && ANY_RETURN_P (next))
+		    next = NULL_RTX;
 		  if (jump_insn == 0)
 		    {
 		      jump_insn = insn;
@@ -562,9 +572,10 @@ find_dead_or_set_registers (rtx target, struct resources *res,
 		  AND_COMPL_HARD_REG_SET (scratch, needed.regs);
 		  AND_COMPL_HARD_REG_SET (fallthrough_res.regs, scratch);
 
-		  find_dead_or_set_registers (JUMP_LABEL (this_jump_insn),
-					      &target_res, 0, jump_count,
-					      target_set, needed);
+		  if (!ANY_RETURN_P (JUMP_LABEL (this_jump_insn)))
+		    find_dead_or_set_registers (JUMP_LABEL (this_jump_insn),
+						&target_res, 0, jump_count,
+						target_set, needed);
 		  find_dead_or_set_registers (next,
 					      &fallthrough_res, 0, jump_count,
 					      set, needed);
@@ -653,10 +664,12 @@ mark_set_resources (rtx x, struct resources *res, int in_dest,
       if (mark_type == MARK_SRC_DEST_CALL)
 	{
 	  rtx link;
+	  HARD_REG_SET regs;
 
 	  res->cc = res->memory = 1;
 
-	  IOR_HARD_REG_SET (res->regs, regs_invalidated_by_call);
+	  get_call_reg_set_usage (x, &regs, regs_invalidated_by_call);
+	  IOR_HARD_REG_SET (res->regs, regs);
 
 	  for (link = CALL_INSN_FUNCTION_USAGE (x);
 	       link; link = XEXP (link, 1))
@@ -826,7 +839,39 @@ return_insn_p (const_rtx insn)
 
   return false;
 }
+
+/* Get the block field from TINFO, if valid.  */
+
+static int
+get_block (struct target_info *tinfo)
+{
+  int b, check_b;
+  rtx label, check_label;
+
+  b = tinfo->block;
+
+  /* Invalid block.  */
+  if (b == -1 || INSN_DELETED_P (BB_HEAD (BASIC_BLOCK (b))))
+    return -1;
+  
+  label = tinfo->label;
+
+  /* Barrier is start of function.  */
+  if (label == NULL_RTX)
+    return b;
 
+  /* Check if label is still valid.  */
+  if (!LABEL_P (label) || INSN_DELETED_P (label))
+    return -1;
+
+  /* Check if find_basic_block still finds the same label.  */
+  check_b = find_basic_block (label, MAX_DELAY_SLOT_LIVE_SEARCH, &check_label);
+  if (b != check_b || label != check_label)
+    return -1;
+
+  return b;
+}
+
 /* Set the resources that are live at TARGET.
 
    If TARGET is zero, we refer to the end of the current function and can
@@ -873,7 +918,7 @@ mark_target_live_regs (rtx insns, rtx target, struct resources *res)
   struct target_info *tinfo = NULL;
   rtx insn;
   rtx jump_insn = 0;
-  rtx jump_target;
+  rtx jump_target, label;
   HARD_REG_SET scratch;
   struct resources set, needed;
 
@@ -906,15 +951,18 @@ mark_target_live_regs (rtx insns, rtx target, struct resources *res)
 	  break;
 
       /* Start by getting the basic block number.  If we have saved
-	 information, we can get it from there unless the insn at the
-	 start of the basic block has been deleted.  */
-      if (tinfo && tinfo->block != -1
-	  && ! INSN_DELETED_P (BB_HEAD (BASIC_BLOCK (tinfo->block))))
-	b = tinfo->block;
+	 information, we can get it from there if unless the information is not
+         valid anymore.  */
+      if (tinfo)
+        {
+          b = get_block (tinfo);
+          if (b == -1 && tinfo->block != -1)
+            tinfo->block = -1;
+        }
     }
 
   if (b == -1)
-    b = find_basic_block (target, MAX_DELAY_SLOT_LIVE_SEARCH);
+    b = find_basic_block (target, MAX_DELAY_SLOT_LIVE_SEARCH, &label);
 
   if (target_hash_table != NULL)
     {
@@ -935,6 +983,7 @@ mark_target_live_regs (rtx insns, rtx target, struct resources *res)
 	  tinfo = XNEW (struct target_info);
 	  tinfo->uid = INSN_UID (target);
 	  tinfo->block = b;
+	  tinfo->label = label;
 	  tinfo->next
 	    = target_hash_table[INSN_UID (target) % TARGET_HASH_PRIME];
 	  target_hash_table[INSN_UID (target) % TARGET_HASH_PRIME] = tinfo;
@@ -994,11 +1043,16 @@ mark_target_live_regs (rtx insns, rtx target, struct resources *res)
 
 	  if (CALL_P (real_insn))
 	    {
+	      HARD_REG_SET regs_invalidated_by_this_call;
 	      /* CALL clobbers all call-used regs that aren't fixed except
 		 sp, ap, and fp.  Do this before setting the result of the
 		 call live.  */
-	      AND_COMPL_HARD_REG_SET (current_live_regs,
+
+	      get_call_reg_set_usage (real_insn,
+				      &regs_invalidated_by_this_call,
 				      regs_invalidated_by_call);
+	      AND_COMPL_HARD_REG_SET (current_live_regs,
+				      regs_invalidated_by_this_call);
 
 	      /* A CALL_INSN sets any global register live, since it may
 		 have been modified by the call.  */
@@ -1072,6 +1126,8 @@ mark_target_live_regs (rtx insns, rtx target, struct resources *res)
       COPY_HARD_REG_SET (res->regs, current_live_regs);
       if (tinfo != NULL)
 	{
+	  if (tinfo->block != b)
+	    tinfo->label = label;
 	  tinfo->block = b;
 	  tinfo->bb_tick = bb_ticks[b];
 	}
@@ -1097,6 +1153,8 @@ mark_target_live_regs (rtx insns, rtx target, struct resources *res)
       struct resources new_resources;
       rtx stop_insn = next_active_insn (jump_insn);
 
+      if (jump_target && ANY_RETURN_P (jump_target))
+	jump_target = NULL_RTX;
       mark_target_live_regs (insns, next_active_insn (jump_target),
 			     &new_resources);
       CLEAR_RESOURCE (&set);
@@ -1269,7 +1327,7 @@ clear_hashed_info_for_insn (rtx insn)
 void
 incr_ticks_for_insn (rtx insn)
 {
-  int b = find_basic_block (insn, MAX_DELAY_SLOT_LIVE_SEARCH);
+  int b = find_basic_block (insn, MAX_DELAY_SLOT_LIVE_SEARCH, NULL);
 
   if (b != -1)
     bb_ticks[b]++;
diff --git a/gcc/rtl.c b/gcc/rtl.c
index c6db47f..5d77d7a 100644
--- a/gcc/rtl.c
+++ b/gcc/rtl.c
@@ -255,6 +255,8 @@ copy_rtx (rtx orig)
     case CODE_LABEL:
     case PC:
     case CC0:
+    case RETURN:
+    case SIMPLE_RETURN:
     case SCRATCH:
       /* SCRATCH must be shared because they represent distinct values.  */
       return orig;
diff --git a/gcc/rtl.def b/gcc/rtl.def
index 6e2aa8b..956eb2e 100644
--- a/gcc/rtl.def
+++ b/gcc/rtl.def
@@ -296,6 +296,10 @@ DEF_RTL_EXPR(CALL, "call", "ee", RTX_EXTRA)
 
 DEF_RTL_EXPR(RETURN, "return", "", RTX_EXTRA)
 
+/* A plain return, to be used on paths that are reached without going
+   through the function prologue.  */
+DEF_RTL_EXPR(SIMPLE_RETURN, "simple_return", "", RTX_EXTRA)
+
 /* Special for EH return from subroutine.  */
 
 DEF_RTL_EXPR(EH_RETURN, "eh_return", "", RTX_EXTRA)
diff --git a/gcc/rtl.h b/gcc/rtl.h
index 66f2755..37bfa23 100644
--- a/gcc/rtl.h
+++ b/gcc/rtl.h
@@ -412,6 +412,10 @@ struct GTY((variable_size)) rtvec_def {
   (JUMP_P (INSN) && (GET_CODE (PATTERN (INSN)) == ADDR_VEC || \
 		     GET_CODE (PATTERN (INSN)) == ADDR_DIFF_VEC))
 
+/* Predicate yielding nonzero iff X is a return or simple_preturn.  */
+#define ANY_RETURN_P(X) \
+  (GET_CODE (X) == RETURN || GET_CODE (X) == SIMPLE_RETURN)
+
 /* 1 if X is a unary operator.  */
 
 #define UNARY_P(X)   \
@@ -971,6 +975,11 @@ extern const char * const note_insn_name[NOTE_INSN_MAX];
 #define GET_NOTE_INSN_NAME(NOTE_CODE) \
   (note_insn_name[(NOTE_CODE)])
 
+/* Predicate yielding nonzero iff X is a deleted insn note.  */
+#define DELETED_NOTE_P(X)					\
+  (NOTE_P (X) && (NOTE_KIND (X) == NOTE_INSN_DELETED		\
+		  || NOTE_KIND (X) == NOTE_INSN_DELETED_LABEL))
+
 /* The name of a label, in case it corresponds to an explicit label
    in the input source code.  */
 #define LABEL_NAME(RTX) XCSTR (RTX, 7, CODE_LABEL)
@@ -1862,6 +1871,11 @@ extern rtx find_last_value (rtx, rtx *, rtx, int);
 extern int refers_to_regno_p (unsigned int, unsigned int, const_rtx, rtx *);
 extern int reg_overlap_mentioned_p (const_rtx, const_rtx);
 extern const_rtx set_of (const_rtx, const_rtx);
+extern void record_hard_reg_sets (rtx, const_rtx, void *);
+extern void record_hard_reg_uses (rtx *, void *);
+#ifdef HARD_CONST
+extern void find_all_hard_reg_sets (const_rtx, HARD_REG_SET *);
+#endif
 extern void note_stores (const_rtx, void (*) (rtx, const_rtx, void *), void *);
 extern void note_uses (rtx *, void (*) (rtx *, void *), void *);
 extern int dead_or_set_p (const_rtx, const_rtx);
@@ -1957,12 +1971,14 @@ extern void subreg_get_info (unsigned int, enum machine_mode,
 
 /* lists.c */
 
-extern void free_EXPR_LIST_list		(rtx *);
-extern void free_INSN_LIST_list		(rtx *);
-extern void free_EXPR_LIST_node		(rtx);
-extern void free_INSN_LIST_node		(rtx);
-extern rtx alloc_INSN_LIST			(rtx, rtx);
-extern rtx alloc_EXPR_LIST			(int, rtx, rtx);
+extern void free_EXPR_LIST_list (rtx *);
+extern void free_INSN_LIST_list (rtx *);
+extern void free_EXPR_LIST_node (rtx);
+extern void free_INSN_LIST_node (rtx);
+extern rtx alloc_INSN_LIST (rtx, rtx);
+extern rtx copy_INSN_LIST (rtx);
+extern rtx concat_INSN_LIST (rtx, rtx);
+extern rtx alloc_EXPR_LIST (int, rtx, rtx);
 extern void remove_free_INSN_LIST_elem (rtx, rtx *);
 extern rtx remove_list_elem (rtx, rtx *);
 extern rtx remove_free_INSN_LIST_node (rtx *);
@@ -2041,6 +2057,8 @@ enum global_rtl_index
 {
   GR_PC,
   GR_CC0,
+  GR_RETURN,
+  GR_SIMPLE_RETURN,
   GR_STACK_POINTER,
   GR_FRAME_POINTER,
 /* For register elimination to work properly these hard_frame_pointer_rtx,
@@ -2130,6 +2148,8 @@ extern struct target_rtl *this_target_rtl;
 
 /* Standard pieces of rtx, to be substituted directly into things.  */
 #define pc_rtx                  (global_rtl[GR_PC])
+#define ret_rtx                 (global_rtl[GR_RETURN])
+#define simple_return_rtx       (global_rtl[GR_SIMPLE_RETURN])
 #define cc0_rtx                 (global_rtl[GR_CC0])
 
 /* All references to certain hard regs, except those created
diff --git a/gcc/rtlanal.c b/gcc/rtlanal.c
index d9710bd..9c06d4e 100644
--- a/gcc/rtlanal.c
+++ b/gcc/rtlanal.c
@@ -999,6 +999,56 @@ set_of (const_rtx pat, const_rtx insn)
   note_stores (INSN_P (insn) ? PATTERN (insn) : insn, set_of_1, &data);
   return data.found;
 }
+
+/* This function, called through note_stores, collects sets and
+   clobbers of hard registers in a HARD_REG_SET, which is pointed to
+   by DATA.  */
+void
+record_hard_reg_sets (rtx x, const_rtx pat ATTRIBUTE_UNUSED, void *data)
+{
+  HARD_REG_SET *pset = (HARD_REG_SET *)data;
+  if (REG_P (x) && HARD_REGISTER_P (x))
+    add_to_hard_reg_set (pset, GET_MODE (x), REGNO (x));
+}
+
+/* Examine INSN, and compute the set of hard registers written by it.
+   Store it in *PSET.  Should only be called after reload.  */
+void
+find_all_hard_reg_sets (const_rtx insn, HARD_REG_SET *pset)
+{
+  rtx link;
+
+  CLEAR_HARD_REG_SET (*pset);
+  note_stores (PATTERN (insn), record_hard_reg_sets, pset);
+  if (CALL_P (insn))
+    IOR_HARD_REG_SET (*pset, call_used_reg_set);
+  for (link = REG_NOTES (insn); link; link = XEXP (link, 1))
+    if (REG_NOTE_KIND (link) == REG_INC)
+      record_hard_reg_sets (XEXP (link, 0), NULL, pset);
+}
+
+/* A for_each_rtx subroutine of record_hard_reg_uses.  */
+static int
+record_hard_reg_uses_1 (rtx *px, void *data)
+{
+  rtx x = *px;
+  HARD_REG_SET *pused = (HARD_REG_SET *)data;
+
+  if (REG_P (x) && REGNO (x) < FIRST_PSEUDO_REGISTER)
+    {
+      int nregs = hard_regno_nregs[REGNO (x)][GET_MODE (x)];
+      while (nregs-- > 0)
+	SET_HARD_REG_BIT (*pused, REGNO (x) + nregs);
+    }
+  return 0;
+}
+
+/* Like record_hard_reg_sets, but called through note_uses.  */
+void
+record_hard_reg_uses (rtx *px, void *data)
+{
+  for_each_rtx (px, record_hard_reg_uses_1, data);
+}
 
 /* Given an INSN, return a SET expression if this insn has only a single SET.
    It may also have CLOBBERs, USEs, or SET whose output
@@ -2662,6 +2712,7 @@ tablejump_p (const_rtx insn, rtx *labelp, rtx *tablep)
 
   if (JUMP_P (insn)
       && (label = JUMP_LABEL (insn)) != NULL_RTX
+      && !ANY_RETURN_P (label)
       && (table = next_active_insn (label)) != NULL_RTX
       && JUMP_TABLE_DATA_P (table))
     {
diff --git a/gcc/sched-deps.c b/gcc/sched-deps.c
index 50494cf..7bdd12c 100644
--- a/gcc/sched-deps.c
+++ b/gcc/sched-deps.c
@@ -51,12 +51,6 @@ along with GCC; see the file COPYING3.  If not see
 #define CHECK (false)
 #endif
 
-/* In deps->last_pending_memory_flush marks JUMP_INSNs that weren't
-   added to the list because of flush_pending_lists, stands just
-   for itself and not for any other pending memory reads/writes.  */
-#define NON_FLUSH_JUMP_KIND REG_DEP_ANTI
-#define NON_FLUSH_JUMP_P(x) (REG_NOTE_KIND (x) == NON_FLUSH_JUMP_KIND)
-
 /* Holds current parameters for the dependency analyzer.  */
 struct sched_deps_info_def *sched_deps_info;
 
@@ -73,6 +67,9 @@ ds_to_dk (ds_t ds)
   if (ds & DEP_OUTPUT)
     return REG_DEP_OUTPUT;
 
+  if (ds & DEP_CONTROL)
+    return REG_DEP_CONTROL;
+
   gcc_assert (ds & DEP_ANTI);
 
   return REG_DEP_ANTI;
@@ -90,6 +87,9 @@ dk_to_ds (enum reg_note dk)
     case REG_DEP_OUTPUT:
       return DEP_OUTPUT;
 
+    case REG_DEP_CONTROL:
+      return DEP_CONTROL;
+
     default:
       gcc_assert (dk == REG_DEP_ANTI);
       return DEP_ANTI;
@@ -106,6 +106,7 @@ init_dep_1 (dep_t dep, rtx pro, rtx con, enum reg_note type, ds_t ds)
   DEP_CON (dep) = con;
   DEP_TYPE (dep) = type;
   DEP_STATUS (dep) = ds;
+  DEP_COST (dep) = UNKNOWN_DEP_COST;
 }
 
 /* Init DEP with the arguments.
@@ -119,7 +120,7 @@ init_dep (dep_t dep, rtx pro, rtx con, enum reg_note kind)
   if ((current_sched_info->flags & USE_DEPS_LIST))
     ds = dk_to_ds (kind);
   else
-    ds = -1;
+    ds = 0;
 
   init_dep_1 (dep, pro, con, kind, ds);
 }
@@ -185,6 +186,10 @@ dump_dep (FILE *dump, dep_t dep, int flags)
 	  t = 'o';
 	  break;
 
+	case REG_DEP_CONTROL:
+	  t = 'c';
+	  break;
+
 	case REG_DEP_ANTI:
 	  t = 'a';
 	  break;
@@ -412,9 +417,28 @@ clear_deps_list (deps_list_t l)
   while (1);
 }
 
+/* Decide whether a dependency should be treated as a hard or a speculative
+   dependency.  */
+static bool
+dep_spec_p (dep_t dep)
+{
+  if (current_sched_info->flags & DO_SPECULATION)
+    {
+      if (DEP_STATUS (dep) & SPECULATIVE)
+	return true;
+    }
+  if (current_sched_info->flags & DO_PREDICATION)
+    {
+      if (DEP_TYPE (dep) == REG_DEP_CONTROL)
+	return true;
+    }
+  return false;
+}
+
 static regset reg_pending_sets;
 static regset reg_pending_clobbers;
 static regset reg_pending_uses;
+static regset reg_pending_control_uses;
 static enum reg_pending_barrier_mode reg_pending_barrier;
 
 /* Hard registers implicitly clobbered or used (or may be implicitly
@@ -442,10 +466,12 @@ static HARD_REG_SET implicit_reg_pending_uses;
 static bitmap_head *true_dependency_cache = NULL;
 static bitmap_head *output_dependency_cache = NULL;
 static bitmap_head *anti_dependency_cache = NULL;
+static bitmap_head *control_dependency_cache = NULL;
 static bitmap_head *spec_dependency_cache = NULL;
 static int cache_size;
 
 static int deps_may_trap_p (const_rtx);
+static void add_dependence_1 (rtx, rtx, enum reg_note);
 static void add_dependence_list (rtx, rtx, int, enum reg_note);
 static void add_dependence_list_and_free (struct deps_desc *, rtx,
 					  rtx *, int, enum reg_note);
@@ -489,7 +515,7 @@ deps_may_trap_p (const_rtx mem)
    it is set to TRUE when the returned comparison should be reversed
    to get the actual condition.  */
 static rtx
-sched_get_condition_with_rev (const_rtx insn, bool *rev)
+sched_get_condition_with_rev_uncached (const_rtx insn, bool *rev)
 {
   rtx pat = PATTERN (insn);
   rtx src;
@@ -526,6 +552,62 @@ sched_get_condition_with_rev (const_rtx insn, bool *rev)
   return 0;
 }
 
+/* Return the condition under which INSN does not execute (i.e.  the
+   not-taken condition for a conditional branch), or NULL if we cannot
+   find such a condition.  The caller should make a copy of the condition
+   before using it.  */
+rtx
+sched_get_reverse_condition_uncached (const_rtx insn)
+{
+  bool rev;
+  rtx cond = sched_get_condition_with_rev_uncached (insn, &rev);
+  if (cond == NULL_RTX)
+    return cond;
+  if (!rev)
+    {
+      enum rtx_code revcode = reversed_comparison_code (cond, insn);
+      cond = gen_rtx_fmt_ee (revcode, GET_MODE (cond),
+			     XEXP (cond, 0),
+			     XEXP (cond, 1));
+    }
+  return cond;
+}
+
+/* Caching variant of sched_get_condition_with_rev_uncached.
+   We only do actual work the first time we come here for an insn; the
+   results are cached in INSN_CACHED_COND and INSN_REVERSE_COND.  */
+static rtx
+sched_get_condition_with_rev (const_rtx insn, bool *rev)
+{
+  bool tmp;
+
+  if (INSN_LUID (insn) == 0)
+    return sched_get_condition_with_rev_uncached (insn, rev);
+
+  if (INSN_CACHED_COND (insn) == const_true_rtx)
+    return NULL_RTX;
+
+  if (INSN_CACHED_COND (insn) != NULL_RTX)
+    {
+      if (rev)
+	*rev = INSN_REVERSE_COND (insn);
+      return INSN_CACHED_COND (insn);
+    }
+
+  INSN_CACHED_COND (insn) = sched_get_condition_with_rev_uncached (insn, &tmp);
+  INSN_REVERSE_COND (insn) = tmp;
+
+  if (INSN_CACHED_COND (insn) == NULL_RTX)
+    {
+      INSN_CACHED_COND (insn) = const_true_rtx;
+      return NULL_RTX;
+    }
+
+  if (rev)
+    *rev = INSN_REVERSE_COND (insn);
+  return INSN_CACHED_COND (insn);
+}
+
 /* True when we can find a condition under which INSN is executed.  */
 static bool
 sched_has_condition_p (const_rtx insn)
@@ -545,7 +627,7 @@ conditions_mutex_p (const_rtx cond1, const_rtx cond2, bool rev1, bool rev2)
 	  (rev1==rev2
 	  ? reversed_comparison_code (cond2, NULL)
 	  : GET_CODE (cond2))
-      && XEXP (cond1, 0) == XEXP (cond2, 0)
+      && rtx_equal_p (XEXP (cond1, 0), XEXP (cond2, 0))
       && XEXP (cond1, 1) == XEXP (cond2, 1))
     return 1;
   return 0;
@@ -814,12 +896,10 @@ sd_find_dep_between (rtx pro, rtx con, bool resolved_p)
       int elem_luid = INSN_LUID (pro);
       int insn_luid = INSN_LUID (con);
 
-      gcc_assert (output_dependency_cache != NULL
-		  && anti_dependency_cache != NULL);
-
       if (!bitmap_bit_p (&true_dependency_cache[insn_luid], elem_luid)
 	  && !bitmap_bit_p (&output_dependency_cache[insn_luid], elem_luid)
-	  && !bitmap_bit_p (&anti_dependency_cache[insn_luid], elem_luid))
+	  && !bitmap_bit_p (&anti_dependency_cache[insn_luid], elem_luid)
+	  && !bitmap_bit_p (&control_dependency_cache[insn_luid], elem_luid))
 	return NULL;
     }
 
@@ -872,7 +952,8 @@ ask_dependency_caches (dep_t dep)
 
   gcc_assert (true_dependency_cache != NULL
 	      && output_dependency_cache != NULL
-	      && anti_dependency_cache != NULL);
+	      && anti_dependency_cache != NULL
+	      && control_dependency_cache != NULL);
 
   if (!(current_sched_info->flags & USE_DEPS_LIST))
     {
@@ -884,6 +965,8 @@ ask_dependency_caches (dep_t dep)
 	present_dep_type = REG_DEP_OUTPUT;
       else if (bitmap_bit_p (&anti_dependency_cache[insn_luid], elem_luid))
 	present_dep_type = REG_DEP_ANTI;
+      else if (bitmap_bit_p (&control_dependency_cache[insn_luid], elem_luid))
+	present_dep_type = REG_DEP_CONTROL;
       else
 	/* There is no existing dep so it should be created.  */
 	return DEP_CREATED;
@@ -902,6 +985,8 @@ ask_dependency_caches (dep_t dep)
 	present_dep_types |= DEP_OUTPUT;
       if (bitmap_bit_p (&anti_dependency_cache[insn_luid], elem_luid))
 	present_dep_types |= DEP_ANTI;
+      if (bitmap_bit_p (&control_dependency_cache[insn_luid], elem_luid))
+	present_dep_types |= DEP_CONTROL;
 
       if (present_dep_types == 0)
 	/* There is no existing dep so it should be created.  */
@@ -955,6 +1040,10 @@ set_dependency_caches (dep_t dep)
 	  bitmap_set_bit (&anti_dependency_cache[insn_luid], elem_luid);
 	  break;
 
+	case REG_DEP_CONTROL:
+	  bitmap_set_bit (&control_dependency_cache[insn_luid], elem_luid);
+	  break;
+
 	default:
 	  gcc_unreachable ();
 	}
@@ -969,6 +1058,8 @@ set_dependency_caches (dep_t dep)
 	bitmap_set_bit (&output_dependency_cache[insn_luid], elem_luid);
       if (ds & DEP_ANTI)
 	bitmap_set_bit (&anti_dependency_cache[insn_luid], elem_luid);
+      if (ds & DEP_CONTROL)
+	bitmap_set_bit (&control_dependency_cache[insn_luid], elem_luid);
 
       if (ds & SPECULATIVE)
 	{
@@ -1000,6 +1091,10 @@ update_dependency_caches (dep_t dep, enum reg_note old_type)
 	  bitmap_clear_bit (&anti_dependency_cache[insn_luid], elem_luid);
 	  break;
 
+	case REG_DEP_CONTROL:
+	  bitmap_clear_bit (&control_dependency_cache[insn_luid], elem_luid);
+	  break;
+
 	default:
 	  gcc_unreachable ();
 	}
@@ -1040,6 +1135,7 @@ update_dep (dep_t dep, dep_t new_dep,
 {
   enum DEPS_ADJUST_RESULT res = DEP_PRESENT;
   enum reg_note old_type = DEP_TYPE (dep);
+  bool was_spec = dep_spec_p (dep);
 
   /* If this is a more restrictive type of dependence than the
      existing one, then change the existing dependence to this
@@ -1058,20 +1154,13 @@ update_dep (dep_t dep, dep_t new_dep,
       ds_t new_status = ds | dep_status;
 
       if (new_status & SPECULATIVE)
-	/* Either existing dep or a dep we're adding or both are
-	   speculative.  */
 	{
+	  /* Either existing dep or a dep we're adding or both are
+	     speculative.  */
 	  if (!(ds & SPECULATIVE)
 	      || !(dep_status & SPECULATIVE))
 	    /* The new dep can't be speculative.  */
-	    {
-	      new_status &= ~SPECULATIVE;
-
-	      if (dep_status & SPECULATIVE)
-		/* The old dep was speculative, but now it
-		   isn't.  */
-		change_spec_dep_to_hard (sd_it);
-	    }
+	    new_status &= ~SPECULATIVE;
 	  else
 	    {
 	      /* Both are speculative.  Merge probabilities.  */
@@ -1096,6 +1185,10 @@ update_dep (dep_t dep, dep_t new_dep,
 	}
     }
 
+  if (was_spec && !dep_spec_p (dep))
+    /* The old dep was speculative, but now it isn't.  */
+    change_spec_dep_to_hard (sd_it);
+
   if (true_dependency_cache != NULL
       && res == DEP_CHANGED)
     update_dependency_caches (dep, old_type);
@@ -1196,8 +1289,7 @@ get_back_and_forw_lists (dep_t dep, bool resolved_p,
 
   if (!resolved_p)
     {
-      if ((current_sched_info->flags & DO_SPECULATION)
-	  && (DEP_STATUS (dep) & SPECULATIVE))
+      if (dep_spec_p (dep))
 	*back_list_ptr = INSN_SPEC_BACK_DEPS (con);
       else
 	*back_list_ptr = INSN_HARD_BACK_DEPS (con);
@@ -1224,8 +1316,8 @@ sd_add_dep (dep_t dep, bool resolved_p)
 
   gcc_assert (INSN_P (insn) && INSN_P (elem) && insn != elem);
 
-  if ((current_sched_info->flags & DO_SPECULATION)
-      && !sched_insn_is_legitimate_for_speculation_p (insn, DEP_STATUS (dep)))
+  if ((current_sched_info->flags & DO_SPECULATION) == 0
+      || !sched_insn_is_legitimate_for_speculation_p (insn, DEP_STATUS (dep)))
     DEP_STATUS (dep) &= ~SPECULATIVE;
 
   copy_dep (DEP_NODE_DEP (n), dep);
@@ -1265,8 +1357,7 @@ sd_resolve_dep (sd_iterator_def sd_it)
   rtx pro = DEP_PRO (dep);
   rtx con = DEP_CON (dep);
 
-  if ((current_sched_info->flags & DO_SPECULATION)
-      && (DEP_STATUS (dep) & SPECULATIVE))
+  if (dep_spec_p (dep))
     move_dep_link (DEP_NODE_BACK (node), INSN_SPEC_BACK_DEPS (con),
 		   INSN_RESOLVED_BACK_DEPS (con));
   else
@@ -1277,6 +1368,27 @@ sd_resolve_dep (sd_iterator_def sd_it)
 		 INSN_RESOLVED_FORW_DEPS (pro));
 }
 
+/* Perform the inverse operation of sd_resolve_dep.  Restore the dependence
+   pointed to by SD_IT to unresolved state.  */
+void
+sd_unresolve_dep (sd_iterator_def sd_it)
+{
+  dep_node_t node = DEP_LINK_NODE (*sd_it.linkp);
+  dep_t dep = DEP_NODE_DEP (node);
+  rtx pro = DEP_PRO (dep);
+  rtx con = DEP_CON (dep);
+
+  if (dep_spec_p (dep))
+    move_dep_link (DEP_NODE_BACK (node), INSN_RESOLVED_BACK_DEPS (con),
+		   INSN_SPEC_BACK_DEPS (con));
+  else
+    move_dep_link (DEP_NODE_BACK (node), INSN_RESOLVED_BACK_DEPS (con),
+		   INSN_HARD_BACK_DEPS (con));
+
+  move_dep_link (DEP_NODE_FORW (node), INSN_RESOLVED_FORW_DEPS (pro),
+		 INSN_FORW_DEPS (pro));
+}
+
 /* Make TO depend on all the FROM's producers.
    If RESOLVED_P is true add dependencies to the resolved lists.  */
 void
@@ -1317,6 +1429,7 @@ sd_delete_dep (sd_iterator_def sd_it)
 
       bitmap_clear_bit (&true_dependency_cache[insn_luid], elem_luid);
       bitmap_clear_bit (&anti_dependency_cache[insn_luid], elem_luid);
+      bitmap_clear_bit (&control_dependency_cache[insn_luid], elem_luid);
       bitmap_clear_bit (&output_dependency_cache[insn_luid], elem_luid);
 
       if (current_sched_info->flags & DO_SPECULATION)
@@ -1382,6 +1495,57 @@ sd_debug_lists (rtx insn, sd_list_types_def types)
   fprintf (stderr, "\n");
 }
 
+/* A wrapper around add_dependence_1, to add a dependence of CON on
+   PRO, with type DEP_TYPE.  This function implements special handling
+   for REG_DEP_CONTROL dependencies.  For these, we optionally promote
+   the type to REG_DEP_ANTI if we can determine that predication is
+   impossible; otherwise we add additional true dependencies on the
+   INSN_COND_DEPS list of the jump (which PRO must be).  */
+void
+add_dependence (rtx con, rtx pro, enum reg_note dep_type)
+{
+  if (dep_type == REG_DEP_CONTROL
+      && !(current_sched_info->flags & DO_PREDICATION))
+    dep_type = REG_DEP_ANTI;
+
+  /* A REG_DEP_CONTROL dependence may be eliminated through predication,
+     so we must also make the insn dependent on the setter of the
+     condition.  */
+  if (dep_type == REG_DEP_CONTROL)
+    {
+      rtx real_pro = pro;
+      rtx other = real_insn_for_shadow (real_pro);
+      rtx cond;
+
+      if (other != NULL_RTX)
+	real_pro = other;
+      cond = sched_get_reverse_condition_uncached (real_pro);
+      /* Verify that the insn does not use a different value in
+	 the condition register than the one that was present at
+	 the jump.  */
+      if (cond == NULL_RTX)
+	dep_type = REG_DEP_ANTI;
+      else if (INSN_CACHED_COND (real_pro) == const_true_rtx)
+	{
+	  HARD_REG_SET uses;
+	  CLEAR_HARD_REG_SET (uses);
+	  note_uses (&PATTERN (con), record_hard_reg_uses, &uses);
+	  if (TEST_HARD_REG_BIT (uses, REGNO (XEXP (cond, 0))))
+	    dep_type = REG_DEP_ANTI;
+	}
+      if (dep_type == REG_DEP_CONTROL)
+	{
+	  if (sched_verbose >= 5)
+	    fprintf (sched_dump, "making DEP_CONTROL for %d\n",
+		     INSN_UID (real_pro));
+	  add_dependence_list (con, INSN_COND_DEPS (real_pro), 0,
+			       REG_DEP_TRUE);
+	}
+    }
+	  
+  add_dependence_1 (con, pro, dep_type);
+}
+
 /* A convenience wrapper to operate on an entire list.  */
 
 static void
@@ -1597,6 +1761,10 @@ flush_pending_lists (struct deps_desc *deps, rtx insn, int for_read,
   add_dependence_list_and_free (deps, insn,
                                 &deps->last_pending_memory_flush, 1,
                                 for_read ? REG_DEP_ANTI : REG_DEP_OUTPUT);
+
+  add_dependence_list_and_free (deps, insn, &deps->pending_jump_insns, 1,
+				REG_DEP_ANTI);
+
   if (!deps->readonly)
     {
       free_EXPR_LIST_list (&deps->pending_write_mems);
@@ -1659,7 +1827,7 @@ haifa_note_mem_dep (rtx mem, rtx pending_mem, rtx pending_insn, ds_t ds)
     dep_def _dep, *dep = &_dep;
 
     init_dep_1 (dep, pending_insn, cur_insn, ds_to_dt (ds),
-                current_sched_info->flags & USE_DEPS_LIST ? ds : -1);
+                current_sched_info->flags & USE_DEPS_LIST ? ds : 0);
     maybe_add_or_update_dep_1 (dep, false, pending_mem, mem);
   }
 
@@ -1718,10 +1886,12 @@ ds_to_dt (ds_t ds)
     return REG_DEP_TRUE;
   else if (ds & DEP_OUTPUT)
     return REG_DEP_OUTPUT;
+  else if (ds & DEP_ANTI)
+    return REG_DEP_ANTI;
   else
     {
-      gcc_assert (ds & DEP_ANTI);
-      return REG_DEP_ANTI;
+      gcc_assert (ds & DEP_CONTROL);
+      return REG_DEP_CONTROL;
     }
 }
 
@@ -2333,6 +2503,8 @@ sched_analyze_1 (struct deps_desc *deps, rtx x, rtx insn)
 
 	  add_dependence_list (insn, deps->last_pending_memory_flush, 1,
 			       REG_DEP_ANTI);
+	  add_dependence_list (insn, deps->pending_jump_insns, 1,
+			       REG_DEP_CONTROL);
 
           if (!deps->readonly)
             add_insn_mem_dependence (deps, false, insn, dest);
@@ -2480,23 +2652,22 @@ sched_analyze_2 (struct deps_desc *deps, rtx x, rtx insn)
 	      }
 
 	    for (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))
-	      {
-		if (! NON_FLUSH_JUMP_P (u))
-		  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);
-		else if (deps_may_trap_p (x))
-		  {
-		    if ((sched_deps_info->generate_spec_deps)
-			&& sel_sched_p () && (spec_info->mask & BEGIN_CONTROL))
-		      {
-			ds_t ds = set_dep_weak (DEP_ANTI, BEGIN_CONTROL,
-						MAX_DEP_WEAK);
-
-			note_dep (XEXP (u, 0), ds);
-		      }
-		    else
-		      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);
-		  }
-	      }
+	      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);
+
+	    for (u = deps->pending_jump_insns; u; u = XEXP (u, 1))
+	      if (deps_may_trap_p (x))
+		{
+		  if ((sched_deps_info->generate_spec_deps)
+		      && sel_sched_p () && (spec_info->mask & BEGIN_CONTROL))
+		    {
+		      ds_t ds = set_dep_weak (DEP_ANTI, BEGIN_CONTROL,
+					      MAX_DEP_WEAK);
+		      
+		      note_dep (XEXP (u, 0), ds);
+		    }
+		  else
+		    add_dependence (insn, XEXP (u, 0), REG_DEP_CONTROL);
+		}
 	  }
 
 	/* Always add these dependencies to pending_reads, since
@@ -2635,6 +2806,18 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
     add_dependence_list (insn, deps->last_function_call_may_noreturn,
 			 1, REG_DEP_ANTI);
 
+  /* We must avoid creating a situation in which two successors of the
+     current block have different unwind info after scheduling.  If at any
+     point the two paths re-join this leads to incorrect unwind info.  */
+  /* ??? There are certain situations involving a forced frame pointer in
+     which, with extra effort, we could fix up the unwind info at a later
+     CFG join.  However, it seems better to notice these cases earlier
+     during prologue generation and avoid marking the frame pointer setup
+     as frame-related at all.  */
+  if (RTX_FRAME_RELATED_P (insn))
+    deps->sched_before_next_jump
+      = alloc_INSN_LIST (insn, deps->sched_before_next_jump);
+
   if (code == COND_EXEC)
     {
       sched_analyze_2 (deps, COND_EXEC_TEST (x), insn);
@@ -2684,7 +2867,7 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
 	{
 	  if (GET_CODE (XEXP (link, 0)) == CLOBBER)
 	    sched_analyze_1 (deps, XEXP (link, 0), insn);
-	  else
+	  else if (GET_CODE (XEXP (link, 0)) != SET)
 	    sched_analyze_2 (deps, XEXP (link, 0), insn);
 	}
       /* Don't schedule anything after a tail call, tail call needs
@@ -2707,14 +2890,11 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
 
           if (sched_deps_info->compute_jump_reg_dependencies)
             {
-              regset_head tmp_uses, tmp_sets;
-              INIT_REG_SET (&tmp_uses);
-              INIT_REG_SET (&tmp_sets);
-
               (*sched_deps_info->compute_jump_reg_dependencies)
-                (insn, &deps->reg_conditional_sets, &tmp_uses, &tmp_sets);
+		(insn, reg_pending_control_uses);
+
               /* Make latency of jump equal to 0 by using anti-dependence.  */
-              EXECUTE_IF_SET_IN_REG_SET (&tmp_uses, 0, i, rsi)
+              EXECUTE_IF_SET_IN_REG_SET (reg_pending_control_uses, 0, i, rsi)
                 {
                   struct deps_reg *reg_last = &deps->reg_last[i];
                   add_dependence_list (insn, reg_last->sets, 0, REG_DEP_ANTI);
@@ -2722,17 +2902,7 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
 				       0, REG_DEP_ANTI);
                   add_dependence_list (insn, reg_last->clobbers, 0,
 				       REG_DEP_ANTI);
-
-                  if (!deps->readonly)
-                    {
-                      reg_last->uses_length++;
-                      reg_last->uses = alloc_INSN_LIST (insn, reg_last->uses);
-                    }
                 }
-              IOR_REG_SET (reg_pending_sets, &tmp_sets);
-
-              CLEAR_REG_SET (&tmp_uses);
-              CLEAR_REG_SET (&tmp_sets);
             }
 
 	  /* All memory writes and volatile reads must happen before the
@@ -2762,6 +2932,8 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
 
 	  add_dependence_list (insn, deps->last_pending_memory_flush, 1,
 			       REG_DEP_ANTI);
+	  add_dependence_list (insn, deps->pending_jump_insns, 1,
+			       REG_DEP_ANTI);
 	}
     }
 
@@ -2797,13 +2969,15 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
 			   REG_DEP_ANTI);
 
       for (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))
-	if (! NON_FLUSH_JUMP_P (u) || !sel_sched_p ())
+	if (!sel_sched_p ())
 	  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);
 
       EXECUTE_IF_SET_IN_REG_SET (reg_pending_uses, 0, i, rsi)
 	{
 	  struct deps_reg *reg_last = &deps->reg_last[i];
 	  add_dependence_list (insn, reg_last->sets, 1, REG_DEP_ANTI);
+	  /* There's no point in making REG_DEP_CONTROL dependencies for
+	     debug insns.  */
 	  add_dependence_list (insn, reg_last->clobbers, 1, REG_DEP_ANTI);
 
 	  if (!deps->readonly)
@@ -2825,6 +2999,8 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
     }
   else
     {
+      regset_head set_or_clobbered;
+
       EXECUTE_IF_SET_IN_REG_SET (reg_pending_uses, 0, i, rsi)
 	{
 	  struct deps_reg *reg_last = &deps->reg_last[i];
@@ -2855,6 +3031,25 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
 	      }
 	  }
 
+      if (targetm.sched.exposed_pipeline)
+	{
+	  INIT_REG_SET (&set_or_clobbered);
+	  bitmap_ior (&set_or_clobbered, reg_pending_clobbers,
+		      reg_pending_sets);
+	  EXECUTE_IF_SET_IN_REG_SET (&set_or_clobbered, 0, i, rsi)
+	    {
+	      struct deps_reg *reg_last = &deps->reg_last[i];
+	      rtx list;
+	      for (list = reg_last->uses; list; list = XEXP (list, 1))
+		{
+		  rtx other = XEXP (list, 0);
+		  if (INSN_CACHED_COND (other) != const_true_rtx
+		      && refers_to_regno_p (i, i + 1, INSN_CACHED_COND (other), NULL))
+		    INSN_CACHED_COND (other) = const_true_rtx;
+		}
+	    }
+	}
+
       /* If the current insn is conditional, we can't free any
 	 of the lists.  */
       if (sched_has_condition_p (insn))
@@ -2866,6 +3061,8 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
 	      add_dependence_list (insn, reg_last->implicit_sets, 0,
 				   REG_DEP_ANTI);
 	      add_dependence_list (insn, reg_last->uses, 0, REG_DEP_ANTI);
+	      add_dependence_list (insn, reg_last->control_uses, 0,
+				   REG_DEP_CONTROL);
 
 	      if (!deps->readonly)
 		{
@@ -2882,12 +3079,11 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
 				   REG_DEP_ANTI);
 	      add_dependence_list (insn, reg_last->clobbers, 0, REG_DEP_OUTPUT);
 	      add_dependence_list (insn, reg_last->uses, 0, REG_DEP_ANTI);
+	      add_dependence_list (insn, reg_last->control_uses, 0,
+				   REG_DEP_CONTROL);
 
 	      if (!deps->readonly)
-		{
-		  reg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);
-		  SET_REGNO_REG_SET (&deps->reg_conditional_sets, i);
-		}
+		reg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);
 	    }
 	}
       else
@@ -2905,6 +3101,9 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
 						REG_DEP_ANTI);
 		  add_dependence_list_and_free (deps, insn, &reg_last->uses, 0,
 						REG_DEP_ANTI);
+		  add_dependence_list_and_free (deps, insn,
+						&reg_last->control_uses, 0,
+						REG_DEP_ANTI);
 		  add_dependence_list_and_free
 		    (deps, insn, &reg_last->clobbers, 0, REG_DEP_OUTPUT);
 
@@ -2921,6 +3120,8 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
 		  add_dependence_list (insn, reg_last->implicit_sets, 0,
 				       REG_DEP_ANTI);
 		  add_dependence_list (insn, reg_last->uses, 0, REG_DEP_ANTI);
+		  add_dependence_list (insn, reg_last->control_uses, 0,
+				       REG_DEP_CONTROL);
 		}
 
 	      if (!deps->readonly)
@@ -2943,16 +3144,26 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
 					    REG_DEP_OUTPUT);
 	      add_dependence_list_and_free (deps, insn, &reg_last->uses, 0,
 					    REG_DEP_ANTI);
+	      add_dependence_list (insn, reg_last->control_uses, 0,
+				   REG_DEP_CONTROL);
 
 	      if (!deps->readonly)
 		{
 		  reg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);
 		  reg_last->uses_length = 0;
 		  reg_last->clobbers_length = 0;
-		  CLEAR_REGNO_REG_SET (&deps->reg_conditional_sets, i);
 		}
 	    }
 	}
+      if (!deps->readonly)
+	{
+	  EXECUTE_IF_SET_IN_REG_SET (reg_pending_control_uses, 0, i, rsi)
+	    {
+	      struct deps_reg *reg_last = &deps->reg_last[i];
+	      reg_last->control_uses
+		= alloc_INSN_LIST (insn, reg_last->control_uses);
+	    }
+	}
     }
 
   for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
@@ -2962,6 +3173,7 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
 	add_dependence_list (insn, reg_last->sets, 0, REG_DEP_ANTI);
 	add_dependence_list (insn, reg_last->clobbers, 0, REG_DEP_ANTI);
 	add_dependence_list (insn, reg_last->uses, 0, REG_DEP_ANTI);
+	add_dependence_list (insn, reg_last->control_uses, 0, REG_DEP_ANTI);
 
 	if (!deps->readonly)
 	  reg_last->implicit_sets
@@ -2985,6 +3197,7 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
   CLEAR_REG_SET (reg_pending_uses);
   CLEAR_REG_SET (reg_pending_clobbers);
   CLEAR_REG_SET (reg_pending_sets);
+  CLEAR_REG_SET (reg_pending_control_uses);
   CLEAR_HARD_REG_SET (implicit_reg_pending_clobbers);
   CLEAR_HARD_REG_SET (implicit_reg_pending_uses);
 
@@ -3016,6 +3229,9 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
 	      struct deps_reg *reg_last = &deps->reg_last[i];
 	      add_dependence_list_and_free (deps, insn, &reg_last->uses, 0,
 					    REG_DEP_ANTI);
+	      add_dependence_list_and_free (deps, insn,
+					    &reg_last->control_uses, 0,
+					    REG_DEP_CONTROL);
 	      add_dependence_list_and_free (deps, insn, &reg_last->sets, 0,
 					    reg_pending_barrier == TRUE_BARRIER
 					    ? REG_DEP_TRUE : REG_DEP_ANTI);
@@ -3047,8 +3263,6 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)
                              && sel_insn_is_speculation_check (insn)))
 	flush_pending_lists (deps, insn, true, true);
 
-      if (!deps->readonly)
-        CLEAR_REG_SET (&deps->reg_conditional_sets);
       reg_pending_barrier = NOT_A_BARRIER;
     }
 
@@ -3229,12 +3443,41 @@ deps_analyze_insn (struct deps_desc *deps, rtx insn)
   if (sched_deps_info->start_insn)
     sched_deps_info->start_insn (insn);
 
-  if (NONJUMP_INSN_P (insn) || DEBUG_INSN_P (insn) || JUMP_P (insn))
+  /* Record the condition for this insn.  */
+  if (NONDEBUG_INSN_P (insn))
+    {
+      rtx t;
+      sched_get_condition_with_rev (insn, NULL);
+      t = INSN_CACHED_COND (insn);
+      INSN_COND_DEPS (insn) = NULL_RTX;
+      if (reload_completed
+	  && (current_sched_info->flags & DO_PREDICATION)
+	  && COMPARISON_P (t)
+	  && REG_P (XEXP (t, 0))
+	  && CONSTANT_P (XEXP (t, 1)))
+	{
+	  unsigned int regno;
+	  int nregs;
+	  t = XEXP (t, 0);
+	  regno = REGNO (t);
+	  nregs = hard_regno_nregs[regno][GET_MODE (t)];
+	  t = NULL_RTX;
+	  while (nregs-- > 0)
+	    {
+	      struct deps_reg *reg_last = &deps->reg_last[regno + nregs];
+	      t = concat_INSN_LIST (reg_last->sets, t);
+	      t = concat_INSN_LIST (reg_last->clobbers, t);
+	      t = concat_INSN_LIST (reg_last->implicit_sets, t);
+	    }
+	  INSN_COND_DEPS (insn) = t;
+	}
+    }
+
+  if (JUMP_P (insn))
     {
       /* Make each JUMP_INSN (but not a speculative check)
          a scheduling barrier for memory references.  */
       if (!deps->readonly
-          && JUMP_P (insn)
           && !(sel_sched_p ()
                && sel_insn_is_speculation_check (insn)))
         {
@@ -3242,17 +3485,19 @@ deps_analyze_insn (struct deps_desc *deps, rtx insn)
           if (deps->pending_flush_length++ > MAX_PENDING_LIST_LENGTH)
             flush_pending_lists (deps, insn, true, true);
           else
-	    {
-	      deps->last_pending_memory_flush
-		= alloc_INSN_LIST (insn, deps->last_pending_memory_flush);
-	      /* Signal to sched_analyze_insn that this jump stands
-		 just for its own, not any other pending memory
-		 reads/writes flush_pending_lists had to flush.  */
-	      PUT_REG_NOTE_KIND (deps->last_pending_memory_flush,
-				 NON_FLUSH_JUMP_KIND);
-	    }
+	    deps->pending_jump_insns
+              = alloc_INSN_LIST (insn, deps->pending_jump_insns);
         }
 
+      /* For each insn which shouldn't cross a jump, add a dependence.  */
+      add_dependence_list_and_free (deps, insn,
+				    &deps->sched_before_next_jump, 1,
+				    REG_DEP_ANTI);
+
+      sched_analyze_insn (deps, PATTERN (insn), insn);
+    }
+  else if (NONJUMP_INSN_P (insn) || DEBUG_INSN_P (insn))
+    {
       sched_analyze_insn (deps, PATTERN (insn), insn);
     }
   else if (CALL_P (insn))
@@ -3449,18 +3694,23 @@ sched_free_deps (rtx head, rtx tail, bool resolved_p)
   rtx insn;
   rtx next_tail = NEXT_INSN (tail);
 
+  /* We make two passes since some insns may be scheduled before their
+     dependencies are resolved.  */
   for (insn = head; insn != next_tail; insn = NEXT_INSN (insn))
     if (INSN_P (insn) && INSN_LUID (insn) > 0)
       {
-	/* Clear resolved back deps together with its dep_nodes.  */
-	delete_dep_nodes_in_back_deps (insn, resolved_p);
-
 	/* Clear forward deps and leave the dep_nodes to the
 	   corresponding back_deps list.  */
 	if (resolved_p)
 	  clear_deps_list (INSN_RESOLVED_FORW_DEPS (insn));
 	else
 	  clear_deps_list (INSN_FORW_DEPS (insn));
+      }
+  for (insn = head; insn != next_tail; insn = NEXT_INSN (insn))
+    if (INSN_P (insn) && INSN_LUID (insn) > 0)
+      {
+	/* Clear resolved back deps together with its dep_nodes.  */
+	delete_dep_nodes_in_back_deps (insn, resolved_p);
 
 	sd_finish_insn (insn);
       }
@@ -3481,12 +3731,12 @@ init_deps (struct deps_desc *deps, bool lazy_reg_last)
   else
     deps->reg_last = XCNEWVEC (struct deps_reg, max_reg);
   INIT_REG_SET (&deps->reg_last_in_use);
-  INIT_REG_SET (&deps->reg_conditional_sets);
 
   deps->pending_read_insns = 0;
   deps->pending_read_mems = 0;
   deps->pending_write_insns = 0;
   deps->pending_write_mems = 0;
+  deps->pending_jump_insns = 0;
   deps->pending_read_list_length = 0;
   deps->pending_write_list_length = 0;
   deps->pending_flush_length = 0;
@@ -3494,6 +3744,7 @@ init_deps (struct deps_desc *deps, bool lazy_reg_last)
   deps->last_function_call = 0;
   deps->last_function_call_may_noreturn = 0;
   deps->sched_before_next_call = 0;
+  deps->sched_before_next_jump = 0;
   deps->in_post_call_group_p = not_post_call;
   deps->last_debug_insn = 0;
   deps->last_reg_pending_barrier = NOT_A_BARRIER;
@@ -3546,11 +3797,12 @@ free_deps (struct deps_desc *deps)
 	free_INSN_LIST_list (&reg_last->sets);
       if (reg_last->implicit_sets)
 	free_INSN_LIST_list (&reg_last->implicit_sets);
+      if (reg_last->control_uses)
+	free_INSN_LIST_list (&reg_last->control_uses);
       if (reg_last->clobbers)
 	free_INSN_LIST_list (&reg_last->clobbers);
     }
   CLEAR_REG_SET (&deps->reg_last_in_use);
-  CLEAR_REG_SET (&deps->reg_conditional_sets);
 
   /* As we initialize reg_last lazily, it is possible that we didn't allocate
      it at all.  */
@@ -3561,8 +3813,7 @@ free_deps (struct deps_desc *deps)
   deps = NULL;
 }
 
-/* Remove INSN from dependence contexts DEPS.  Caution: reg_conditional_sets
-   is not handled.  */
+/* Remove INSN from dependence contexts DEPS.  */
 void
 remove_from_deps (struct deps_desc *deps, rtx insn)
 {
@@ -3577,6 +3828,9 @@ remove_from_deps (struct deps_desc *deps, rtx insn)
   removed = remove_from_both_dependence_lists (insn, &deps->pending_write_insns,
                                                &deps->pending_write_mems);
   deps->pending_write_list_length -= removed;
+
+  removed = remove_from_dependence_list (insn, &deps->pending_jump_insns);
+  deps->pending_flush_length -= removed;
   removed = remove_from_dependence_list (insn, &deps->last_pending_memory_flush);
   deps->pending_flush_length -= removed;
 
@@ -3671,6 +3925,8 @@ extend_dependency_caches (int n, bool create_p)
 					    output_dependency_cache, luid);
       anti_dependency_cache = XRESIZEVEC (bitmap_head, anti_dependency_cache,
 					  luid);
+      control_dependency_cache = XRESIZEVEC (bitmap_head, control_dependency_cache,
+					  luid);
 
       if (current_sched_info->flags & DO_SPECULATION)
         spec_dependency_cache = XRESIZEVEC (bitmap_head, spec_dependency_cache,
@@ -3681,6 +3937,7 @@ extend_dependency_caches (int n, bool create_p)
 	  bitmap_initialize (&true_dependency_cache[i], 0);
 	  bitmap_initialize (&output_dependency_cache[i], 0);
 	  bitmap_initialize (&anti_dependency_cache[i], 0);
+	  bitmap_initialize (&control_dependency_cache[i], 0);
 
           if (current_sched_info->flags & DO_SPECULATION)
             bitmap_initialize (&spec_dependency_cache[i], 0);
@@ -3710,6 +3967,7 @@ sched_deps_finish (void)
 	  bitmap_clear (&true_dependency_cache[i]);
 	  bitmap_clear (&output_dependency_cache[i]);
 	  bitmap_clear (&anti_dependency_cache[i]);
+	  bitmap_clear (&control_dependency_cache[i]);
 
           if (sched_deps_info->generate_spec_deps)
             bitmap_clear (&spec_dependency_cache[i]);
@@ -3720,6 +3978,8 @@ sched_deps_finish (void)
       output_dependency_cache = NULL;
       free (anti_dependency_cache);
       anti_dependency_cache = NULL;
+      free (control_dependency_cache);
+      control_dependency_cache = NULL;
 
       if (sched_deps_info->generate_spec_deps)
         {
@@ -3741,6 +4001,7 @@ init_deps_global (void)
   reg_pending_sets = ALLOC_REG_SET (&reg_obstack);
   reg_pending_clobbers = ALLOC_REG_SET (&reg_obstack);
   reg_pending_uses = ALLOC_REG_SET (&reg_obstack);
+  reg_pending_control_uses = ALLOC_REG_SET (&reg_obstack);
   reg_pending_barrier = NOT_A_BARRIER;
 
   if (!sel_sched_p () || sched_emulate_haifa_p)
@@ -3765,6 +4026,7 @@ finish_deps_global (void)
   FREE_REG_SET (reg_pending_sets);
   FREE_REG_SET (reg_pending_clobbers);
   FREE_REG_SET (reg_pending_uses);
+  FREE_REG_SET (reg_pending_control_uses);
 }
 
 /* Estimate the weakness of dependence between MEM1 and MEM2.  */
@@ -3798,8 +4060,8 @@ estimate_dep_weak (rtx mem1, rtx mem2)
 /* Add or update backward dependence between INSN and ELEM with type DEP_TYPE.
    This function can handle same INSN and ELEM (INSN == ELEM).
    It is a convenience wrapper.  */
-void
-add_dependence (rtx insn, rtx elem, enum reg_note dep_type)
+static void
+add_dependence_1 (rtx insn, rtx elem, enum reg_note dep_type)
 {
   ds_t ds;
   bool internal;
@@ -3808,6 +4070,8 @@ add_dependence (rtx insn, rtx elem, enum reg_note dep_type)
     ds = DEP_TRUE;
   else if (dep_type == REG_DEP_OUTPUT)
     ds = DEP_OUTPUT;
+  else if (dep_type == REG_DEP_CONTROL)
+    ds = DEP_CONTROL;
   else
     {
       gcc_assert (dep_type == REG_DEP_ANTI);
@@ -4074,10 +4338,12 @@ dump_ds (FILE *f, ds_t s)
 
   if (s & DEP_TRUE)
     fprintf (f, "DEP_TRUE; ");
-  if (s & DEP_ANTI)
-    fprintf (f, "DEP_ANTI; ");
   if (s & DEP_OUTPUT)
     fprintf (f, "DEP_OUTPUT; ");
+  if (s & DEP_ANTI)
+    fprintf (f, "DEP_ANTI; ");
+  if (s & DEP_CONTROL)
+    fprintf (f, "DEP_CONTROL; ");
 
   fprintf (f, "}");
 }
@@ -4102,7 +4368,7 @@ check_dep (dep_t dep, bool relaxed_p)
 
   if (!(current_sched_info->flags & USE_DEPS_LIST))
     {
-      gcc_assert (ds == -1);
+      gcc_assert (ds == 0);
       return;
     }
 
@@ -4112,10 +4378,13 @@ check_dep (dep_t dep, bool relaxed_p)
   else if (dt == REG_DEP_OUTPUT)
     gcc_assert ((ds & DEP_OUTPUT)
 		&& !(ds & DEP_TRUE));
-  else
-    gcc_assert ((dt == REG_DEP_ANTI)
-		&& (ds & DEP_ANTI)
+  else if (dt == REG_DEP_ANTI)
+    gcc_assert ((ds & DEP_ANTI)
 		&& !(ds & (DEP_OUTPUT | DEP_TRUE)));
+  else
+    gcc_assert (dt == REG_DEP_CONTROL
+		&& (ds & DEP_CONTROL)
+		&& !(ds & (DEP_OUTPUT | DEP_ANTI | DEP_TRUE)));
 
   /* HARD_DEP can not appear in dep_status of a link.  */
   gcc_assert (!(ds & HARD_DEP));
diff --git a/gcc/sched-ebb.c b/gcc/sched-ebb.c
index 9fbb9b9..55eb6eb 100644
--- a/gcc/sched-ebb.c
+++ b/gcc/sched-ebb.c
@@ -59,7 +59,7 @@ static basic_block last_bb;
 
 /* Implementations of the sched_info functions for region scheduling.  */
 static void init_ready_list (void);
-static void begin_schedule_ready (rtx, rtx);
+static void begin_schedule_ready (rtx);
 static int schedule_more_p (void);
 static const char *ebb_print_insn (const_rtx, int);
 static int rank (rtx, rtx);
@@ -74,6 +74,25 @@ static void ebb_add_block (basic_block, basic_block);
 static basic_block advance_target_bb (basic_block, rtx);
 static void ebb_fix_recovery_cfg (int, int, int);
 
+/* Allocate memory and store the state of the frontend.  Return the allocated
+   memory.  */
+static void *
+save_ebb_state (void)
+{
+  int *p = XNEW (int);
+  *p = sched_rgn_n_insns;
+  return p;
+}
+
+/* Restore the state of the frontend from P_, then free it.  */
+static void
+restore_ebb_state (void *p_)
+{
+  int *p = (int *)p_;
+  sched_rgn_n_insns = *p;
+  free (p_);
+}
+
 /* Return nonzero if there are more insns that should be scheduled.  */
 
 static int
@@ -125,10 +144,15 @@ init_ready_list (void)
 
 /* INSN is being scheduled after LAST.  Update counters.  */
 static void
-begin_schedule_ready (rtx insn, rtx last)
+begin_schedule_ready (rtx insn ATTRIBUTE_UNUSED)
 {
   sched_rgn_n_insns++;
+}
 
+/* INSN is being moved to its place in the schedule, after LAST.  */
+static void
+begin_move_insn (rtx insn, rtx last)
+{
   if (BLOCK_FOR_INSN (insn) == last_bb
       /* INSN is a jump in the last block, ...  */
       && control_flow_insn_p (insn)
@@ -233,28 +257,18 @@ ebb_contributes_to_priority (rtx next ATTRIBUTE_UNUSED,
   return 1;
 }
 
- /* INSN is a JUMP_INSN, COND_SET is the set of registers that are
-    conditionally set before INSN.  Store the set of registers that
-    must be considered as used by this jump in USED and that of
-    registers that must be considered as set in SET.  */
+ /* INSN is a JUMP_INSN.  Store the set of registers that
+    must be considered as used by this jump in USED.  */
 
 void
-ebb_compute_jump_reg_dependencies (rtx insn, regset cond_set, regset used,
-				   regset set)
+ebb_compute_jump_reg_dependencies (rtx insn, regset used)
 {
   basic_block b = BLOCK_FOR_INSN (insn);
   edge e;
   edge_iterator ei;
 
   FOR_EACH_EDGE (e, ei, b->succs)
-    if (e->flags & EDGE_FALLTHRU)
-      /* The jump may be a by-product of a branch that has been merged
-	 in the main codepath after being conditionalized.  Therefore
-	 it may guard the fallthrough block from using a value that has
-	 conditionally overwritten that of the main codepath.  So we
-	 consider that it restores the value of the main codepath.  */
-      bitmap_and (set, df_get_live_in (e->dest), cond_set);
-    else
+    if ((e->flags & EDGE_FALLTHRU) == 0)
       bitmap_ior_into (used, df_get_live_in (e->dest));
 }
 
@@ -288,7 +302,12 @@ static struct haifa_sched_info ebb_sched_info =
 
   ebb_add_remove_insn,
   begin_schedule_ready,
+  begin_move_insn,
   advance_target_bb,
+
+  save_ebb_state,
+  restore_ebb_state,
+
   SCHED_EBB
   /* We can create new blocks in begin_schedule_ready ().  */
   | NEW_BBS
@@ -371,76 +390,71 @@ add_deps_for_risky_insns (rtx head, rtx tail)
   basic_block last_block = NULL, bb;
 
   for (insn = head; insn != next_tail; insn = NEXT_INSN (insn))
-    if (control_flow_insn_p (insn))
-      {
-	bb = BLOCK_FOR_INSN (insn);
-	bb->aux = last_block;
-	last_block = bb;
-	last_jump = insn;
-      }
-    else if (INSN_P (insn) && last_jump != NULL_RTX)
-      {
-	classification = haifa_classify_insn (insn);
-	prev = last_jump;
-	switch (classification)
-	  {
-	  case PFREE_CANDIDATE:
-	    if (flag_schedule_speculative_load)
-	      {
-		bb = earliest_block_with_similiar_load (last_block, insn);
-		if (bb)
-		  {
-		    bb = (basic_block) bb->aux;
-		    if (!bb)
-		      break;
-		    prev = BB_END (bb);
-		  }
-	      }
-	    /* Fall through.  */
-	  case TRAP_RISKY:
-	  case IRISKY:
-	  case PRISKY_CANDIDATE:
-	    /* ??? We could implement better checking PRISKY_CANDIDATEs
-	       analogous to sched-rgn.c.  */
-	    /* We can not change the mode of the backward
-	       dependency because REG_DEP_ANTI has the lowest
-	       rank.  */
-	    if (! sched_insns_conditions_mutex_p (insn, prev))
-	      {
-		dep_def _dep, *dep = &_dep;
-
-		init_dep (dep, prev, insn, REG_DEP_ANTI);
-
-		if (!(current_sched_info->flags & USE_DEPS_LIST))
-		  {
-		    enum DEPS_ADJUST_RESULT res;
-
-		    res = sd_add_or_update_dep (dep, false);
-
-		    /* We can't change an existing dependency with
-		       DEP_ANTI.  */
-		    gcc_assert (res != DEP_CHANGED);
-		  }
-		else
-		  {
-		    if ((current_sched_info->flags & DO_SPECULATION)
-			&& (spec_info->mask & BEGIN_CONTROL))
-		      DEP_STATUS (dep) = set_dep_weak (DEP_ANTI, BEGIN_CONTROL,
-						       MAX_DEP_WEAK);
-
-		    sd_add_or_update_dep (dep, false);
-
-		    /* Dep_status could have been changed.
-		       No assertion here.  */
-		  }
-	      }
-
-            break;
-
-          default:
-            break;
-	  }
-      }
+    {
+      add_delay_dependencies (insn);
+      if (control_flow_insn_p (insn))
+	{
+	  bb = BLOCK_FOR_INSN (insn);
+	  bb->aux = last_block;
+	  last_block = bb;
+	  last_jump = insn;
+	}
+      else if (INSN_P (insn) && last_jump != NULL_RTX)
+	{
+	  classification = haifa_classify_insn (insn);
+	  prev = last_jump;
+
+	  switch (classification)
+	    {
+	    case PFREE_CANDIDATE:
+	      if (flag_schedule_speculative_load)
+		{
+		  bb = earliest_block_with_similiar_load (last_block, insn);
+		  if (bb)
+		    {
+		      bb = (basic_block) bb->aux;
+		      if (!bb)
+			break;
+		      prev = BB_END (bb);
+		    }
+		}
+	      /* Fall through.  */
+	    case TRAP_RISKY:
+	    case IRISKY:
+	    case PRISKY_CANDIDATE:
+	      /* ??? We could implement better checking PRISKY_CANDIDATEs
+		 analogous to sched-rgn.c.  */
+	      /* We can not change the mode of the backward
+		 dependency because REG_DEP_ANTI has the lowest
+		 rank.  */
+	      if (! sched_insns_conditions_mutex_p (insn, prev))
+		{
+		  if ((current_sched_info->flags & DO_SPECULATION)
+		      && (spec_info->mask & BEGIN_CONTROL))
+		    {
+		      dep_def _dep, *dep = &_dep;
+
+		      init_dep (dep, prev, insn, REG_DEP_ANTI);
+
+		      if (current_sched_info->flags & USE_DEPS_LIST)
+			{
+			  DEP_STATUS (dep) = set_dep_weak (DEP_ANTI, BEGIN_CONTROL,
+							   MAX_DEP_WEAK);
+
+			}
+		      sd_add_or_update_dep (dep, false);
+		    }
+		  else
+		    add_dependence (insn, prev, REG_DEP_CONTROL);
+		}
+
+	      break;
+
+	    default:
+	      break;
+	    }
+	}
+    }
   /* Maintain the invariant that bb->aux is clear after use.  */
   while (last_block)
     {
@@ -504,7 +518,7 @@ schedule_ebb (rtx head, rtx tail)
 
   /* Make ready list big enough to hold all the instructions from the ebb.  */
   sched_extend_ready_list (rgn_n_insns);
-  schedule_block (&target_bb);
+  schedule_block (&target_bb, NULL);
   /* Free ready list.  */
   sched_finish_ready_list ();
 
diff --git a/gcc/sched-int.h b/gcc/sched-int.h
index 9077c88..24d44b3 100644
--- a/gcc/sched-int.h
+++ b/gcc/sched-int.h
@@ -197,7 +197,7 @@ extern struct ready_list ready;
 
 extern int max_issue (struct ready_list *, int, state_t, bool, int *);
 
-extern void ebb_compute_jump_reg_dependencies (rtx, regset, regset, regset);
+extern void ebb_compute_jump_reg_dependencies (rtx, regset);
 
 extern edge find_fallthru_edge_from (basic_block);
 
@@ -239,6 +239,9 @@ struct _dep
   /* Dependency status.  This field holds all dependency types and additional
      information for speculative dependencies.  */
   ds_t status;
+
+  /* Cached cost of the dependency.  */
+  int cost;
 };
 
 typedef struct _dep dep_def;
@@ -248,6 +251,9 @@ typedef dep_def *dep_t;
 #define DEP_CON(D) ((D)->con)
 #define DEP_TYPE(D) ((D)->type)
 #define DEP_STATUS(D) ((D)->status)
+#define DEP_COST(D) ((D)->cost)
+
+#define UNKNOWN_DEP_COST INT_MIN
 
 /* Functions to work with dep.  */
 
@@ -442,6 +448,7 @@ struct deps_reg
   rtx uses;
   rtx sets;
   rtx implicit_sets;
+  rtx control_uses;
   rtx clobbers;
   int uses_length;
   int clobbers_length;
@@ -471,6 +478,9 @@ struct deps_desc
   /* An EXPR_LIST containing all MEM rtx's which are pending writes.  */
   rtx pending_write_mems;
 
+  /* An INSN_LIST containing all jump insns.  */
+  rtx pending_jump_insns;
+
   /* We must prevent the above lists from ever growing too large since
      the number of dependencies produced is at least O(N*N),
      and execution time is at least O(4*N*N), as a function of the
@@ -482,8 +492,9 @@ struct deps_desc
   /* Indicates the length of the pending_write list.  */
   int pending_write_list_length;
 
-  /* Length of the pending memory flush list. Large functions with no
-     calls may build up extremely large lists.  */
+  /* Length of the pending memory flush list plus the length of the pending
+     jump insn list.  Large functions with no calls may build up extremely
+     large lists.  */
   int pending_flush_length;
 
   /* The last insn upon which all memory references must depend.
@@ -514,6 +525,9 @@ struct deps_desc
      scheduling is done.  */
   rtx sched_before_next_call;
 
+  /* Similarly, a list of insns which should not cross a branch.  */
+  rtx sched_before_next_jump;
+
   /* Used to keep post-call pseudo/hard reg movements together with
      the call.  */
   enum post_call_group in_post_call_group_p;
@@ -535,9 +549,6 @@ struct deps_desc
      in reg_last[N].{uses,sets,clobbers}.  */
   regset_head reg_last_in_use;
 
-  /* Element N is set for each register that is conditionally set.  */
-  regset_head reg_conditional_sets;
-
   /* Shows the last value of reg_pending_barrier associated with the insn.  */
   enum reg_pending_barrier_mode last_reg_pending_barrier;
 
@@ -605,10 +616,15 @@ struct haifa_sched_info
      parameter == 0) or removed (second parameter == 1).  */
   void (*add_remove_insn) (rtx, int);
 
-  /* Called to notify frontend that instruction is being scheduled.
-     The first parameter - instruction to scheduled, the second parameter -
-     last scheduled instruction.  */
-  void (*begin_schedule_ready) (rtx, rtx);
+  /* Called to notify the frontend that instruction INSN is being
+     scheduled.  */
+  void (*begin_schedule_ready) (rtx insn);
+
+  /* Called to notify the frontend that an instruction INSN is about to be
+     moved to its correct place in the final schedule.  This is done for all
+     insns in order of the schedule.  LAST indicates the last scheduled
+     instruction.  */
+  void (*begin_move_insn) (rtx insn, rtx last);
 
   /* If the second parameter is not NULL, return nonnull value, if the
      basic block should be advanced.
@@ -616,6 +632,13 @@ struct haifa_sched_info
      The first parameter is the current basic block in EBB.  */
   basic_block (*advance_target_bb) (basic_block, rtx);
 
+  /* Allocate memory, store the frontend scheduler state in it, and
+     return it.  */
+  void *(*save_state) (void);
+  /* Restore frontend scheduler state from the argument, and free the
+     memory.  */
+  void (*restore_state) (void *);
+
   /* ??? FIXME: should use straight bitfields inside sched_info instead of
      this flag field.  */
   unsigned int flags;
@@ -698,6 +721,21 @@ struct _haifa_deps_insn_data
      search in 'forw_deps'.  */
   deps_list_t resolved_forw_deps;
 
+  /* If the insn is conditional (either through COND_EXEC, or because
+     it is a conditional branch), this records the condition.  NULL
+     for insns that haven't been seen yet or don't have a condition;
+     const_true_rtx to mark an insn without a condition, or with a
+     condition that has been clobbered by a subsequent insn.  */
+  rtx cond;
+
+  /* For a conditional insn, a list of insns that could set the condition
+     register.  Used when generating control dependencies.  */
+  rtx cond_deps;
+
+  /* True if the condition in 'cond' should be reversed to get the actual
+     condition.  */
+  unsigned int reverse_cond : 1;
+
   /* Some insns (e.g. call) are not allowed to move across blocks.  */
   unsigned int cant_move : 1;
 };
@@ -764,10 +802,18 @@ struct _haifa_insn_data
      used to note timing constraints for the insns in the pending list.  */
   int tick;
 
+  /* For insns that are scheduled at a fixed difference from another,
+     this records the tick in which they must be ready.  */
+  int exact_tick;
+
   /* INTER_TICK is used to adjust INSN_TICKs of instructions from the
      subsequent blocks in a region.  */
   int inter_tick;
 
+  /* Used temporarily to estimate an INSN_TICK value for an insn given
+     current knowledge.  */
+  int tick_estimate;
+
   /* See comment on QUEUE_INDEX macro in haifa-sched.c.  */
   int queue_index;
 
@@ -777,6 +823,18 @@ struct _haifa_insn_data
      moved load insn and this one.  */
   unsigned int fed_by_spec_load : 1;
   unsigned int is_load_insn : 1;
+  /* Nonzero if this insn has negative-cost forward dependencies against
+     an already scheduled insn.  */
+  unsigned int feeds_backtrack_insn : 1;
+
+  /* Nonzero if this insn is a shadow of another, scheduled after a fixed
+     delay.  We only emit shadows at the end of a cycle, with no other
+     real insns following them.  */
+  unsigned int shadow_p : 1;
+
+  /* Used internally in unschedule_insns_until to mark insns that must have
+     their TODO_SPEC recomputed.  */
+  unsigned int must_recompute_spec : 1;
 
   /* '> 0' if priority is valid,
      '== 0' if priority was not yet computed,
@@ -798,6 +856,10 @@ struct _haifa_insn_data
   /* Original pattern of the instruction.  */
   rtx orig_pat;
 
+  /* For insns with DEP_CONTROL dependencies, the predicated pattern if it
+     was ever successfully constructed.  */
+  rtx predicated_pat;
+
   /* The following array contains info how the insn increases register
      pressure.  There is an element for each cover class of pseudos
      referenced in insns.  */
@@ -857,6 +919,9 @@ extern VEC(haifa_deps_insn_data_def, heap) *h_d_i_d;
 #define INSN_RESOLVED_FORW_DEPS(INSN) (HDID (INSN)->resolved_forw_deps)
 #define INSN_HARD_BACK_DEPS(INSN) (HDID (INSN)->hard_back_deps)
 #define INSN_SPEC_BACK_DEPS(INSN) (HDID (INSN)->spec_back_deps)
+#define INSN_CACHED_COND(INSN)	(HDID (INSN)->cond)
+#define INSN_REVERSE_COND(INSN) (HDID (INSN)->reverse_cond)
+#define INSN_COND_DEPS(INSN)	(HDID (INSN)->cond_deps)
 #define CANT_MOVE(INSN)	(HDID (INSN)->cant_move)
 #define CANT_MOVE_BY_LUID(LUID)	(VEC_index (haifa_deps_insn_data_def, h_d_i_d, \
                                             LUID)->cant_move)
@@ -870,6 +935,7 @@ extern VEC(haifa_deps_insn_data_def, heap) *h_d_i_d;
 #define CHECK_SPEC(INSN) (HID (INSN)->check_spec)
 #define RECOVERY_BLOCK(INSN) (HID (INSN)->recovery_block)
 #define ORIG_PAT(INSN) (HID (INSN)->orig_pat)
+#define PREDICATED_PAT(INSN) (HID (INSN)->predicated_pat)
 
 /* INSN is either a simple or a branchy speculation check.  */
 #define IS_SPECULATION_CHECK_P(INSN) \
@@ -909,10 +975,11 @@ extern VEC(haifa_deps_insn_data_def, heap) *h_d_i_d;
 /* We exclude sign bit.  */
 #define BITS_PER_DEP_STATUS (HOST_BITS_PER_INT - 1)
 
-/* First '4' stands for 3 dep type bits and HARD_DEP bit.
+/* First '6' stands for 4 dep type bits and the HARD_DEP and DEP_CANCELLED
+   bits.
    Second '4' stands for BEGIN_{DATA, CONTROL}, BE_IN_{DATA, CONTROL}
    dep weakness.  */
-#define BITS_PER_DEP_WEAK ((BITS_PER_DEP_STATUS - 4) / 4)
+#define BITS_PER_DEP_WEAK ((BITS_PER_DEP_STATUS - 6) / 4)
 
 /* Mask of speculative weakness in dep_status.  */
 #define DEP_WEAK_MASK ((1 << BITS_PER_DEP_WEAK) - 1)
@@ -986,13 +1053,16 @@ enum SPEC_TYPES_OFFSETS {
 #define DEP_TRUE (((ds_t) 1) << (BE_IN_CONTROL_BITS_OFFSET + BITS_PER_DEP_WEAK))
 #define DEP_OUTPUT (DEP_TRUE << 1)
 #define DEP_ANTI (DEP_OUTPUT << 1)
+#define DEP_CONTROL (DEP_ANTI << 1)
 
-#define DEP_TYPES (DEP_TRUE | DEP_OUTPUT | DEP_ANTI)
+#define DEP_TYPES (DEP_TRUE | DEP_OUTPUT | DEP_ANTI | DEP_CONTROL)
 
 /* Instruction has non-speculative dependence.  This bit represents the
    property of an instruction - not the one of a dependence.
    Therefore, it can appear only in TODO_SPEC field of an instruction.  */
-#define HARD_DEP (DEP_ANTI << 1)
+#define HARD_DEP (DEP_CONTROL << 1)
+
+#define DEP_CANCELLED (HARD_DEP << 1)
 
 /* This represents the results of calling sched-deps.c functions,
    which modify dependencies.  */
@@ -1017,7 +1087,9 @@ enum SCHED_FLAGS {
      Results in generation of data and control speculative dependencies.
      Requires USE_DEPS_LIST set.  */
   DO_SPECULATION = USE_DEPS_LIST << 1,
-  SCHED_RGN = DO_SPECULATION << 1,
+  DO_BACKTRACKING = DO_SPECULATION << 1,
+  DO_PREDICATION = DO_BACKTRACKING << 1,
+  SCHED_RGN = DO_PREDICATION << 1,
   SCHED_EBB = SCHED_RGN << 1,
   /* Scheduler can possibly create new basic blocks.  Used for assertions.  */
   NEW_BBS = SCHED_EBB << 1,
@@ -1123,7 +1195,7 @@ struct sched_deps_info_def
   /* Called when computing dependencies for a JUMP_INSN.  This function
      should store the set of registers that must be considered as set by
      the jump in the regset.  */
-  void (*compute_jump_reg_dependencies) (rtx, regset, regset, regset);
+  void (*compute_jump_reg_dependencies) (rtx, regset);
 
   /* Start analyzing insn.  */
   void (*start_insn) (rtx);
@@ -1178,6 +1250,7 @@ extern struct sched_deps_info_def *sched_deps_info;
 
 
 /* Functions in sched-deps.c.  */
+extern rtx sched_get_reverse_condition_uncached (const_rtx);
 extern bool sched_insns_conditions_mutex_p (const_rtx, const_rtx);
 extern bool sched_insn_is_legitimate_for_speculation_p (const_rtx, ds_t);
 extern void add_dependence (rtx, rtx, enum reg_note);
@@ -1232,7 +1305,7 @@ extern int dep_cost (dep_t);
 extern int set_priorities (rtx, rtx);
 
 extern void sched_setup_bb_reg_pressure_info (basic_block, rtx);
-extern void schedule_block (basic_block *);
+extern bool schedule_block (basic_block *, state_t);
 
 extern int cycle_issued_insns;
 extern int issue_rate;
@@ -1303,7 +1376,14 @@ extern int *ebb_head;
 extern int current_nr_blocks;
 extern int current_blocks;
 extern int target_bb;
-
+extern bool sched_no_dce;
+
+extern void set_modulo_params (int, int, int, int);
+extern void record_delay_slot_pair (rtx, rtx, int, int);
+extern rtx real_insn_for_shadow (rtx);
+extern void discard_delay_pairs_above (int);
+extern void free_delay_pairs (void);
+extern void add_delay_dependencies (rtx);
 extern bool sched_is_disabled_for_current_region_p (void);
 extern void sched_rgn_init (bool);
 extern void sched_rgn_finish (void);
@@ -1477,6 +1557,7 @@ extern dep_t sd_find_dep_between (rtx, rtx, bool);
 extern void sd_add_dep (dep_t, bool);
 extern enum DEPS_ADJUST_RESULT sd_add_or_update_dep (dep_t, bool);
 extern void sd_resolve_dep (sd_iterator_def);
+extern void sd_unresolve_dep (sd_iterator_def);
 extern void sd_copy_back_deps (rtx, rtx, bool);
 extern void sd_delete_dep (sd_iterator_def);
 extern void sd_debug_lists (rtx, sd_list_types_def);
@@ -1490,3 +1571,4 @@ extern void print_pattern (char *, const_rtx, int);
 extern void print_value (char *, const_rtx, int);
 
 #endif /* GCC_SCHED_INT_H */
+
diff --git a/gcc/sched-rgn.c b/gcc/sched-rgn.c
index e6400b6..1ef9da6 100644
--- a/gcc/sched-rgn.c
+++ b/gcc/sched-rgn.c
@@ -126,6 +126,9 @@ int current_blocks;
 static basic_block *bblst_table;
 static int bblst_size, bblst_last;
 
+static char *bb_state_array;
+static state_t *bb_state;
+
 /* Target info declarations.
 
    The block currently being scheduled is referred to as the "target" block,
@@ -234,7 +237,6 @@ static void add_branch_dependences (rtx, rtx);
 static void compute_block_dependences (int);
 
 static void schedule_region (int);
-static rtx concat_INSN_LIST (rtx, rtx);
 static void concat_insn_mem_list (rtx, rtx, rtx *, rtx *);
 static void propagate_deps (int, struct deps_desc *);
 static void free_pending_lists (void);
@@ -2068,12 +2070,12 @@ static int sched_n_insns;
 /* Implementations of the sched_info functions for region scheduling.  */
 static void init_ready_list (void);
 static int can_schedule_ready_p (rtx);
-static void begin_schedule_ready (rtx, rtx);
+static void begin_schedule_ready (rtx);
 static ds_t new_ready (rtx, ds_t);
 static int schedule_more_p (void);
 static const char *rgn_print_insn (const_rtx, int);
 static int rgn_rank (rtx, rtx);
-static void compute_jump_reg_dependencies (rtx, regset, regset, regset);
+static void compute_jump_reg_dependencies (rtx, regset);
 
 /* Functions for speculative scheduling.  */
 static void rgn_add_remove_insn (rtx, int);
@@ -2163,7 +2165,7 @@ can_schedule_ready_p (rtx insn)
    can_schedule_ready_p () differs from the one passed to
    begin_schedule_ready ().  */
 static void
-begin_schedule_ready (rtx insn, rtx last ATTRIBUTE_UNUSED)
+begin_schedule_ready (rtx insn)
 {
   /* An interblock motion?  */
   if (INSN_BB (insn) != target_bb)
@@ -2306,16 +2308,12 @@ contributes_to_priority (rtx next, rtx insn)
   return BLOCK_TO_BB (BLOCK_NUM (next)) == BLOCK_TO_BB (BLOCK_NUM (insn));
 }
 
-/* INSN is a JUMP_INSN, COND_SET is the set of registers that are
-   conditionally set before INSN.  Store the set of registers that
-   must be considered as used by this jump in USED and that of
-   registers that must be considered as set in SET.  */
+/* INSN is a JUMP_INSN.  Store the set of registers that must be
+   considered as used by this jump in USED.  */
 
 static void
 compute_jump_reg_dependencies (rtx insn ATTRIBUTE_UNUSED,
-			       regset cond_exec ATTRIBUTE_UNUSED,
-			       regset used ATTRIBUTE_UNUSED,
-			       regset set ATTRIBUTE_UNUSED)
+			       regset used ATTRIBUTE_UNUSED)
 {
   /* Nothing to do here, since we postprocess jumps in
      add_branch_dependences.  */
@@ -2380,7 +2378,9 @@ static const struct haifa_sched_info rgn_const_sched_info =
 
   rgn_add_remove_insn,
   begin_schedule_ready,
+  NULL,
   advance_target_bb,
+  NULL, NULL,
   SCHED_RGN
 };
 
@@ -2565,20 +2565,6 @@ add_branch_dependences (rtx head, rtx tail)
 
 static struct deps_desc *bb_deps;
 
-/* Duplicate the INSN_LIST elements of COPY and prepend them to OLD.  */
-
-static rtx
-concat_INSN_LIST (rtx copy, rtx old)
-{
-  rtx new_rtx = old;
-  for (; copy ; copy = XEXP (copy, 1))
-    {
-      new_rtx = alloc_INSN_LIST (XEXP (copy, 0), new_rtx);
-      PUT_REG_NOTE_KIND (new_rtx, REG_NOTE_KIND (copy));
-    }
-  return new_rtx;
-}
-
 static void
 concat_insn_mem_list (rtx copy_insns, rtx copy_mems, rtx *old_insns_p,
 		      rtx *old_mems_p)
@@ -2632,6 +2618,9 @@ deps_join (struct deps_desc *succ_deps, struct deps_desc *pred_deps)
                         &succ_deps->pending_write_insns,
                         &succ_deps->pending_write_mems);
 
+  succ_deps->pending_jump_insns
+    = concat_INSN_LIST (pred_deps->pending_jump_insns,
+                        succ_deps->pending_jump_insns);
   succ_deps->last_pending_memory_flush
     = concat_INSN_LIST (pred_deps->last_pending_memory_flush,
                         succ_deps->last_pending_memory_flush);
@@ -2683,12 +2672,14 @@ propagate_deps (int bb, struct deps_desc *pred_deps)
   bb_deps[bb].pending_read_mems = pred_deps->pending_read_mems;
   bb_deps[bb].pending_write_insns = pred_deps->pending_write_insns;
   bb_deps[bb].pending_write_mems = pred_deps->pending_write_mems;
+  bb_deps[bb].pending_jump_insns = pred_deps->pending_jump_insns;
 
   /* Can't allow these to be freed twice.  */
   pred_deps->pending_read_insns = 0;
   pred_deps->pending_read_mems = 0;
   pred_deps->pending_write_insns = 0;
   pred_deps->pending_write_mems = 0;
+  pred_deps->pending_jump_insns = 0;
 }
 
 /* Compute dependences inside bb.  In a multiple blocks region:
@@ -2767,6 +2758,7 @@ free_pending_lists (void)
       free_INSN_LIST_list (&bb_deps[bb].pending_write_insns);
       free_EXPR_LIST_list (&bb_deps[bb].pending_read_mems);
       free_EXPR_LIST_list (&bb_deps[bb].pending_write_mems);
+      free_INSN_LIST_list (&bb_deps[bb].pending_jump_insns);
     }
 }
 
@@ -2997,9 +2989,21 @@ schedule_region (int rgn)
       curr_bb = first_bb;
       if (dbg_cnt (sched_block))
         {
-          schedule_block (&curr_bb);
+	  edge f;
+
+          schedule_block (&curr_bb, bb_state[first_bb->index]);
           gcc_assert (EBB_FIRST_BB (bb) == first_bb);
           sched_rgn_n_insns += sched_n_insns;
+	  f = find_fallthru_edge (last_bb->succs);
+	  if (f && f->probability * 100 / REG_BR_PROB_BASE >=
+	      PARAM_VALUE (PARAM_SCHED_STATE_EDGE_PROB_CUTOFF))
+	    {
+	      memcpy (bb_state[f->dest->index], curr_state,
+		      dfa_state_size);
+	      if (sched_verbose >= 5)
+		fprintf (sched_dump, "saving state for edge %d->%d\n",
+			 f->src->index, f->dest->index);
+	    }
         }
       else
         {
@@ -3032,6 +3036,8 @@ schedule_region (int rgn)
 void
 sched_rgn_init (bool single_blocks_p)
 {
+  int i;
+
   min_spec_prob = ((PARAM_VALUE (PARAM_MIN_SPEC_PROB) * REG_BR_PROB_BASE)
 		    / 100);
 
@@ -3043,6 +3049,15 @@ sched_rgn_init (bool single_blocks_p)
   CONTAINING_RGN (ENTRY_BLOCK) = -1;
   CONTAINING_RGN (EXIT_BLOCK) = -1;
 
+  bb_state_array = (char *) xmalloc (last_basic_block * dfa_state_size);
+  bb_state = XNEWVEC (state_t, last_basic_block);
+  for (i = 0; i < last_basic_block; i++)
+    {
+      bb_state[i] = (state_t) (bb_state_array + i * dfa_state_size);
+      
+      state_reset (bb_state[i]);
+    }
+
   /* Compute regions for scheduling.  */
   if (single_blocks_p
       || n_basic_blocks == NUM_FIXED_BLOCKS + 1
@@ -3079,6 +3094,9 @@ sched_rgn_init (bool single_blocks_p)
 void
 sched_rgn_finish (void)
 {
+  free (bb_state_array);
+  free (bb_state);
+
   /* Reposition the prologue and epilogue notes in case we moved the
      prologue/epilogue insns.  */
   if (reload_completed)
diff --git a/gcc/sched-vis.c b/gcc/sched-vis.c
index 25a0b60..8c15788 100644
--- a/gcc/sched-vis.c
+++ b/gcc/sched-vis.c
@@ -554,6 +554,9 @@ print_pattern (char *buf, const_rtx x, int verbose)
     case RETURN:
       sprintf (buf, "return");
       break;
+    case SIMPLE_RETURN:
+      sprintf (buf, "simple_return");
+      break;
     case CALL:
       print_exp (buf, x, verbose);
       break;
diff --git a/gcc/sel-sched-ir.c b/gcc/sel-sched-ir.c
index 38aaff6..4614d2e 100644
--- a/gcc/sel-sched-ir.c
+++ b/gcc/sel-sched-ir.c
@@ -1,5 +1,6 @@
 /* Instruction scheduling pass.  Selective scheduler and pipeliner.
-   Copyright (C) 2006, 2007, 2008, 2009, 2010 Free Software Foundation, Inc.
+   Copyright (C) 2006, 2007, 2008, 2009, 2010, 2011
+   Free Software Foundation, Inc.
 
 This file is part of GCC.
 
@@ -5654,7 +5655,12 @@ static struct haifa_sched_info sched_sel_haifa_sched_info =
 
   NULL, /* add_remove_insn */
   NULL, /* begin_schedule_ready */
+  NULL, /* begin_move_insn */
   NULL, /* advance_target_bb */
+
+  NULL,
+  NULL,
+
   SEL_SCHED | NEW_BBS
 };
 
diff --git a/gcc/sel-sched.c b/gcc/sel-sched.c
index bc390d5..7049c0b 100644
--- a/gcc/sel-sched.c
+++ b/gcc/sel-sched.c
@@ -7695,8 +7695,8 @@ sel_global_init (void)
   sel_setup_sched_infos ();
   setup_sched_dump ();
 
-  sched_rgn_init (false);
   sched_init ();
+  sched_rgn_init (false);
 
   sched_init_bbs ();
   /* Reset AFTER_RECOVERY if it has been set by the 1st scheduler pass.  */
diff --git a/gcc/stmt.c b/gcc/stmt.c
index 82ce350..f2d4a5f 100644
--- a/gcc/stmt.c
+++ b/gcc/stmt.c
@@ -119,7 +119,8 @@ static void expand_value_return (rtx);
 static int estimate_case_costs (case_node_ptr);
 static bool lshift_cheap_p (void);
 static int case_bit_test_cmp (const void *, const void *);
-static void emit_case_bit_tests (tree, tree, tree, tree, case_node_ptr, rtx);
+static void emit_case_bit_tests (tree, tree, tree, tree, case_node_ptr, tree,
+                                 rtx, basic_block);
 static void balance_case_nodes (case_node_ptr *, case_node_ptr);
 static int node_has_low_bound (case_node_ptr, tree);
 static int node_has_high_bound (case_node_ptr, tree);
@@ -1757,7 +1758,8 @@ expand_return (tree retval)
 
 	  /* Use bitpos for the source extraction (left justified) and
 	     xbitpos for the destination store (right justified).  */
-	  store_bit_field (dst, bitsize, xbitpos % BITS_PER_WORD, word_mode,
+	  store_bit_field (dst, bitsize, xbitpos % BITS_PER_WORD, false,
+			   word_mode,
 			   extract_bit_field (src, bitsize,
 					      bitpos % BITS_PER_WORD, 1, false,
 					      NULL_RTX, word_mode, word_mode));
@@ -2111,8 +2113,11 @@ struct case_bit_test
 {
   HOST_WIDE_INT hi;
   HOST_WIDE_INT lo;
+  HOST_WIDE_INT rev_hi;
+  HOST_WIDE_INT rev_lo;
   rtx label;
   int bits;
+  int prob;
 };
 
 /* Determine whether "1 << x" is relatively cheap in word_mode.  */
@@ -2154,10 +2159,195 @@ case_bit_test_cmp (const void *p1, const void *p2)
   return CODE_LABEL_NUMBER (d2->label) - CODE_LABEL_NUMBER (d1->label);
 }
 
+/* Emit a bit test and a conditional jump.  */
+
+static void
+emit_case_bit_test_jump (unsigned int count, rtx index, rtx label,
+                         unsigned int method, HOST_WIDE_INT hi,
+                         HOST_WIDE_INT lo, HOST_WIDE_INT rev_hi,
+                         HOST_WIDE_INT rev_lo)
+{
+  rtx expr;
+
+  if (method == 1)
+    {
+      /* (1 << index). */
+      if (count == 0)
+        index = expand_binop (word_mode, ashl_optab, const1_rtx,
+                              index, NULL_RTX, 1, OPTAB_WIDEN);
+      /* CST.  */
+      expr = immed_double_const (lo, hi, word_mode);
+      /* ((1 << index) & CST).  */
+      expr = expand_binop (word_mode, and_optab, index, expr,
+                           NULL_RTX, 1, OPTAB_WIDEN);
+      /* if (((1 << index) & CST)).   */
+      emit_cmp_and_jump_insns (expr, const0_rtx, NE, NULL_RTX,
+                               word_mode, 1, label);
+    }
+  else if (method == 2)
+    {
+      /* (bit_reverse (CST)) */
+      expr = immed_double_const (rev_lo, rev_hi, word_mode);
+      /* ((bit_reverse (CST)) << index) */
+      expr = expand_binop (word_mode, ashl_optab, expr,
+                           index, NULL_RTX, 1, OPTAB_WIDEN);
+      /* if (((bit_reverse (CST)) << index) < 0).  */
+      emit_cmp_and_jump_insns (expr, const0_rtx, LT, NULL_RTX,
+                               word_mode, 0, label);
+    }
+  else
+    gcc_unreachable ();
+}
+
+/* Return the cost of rtx sequence SEQ.  The sequence is supposed to contain one
+   jump, which has no effect in the cost.  */
+
+static unsigned int
+rtx_seq_cost (rtx seq)
+{
+  rtx one;
+  unsigned int nr_branches = 0;
+  unsigned int sum = 0, cost;
+
+  for (one = seq; one != NULL_RTX; one = NEXT_INSN (one))
+    if (JUMP_P (one))
+      nr_branches++;
+    else
+      {
+        cost = insn_rtx_cost (PATTERN (one), optimize_insn_for_speed_p ());
+        if (dump_file)
+          {
+            print_rtl_single (dump_file, one);
+            fprintf (dump_file, "cost: %u\n", cost);
+          }
+        sum += cost;
+      }
+
+  gcc_assert (nr_branches == 1);
+
+  if (dump_file)
+    fprintf (dump_file, "total cost: %u\n", sum);
+  return sum;
+}
+
+/* Generate the rtx sequences for 2 bit test expansion methods, measure the cost
+   and choose the cheapest.  */
+
+static unsigned int
+choose_case_bit_test_expand_method (rtx label)
+{
+  rtx seq, index;
+  unsigned int cost[2];
+  static bool method_known = false;
+  static unsigned int method;
+
+  /* If already known, return the method.  */
+  if (method_known)
+    return method;
+
+  index = gen_rtx_REG (word_mode, 10000);
+
+  for (method = 1; method <= 2; ++method)
+    {
+      start_sequence ();
+      emit_case_bit_test_jump (0, index, label, method, 0, 0x0f0f0f0f, 0,
+                               0x0f0f0f0f);
+      seq = get_insns ();
+      end_sequence ();
+      cost[method - 1] = rtx_seq_cost (seq);
+    }
+
+  /* Determine method based on heuristic.  */
+  method = ((cost[1] < cost[0]) ? 1 : 0) + 1;
+
+  /* Save and return method.  */
+  method_known = true;
+  return method;
+}
+
+/* Get the edge probability of the edge from SRC to LABEL_DECL.  */
+
+static int
+get_label_prob (basic_block src, tree label_decl)
+{
+  basic_block dest;
+  int prob = 0, nr_prob = 0;
+  unsigned int i;
+  edge e;
+
+  if (label_decl == NULL_TREE)
+    return 0;
+
+  dest = VEC_index (basic_block, label_to_block_map,
+                    LABEL_DECL_UID (label_decl));
+
+  for (i = 0; i < EDGE_COUNT (src->succs); ++i)
+    {
+      e = EDGE_SUCC (src, i);
+
+      if (e->dest != dest)
+        continue;
+
+      prob += e->probability;
+      nr_prob++;
+    }
+
+  gcc_assert (nr_prob == 1);
+
+  return prob;
+}
+
+/* Add probability note with scaled PROB to JUMP and update INV_SCALE.  This
+   function is intended to be used with a series of conditional jumps to L[i]
+   where the probabilities p[i] to get to L[i] are known, and the jump
+   probabilities j[i] need to be computed.
+
+   The algorithm to calculate the probabilities is
+
+   scale = REG_BR_PROB_BASE;
+   for (i = 0; i < n; ++i)
+     {
+       j[i] = p[i] * scale / REG_BR_PROB_BASE;
+       f[i] = REG_BR_PROB_BASE - j[i];
+       scale = scale / (f[i] / REG_BR_PROB_BASE);
+     }
+
+   The implementation uses inv_scale (REG_BR_PROB_BASE / scale) instead of
+   scale, because scale tends to grow bigger than REG_BR_PROB_BASE.  */
+
+static void
+set_jump_prob (rtx jump, int prob, int *inv_scale)
+{
+  /* j[i] = p[i] * scale / REG_BR_PROB_BASE.  */
+  int jump_prob = (*inv_scale > 0
+                   ? prob * REG_BR_PROB_BASE / *inv_scale
+                   : REG_BR_PROB_BASE / 2);
+  /* f[i] = REG_BR_PROB_BASE - j[i].  */
+  int fallthrough_prob = REG_BR_PROB_BASE - jump_prob;
+
+  gcc_assert (jump_prob <= REG_BR_PROB_BASE);
+  add_reg_note (jump, REG_BR_PROB, GEN_INT (jump_prob));
+
+  /* scale = scale / (f[i] / REG_BR_PROB_BASE).  */
+  *inv_scale = *inv_scale * fallthrough_prob / REG_BR_PROB_BASE;
+}
+
+/* Set bit in hwi hi/lo pair.  */
+
+static void
+set_bit (HOST_WIDE_INT *hi, HOST_WIDE_INT *lo, unsigned int j)
+{
+  if (j >= HOST_BITS_PER_WIDE_INT)
+    *hi |= (HOST_WIDE_INT) 1 << (j - HOST_BITS_PER_INT);
+  else
+    *lo |= (HOST_WIDE_INT) 1 << j;
+}
+
 /*  Expand a switch statement by a short sequence of bit-wise
     comparisons.  "switch(x)" is effectively converted into
-    "if ((1 << (x-MINVAL)) & CST)" where CST and MINVAL are
-    integer constants.
+    "if ((1 << (x-MINVAL)) & CST)" or
+    "if (((bit_reverse (CST)) << (x-MINVAL)) < 0)", where CST
+    and MINVAL are integer constants.
 
     INDEX_EXPR is the value being switched on, which is of
     type INDEX_TYPE.  MINVAL is the lowest case value of in
@@ -2171,14 +2361,18 @@ case_bit_test_cmp (const void *p1, const void *p2)
 
 static void
 emit_case_bit_tests (tree index_type, tree index_expr, tree minval,
-		     tree range, case_node_ptr nodes, rtx default_label)
+		     tree range, case_node_ptr nodes, tree default_label_decl,
+		     rtx default_label, basic_block bb)
 {
   struct case_bit_test test[MAX_CASE_BIT_TESTS];
   enum machine_mode mode;
   rtx expr, index, label;
   unsigned int i,j,lo,hi;
   struct case_node *n;
-  unsigned int count;
+  unsigned int count, method;
+  int inv_scale = REG_BR_PROB_BASE;
+  int default_prob = get_label_prob (bb, default_label_decl);
+  int total_prob = default_prob;
 
   count = 0;
   for (n = nodes; n; n = n->right)
@@ -2193,8 +2387,12 @@ emit_case_bit_tests (tree index_type, tree index_expr, tree minval,
 	  gcc_assert (count < MAX_CASE_BIT_TESTS);
 	  test[i].hi = 0;
 	  test[i].lo = 0;
+	  test[i].rev_hi = 0;
+	  test[i].rev_lo = 0;
 	  test[i].label = label;
 	  test[i].bits = 1;
+	  test[i].prob = get_label_prob (bb, n->code_label);
+	  total_prob += test[i].prob;
 	  count++;
 	}
       else
@@ -2205,10 +2403,11 @@ emit_case_bit_tests (tree index_type, tree index_expr, tree minval,
       hi = tree_low_cst (fold_build2 (MINUS_EXPR, index_type,
 				      n->high, minval), 1);
       for (j = lo; j <= hi; j++)
-        if (j >= HOST_BITS_PER_WIDE_INT)
-	  test[i].hi |= (HOST_WIDE_INT) 1 << (j - HOST_BITS_PER_INT);
-	else
-	  test[i].lo |= (HOST_WIDE_INT) 1 << j;
+        {
+          set_bit (&test[i].hi, &test[i].lo, j);
+          set_bit (&test[i].rev_hi, &test[i].rev_lo,
+                   GET_MODE_BITSIZE (word_mode) - j - 1);
+        }
     }
 
   qsort (test, count, sizeof(*test), case_bit_test_cmp);
@@ -2222,20 +2421,28 @@ emit_case_bit_tests (tree index_type, tree index_expr, tree minval,
   mode = TYPE_MODE (index_type);
   expr = expand_normal (range);
   if (default_label)
-    emit_cmp_and_jump_insns (index, expr, GTU, NULL_RTX, mode, 1,
-			     default_label);
+    {
+      emit_cmp_and_jump_insns (index, expr, GTU, NULL_RTX, mode, 1,
+                               default_label);
+      if (profile_status_for_function (cfun) != PROFILE_ABSENT)
+	{
+	  default_prob = default_prob * REG_BR_PROB_BASE / total_prob;
+	  set_jump_prob (get_last_insn (), default_prob / 2, &inv_scale);
+	}
+    }
 
   index = convert_to_mode (word_mode, index, 0);
-  index = expand_binop (word_mode, ashl_optab, const1_rtx,
-			index, NULL_RTX, 1, OPTAB_WIDEN);
 
+  method = choose_case_bit_test_expand_method (test[0].label);
   for (i = 0; i < count; i++)
     {
-      expr = immed_double_const (test[i].lo, test[i].hi, word_mode);
-      expr = expand_binop (word_mode, and_optab, index, expr,
-			   NULL_RTX, 1, OPTAB_WIDEN);
-      emit_cmp_and_jump_insns (expr, const0_rtx, NE, NULL_RTX,
-			       word_mode, 1, test[i].label);
+      emit_case_bit_test_jump (i, index, test[i].label, method, test[i].hi,
+                               test[i].lo, test[i].rev_hi, test[i].rev_lo);
+      if (profile_status_for_function (cfun) != PROFILE_ABSENT)
+	{
+	  test[i].prob = test[i].prob * REG_BR_PROB_BASE / total_prob;
+	  set_jump_prob (get_last_insn (), test[i].prob, &inv_scale);
+	}
     }
 
   if (default_label)
@@ -2415,7 +2622,8 @@ expand_case (gimple stmt)
 	      range = maxval;
 	    }
 	  emit_case_bit_tests (index_type, index_expr, minval, range,
-			       case_list, default_label);
+			       case_list, default_label_decl, default_label,
+			       gimple_bb (stmt));
 	}
 
       /* If range of values is much bigger than number of values,
diff --git a/gcc/target.def b/gcc/target.def
index 6a34904..f514c20 100644
--- a/gcc/target.def
+++ b/gcc/target.def
@@ -949,6 +949,13 @@ DEFHOOK
 bool, (rtx insn, int x),
 hook_bool_rtx_int_false)
 
+DEFHOOKPOD
+(exposed_pipeline,
+"True if the processor has an exposed pipeline, which means that not just\n\
+the order of instructions is important for correctness when scheduling, but\n\
+also the latencies of operations.",
+bool, false)
+
 HOOK_VECTOR_END (sched)
 
 /* Functions relating to vectorization.  */
@@ -1295,6 +1302,17 @@ DEFHOOK
  void, (void),
  hook_void_void)
 
+ /* Add a __gnu_ prefix to library functions rather than just __.  */
+DEFHOOKPOD
+(libfunc_gnu_prefix,
+ "If false (the default), internal library routines start with two\n\
+underscores.  If set to true, these routines start with @code{__gnu_}\n\
+instead.  E.g., @code{__muldi3} changes to @code{__gnu_muldi3}.  This\n\
+currently only affects functions defined in @file{libgcc2.c}.  If this\n\
+is set to true, the @file{tm.h} file must also\n\
+@code{#define LIBGCC2_GNU_PREFIX}.",
+  bool, false)
+
 /* Given a decl, a section name, and whether the decl initializer
    has relocs, choose attributes for the section.  */
 /* ??? Should be merged with SELECT_SECTION and UNIQUE_SECTION.  */
@@ -1801,6 +1819,15 @@ DEFHOOK
  const char *, (const void *data, size_t sz),
  default_pch_valid_p)
 
+DEFHOOK
+(prepare_pch_save,
+ "Called before writing out a PCH file.  If the target has some\n\
+garbage-collected data that needs to be in a particular state on PCH loads,\n\
+it can use this hook to enforce that state.  Very few targets need\n\
+to do anything here.",
+ void, (void),
+ hook_void_void)
+
 /* If nonnull, this function checks whether a PCH file with the
    given set of target flags can be used.  It returns NULL if so,
    otherwise it returns an error message.  */
@@ -2625,6 +2652,17 @@ DEFHOOK
 
 HOOK_VECTOR_END (target_option)
 
+HOOK_VECTOR (TARGET_ALIGN_HOOKS, target_align_hooks)
+
+/* Function that returns the log2 of the instruction alignment in bytes.  */
+DEFHOOK
+(align_insns,
+ "Return the log2 of the instruction alignment in bytes.",
+ int, (void),
+ hook_int_void_0)
+
+HOOK_VECTOR_END (target_align)
+
 /* For targets that need to mark extra registers as live on entry to
    the function, they should define this target hook and set their
    bits in the bitmap passed in. */
@@ -2634,6 +2672,15 @@ DEFHOOK
  void, (bitmap regs),
  hook_void_bitmap)
 
+/* For targets that have attributes that can affect whether a
+   function's return statements need checking.  For instance a 'naked'
+   function attribute.  */
+DEFHOOK
+(warn_func_result,
+ "True if a function's return statements should be checked for matching the function's return type.  This includes checking for falling off the end of a non-void function.  Return false if no such check should be made.",
+ bool, (void),
+ hook_bool_void_true)
+
 /* Determine the type of unwind info to emit for debugging.  */
 DEFHOOK
 (debug_unwind_info,
diff --git a/gcc/targhooks.c b/gcc/targhooks.c
index b751f64..7988188 100644
--- a/gcc/targhooks.c
+++ b/gcc/targhooks.c
@@ -124,7 +124,7 @@ default_promote_function_mode (const_tree type ATTRIBUTE_UNUSED,
 			       const_tree funtype ATTRIBUTE_UNUSED,
 			       int for_return ATTRIBUTE_UNUSED)
 {
-  if (for_return == 2)
+  if (type != NULL_TREE && for_return == 2)
     return promote_mode (type, mode, punsignedp);
   return mode;
 }
diff --git a/gcc/testsuite/g++.dg/compat/struct-layout-1_generate.c b/gcc/testsuite/g++.dg/compat/struct-layout-1_generate.c
index b6846a9..dfb87a1 100644
--- a/gcc/testsuite/g++.dg/compat/struct-layout-1_generate.c
+++ b/gcc/testsuite/g++.dg/compat/struct-layout-1_generate.c
@@ -46,7 +46,7 @@ const char *dg_options[] = {
 "/* { dg-options \"%s-I%s\" } */\n",
 "/* { dg-options \"%s-I%s -mno-mmx -Wno-abi\" { target i?86-*-* x86_64-*-* } } */\n",
 "/* { dg-options \"%s-I%s -fno-common\" { target hppa*-*-hpux* powerpc*-*-darwin* *-*-mingw32* *-*-cygwin* } } */\n",
-"/* { dg-options \"%s-I%s -mno-mmx -fno-common -Wno-abi\" { target i?86-*-darwin* x86_64-*-darwin* } } */\n",
+"/* { dg-options \"%s-I%s -mno-mmx -fno-common -Wno-abi\" { target i?86-*-darwin* x86_64-*-darwin* i?86-*-mingw32* x86_64-*-mingw32* i?86-*-cygwin* } } */\n",
 "/* { dg-options \"%s-I%s -mno-base-addresses\" { target mmix-*-* } } */\n",
 "/* { dg-options \"%s-I%s -mlongcalls -mtext-section-literals\" { target xtensa*-*-* } } */\n"
 #define NDG_OPTIONS (sizeof (dg_options) / sizeof (dg_options[0]))
diff --git a/gcc/testsuite/g++.dg/ext/arm-fp16/arm-fp16-ops-5.C b/gcc/testsuite/g++.dg/ext/arm-fp16/arm-fp16-ops-5.C
index 92bc8a9..738d26d 100644
--- a/gcc/testsuite/g++.dg/ext/arm-fp16/arm-fp16-ops-5.C
+++ b/gcc/testsuite/g++.dg/ext/arm-fp16/arm-fp16-ops-5.C
@@ -13,3 +13,5 @@
 /* { dg-final { scan-assembler-not "\tbl\t__gnu_h\[a-z\]*_ieee" } } */
 /* { dg-final { scan-assembler-not "\tbl\t__gnu_h2f_ieee" } } */
 /* { dg-final { scan-assembler-not "\tbl\t__gnu_f2h_ieee" } } */
+/* { dg-final { scan-assembler-not "\tbl\t__aeabi_h2f" } } */
+/* { dg-final { scan-assembler-not "\tbl\t__aeabi_f2h" } } */
diff --git a/gcc/testsuite/g++.dg/ext/arm-fp16/arm-fp16-ops-6.C b/gcc/testsuite/g++.dg/ext/arm-fp16/arm-fp16-ops-6.C
index ae40b1e..a0e09c8 100644
--- a/gcc/testsuite/g++.dg/ext/arm-fp16/arm-fp16-ops-6.C
+++ b/gcc/testsuite/g++.dg/ext/arm-fp16/arm-fp16-ops-6.C
@@ -13,3 +13,5 @@
 /* { dg-final { scan-assembler-not "\tbl\t__gnu_h\[a-z\]*_ieee" } } */
 /* { dg-final { scan-assembler-not "\tbl\t__gnu_h2f_ieee" } } */
 /* { dg-final { scan-assembler-not "\tbl\t__gnu_f2h_ieee" } } */
+/* { dg-final { scan-assembler-not "\tbl\t__aeabi_h2f" } } */
+/* { dg-final { scan-assembler-not "\tbl\t__aeabi_f2h" } } */
diff --git a/gcc/testsuite/gcc.c-torture/execute/20101011-1.c b/gcc/testsuite/gcc.c-torture/execute/20101011-1.c
index 7180e68..485e61e 100644
--- a/gcc/testsuite/gcc.c-torture/execute/20101011-1.c
+++ b/gcc/testsuite/gcc.c-torture/execute/20101011-1.c
@@ -22,6 +22,26 @@
   /* Not all Linux kernels deal correctly the breakpoints generated by
      MIPS16 divisions by zero.  They show up as a SIGTRAP instead.  */
 # define DO_TEST 0
+#elif defined (__arm__) && defined (__ARM_EABI__)
+# ifdef __ARM_ARCH_EXT_IDIV__
+  /* Hardware division instructions may not trap, and handle trapping
+     differently anyway.  Skip the test if we have those instructions.  */
+#  define DO_TEST 0
+# else
+#  include <signal.h>
+  /* ARM division-by-zero behaviour is to call a helper function, which
+     can do several different things, depending on requirements.  Emulate
+     the behaviour of other targets here by raising SIGFPE.  */
+int
+__aeabi_idiv0 (int return_value)
+{
+  raise (SIGFPE);
+  return return_value;
+}
+#  define DO_TEST 1
+# endif
+#elif defined (__m68k__) && !defined(__linux__)
+# define DO_TEST 0
 #else
 # define DO_TEST 1
 #endif
--- a/gcc/testsuite/gcc.c-torture/execute/loop-2f.c
+++ b/gcc/testsuite/gcc.c-torture/execute/loop-2f.c
@@ -1,6 +1,5 @@
 #include <limits.h>
 
-#ifdef __unix__ /* ??? Is that good enough? */
 #include <sys/types.h>
 #include <sys/mman.h>
 #include <sys/stat.h>
@@ -18,7 +17,6 @@
 #ifndef MAP_FIXED
 #define MAP_FIXED 0
 #endif
-#endif
 
 #define MAP_START (void *)0x7fff8000
 #define MAP_LEN 0x10000
diff --git a/gcc/testsuite/gcc.c-torture/execute/loop-2f.x b/gcc/testsuite/gcc.c-torture/execute/loop-2f.x
index 334ef9c..ad024dd 100644
--- a/gcc/testsuite/gcc.c-torture/execute/loop-2f.x
+++ b/gcc/testsuite/gcc.c-torture/execute/loop-2f.x
@@ -1,3 +1,9 @@
+load_lib target-supports.exp
+
+if { ! [check_effective_target_mmap] } {
+        return 1
+}
+
 if [istarget "m68k-*-linux*"] {
     # the executable is at the same position the test tries to remap
     return 1
diff --git a/gcc/testsuite/gcc.c-torture/execute/loop-2g.c b/gcc/testsuite/gcc.c-torture/execute/loop-2g.c
index 8792dbf..91af413 100644
--- a/gcc/testsuite/gcc.c-torture/execute/loop-2g.c
+++ b/gcc/testsuite/gcc.c-torture/execute/loop-2g.c
@@ -1,6 +1,5 @@
 #include <limits.h>
 
-#ifdef __unix__ /* ??? Is that good enough? */
 #include <sys/types.h>
 #include <sys/mman.h>
 #include <sys/stat.h>
@@ -18,7 +17,6 @@
 #ifndef MAP_FIXED
 #define MAP_FIXED 0
 #endif
-#endif
 
 #define MAP_START (void *)0x7fff8000
 #define MAP_LEN 0x10000
diff --git a/gcc/testsuite/gcc.c-torture/execute/loop-2g.x b/gcc/testsuite/gcc.c-torture/execute/loop-2g.x
index 334ef9c..ad024dd 100644
--- a/gcc/testsuite/gcc.c-torture/execute/loop-2g.x
+++ b/gcc/testsuite/gcc.c-torture/execute/loop-2g.x
@@ -1,3 +1,9 @@
+load_lib target-supports.exp
+
+if { ! [check_effective_target_mmap] } {
+        return 1
+}
+
 if [istarget "m68k-*-linux*"] {
     # the executable is at the same position the test tries to remap
     return 1
diff --git a/gcc/testsuite/gcc.dg/20030711-1.c b/gcc/testsuite/gcc.dg/20030711-1.c
index e76f54f..7649059 100644
--- a/gcc/testsuite/gcc.dg/20030711-1.c
+++ b/gcc/testsuite/gcc.dg/20030711-1.c
@@ -1,6 +1,6 @@
 /* Test whether strncmp has not been "optimized" into memcmp
    nor any code with memcmp semantics.  */
-/* { dg-do run { target i?86-*-linux* x86_64-*-linux* ia64-*-linux* alpha*-*-linux* powerpc*-*-linux* s390*-*-linux* sparc*-*-linux* *-*-darwin* } } */
+/* { dg-do run { target mmap } } */
 /* { dg-options "-O2" } */
 #include <stddef.h>
 #include <stdio.h>
@@ -8,6 +8,9 @@
 #ifndef MAP_ANONYMOUS
 #define MAP_ANONYMOUS MAP_ANON
 #endif
+#ifndef MAP_ANON
+#define MAP_ANON 0
+#endif
 #include <stdlib.h>
 
 void __attribute__((noinline)) test (const char *p)
diff --git a/gcc/testsuite/gcc.dg/20050826-1.c b/gcc/testsuite/gcc.dg/20050826-1.c
index 9101fb0..e622505 100644
--- a/gcc/testsuite/gcc.dg/20050826-1.c
+++ b/gcc/testsuite/gcc.dg/20050826-1.c
@@ -1,6 +1,6 @@
 /* Test whether strncmp has not been "optimized" into memcmp
    nor any code with memcmp semantics.  */
-/* { dg-do run { target i?86-*-linux* x86_64-*-linux* ia64-*-linux* alpha*-*-linux* powerpc*-*-linux* s390*-*-linux* sparc*-*-linux* *-*-darwin* } } */
+/* { dg-do run { target mmap } } */
 /* { dg-options "-O2" } */
 #include <stddef.h>
 #include <stdio.h>
@@ -9,6 +9,9 @@
 #ifndef MAP_ANONYMOUS
 #define MAP_ANONYMOUS MAP_ANON
 #endif
+#ifndef MAP_ANON
+#define MAP_ANON 0
+#endif
 #include <stdlib.h>
 
     struct Flags {
diff --git a/gcc/testsuite/gcc.dg/20100906-1.c b/gcc/testsuite/gcc.dg/20100906-1.c
index a6541e3..042ba99 100644
--- a/gcc/testsuite/gcc.dg/20100906-1.c
+++ b/gcc/testsuite/gcc.dg/20100906-1.c
@@ -1,5 +1,6 @@
 /* { dg-do run } */
 /* { dg-options "-O2" } */
+/* { dg-options "-O2 -fno-short-enums -Wl,--no-enum-size-warning" {target arm_eabi} } */
 
 /* This testcase got misoptimized by combine due to a wrong setting of
    subst_low_luid in try_combine.  */
diff --git a/gcc/testsuite/gcc.dg/Warray-bounds-3.c b/gcc/testsuite/gcc.dg/Warray-bounds-3.c
index 19cdb8e..cde7e31 100644
--- a/gcc/testsuite/gcc.dg/Warray-bounds-3.c
+++ b/gcc/testsuite/gcc.dg/Warray-bounds-3.c
@@ -1,5 +1,7 @@
 /* { dg-do compile } */
 /* { dg-options "-O2 -Warray-bounds" } */
+/* { dg-options "-O2 -Warray-bounds -fno-unroll-loops" { target arm*-*-* } } */
+
 /* based on PR 31227 */
 
 typedef __SIZE_TYPE__ size_t;
diff --git a/gcc/testsuite/gcc.dg/format/ms_c90-printf-1.c b/gcc/testsuite/gcc.dg/format/ms_c90-printf-1.c
index f99d4c4..cce2342 100644
--- a/gcc/testsuite/gcc.dg/format/ms_c90-printf-1.c
+++ b/gcc/testsuite/gcc.dg/format/ms_c90-printf-1.c
@@ -167,7 +167,7 @@ foo (int i, int i1, int i2, unsigned int u, double d, char *s, void *p,
   */
   printf ("%d", u);
   /* Wrong number of arguments.  */
-  printf ("%d%d", i); /* { dg-warning "arguments" "wrong number of args" } */
+  printf ("%d%d", i); /* { dg-warning "matching" "wrong number of args" } */
   printf ("%d", i, i); /* { dg-warning "arguments" "wrong number of args" } */
   /* Miscellaneous bogus constructions.  */
   printf (""); /* { dg-warning "zero-length" "warning for empty format" } */
diff --git a/gcc/testsuite/gcc.dg/format/ms_c90-scanf-1.c b/gcc/testsuite/gcc.dg/format/ms_c90-scanf-1.c
index 6e2cb10..f52ed20 100644
--- a/gcc/testsuite/gcc.dg/format/ms_c90-scanf-1.c
+++ b/gcc/testsuite/gcc.dg/format/ms_c90-scanf-1.c
@@ -106,7 +106,7 @@ foo (int *ip, unsigned int *uip, short int *hp, unsigned short int *uhp,
   scanf ("%s", cs); /* { dg-warning "constant" "%s writing into const" } */
   scanf ("%p", pcp); /* { dg-warning "constant" "%p writing into const" } */
   /* Wrong number of arguments.  */
-  scanf ("%d%d", ip); /* { dg-warning "arguments" "wrong number of args" } */
+  scanf ("%d%d", ip); /* { dg-warning "matching" "wrong number of args" } */
   scanf ("%d", ip, ip); /* { dg-warning "arguments" "wrong number of args" } */
   /* Miscellaneous bogus constructions.  */
   scanf (""); /* { dg-warning "zero-length" "warning for empty format" } */
diff --git a/gcc/testsuite/gcc.dg/graphite/id-pr46845.c b/gcc/testsuite/gcc.dg/graphite/id-pr46845.c
index f4da78e..4580721 100644
--- a/gcc/testsuite/gcc.dg/graphite/id-pr46845.c
+++ b/gcc/testsuite/gcc.dg/graphite/id-pr46845.c
@@ -1,4 +1,4 @@
-/* { dg-options "-O2 -ffast-math -fgraphite-identity -w -Wno-psabi" { target { i?86-*-* x86_64-*-* } } } */
+/* { dg-options "-O2 -ffast-math -fgraphite-identity -w -Wno-psabi" { target { i?86-*-* x86_64-*-* powerpc*-*-* } } } */
 
 typedef float V2SF __attribute__ ((vector_size (128)));
 
diff --git a/gcc/testsuite/gcc.dg/graphite/interchange-7.c b/gcc/testsuite/gcc.dg/graphite/interchange-7.c
index b3710ad..a4194a5 100644
--- a/gcc/testsuite/gcc.dg/graphite/interchange-7.c
+++ b/gcc/testsuite/gcc.dg/graphite/interchange-7.c
@@ -1,4 +1,5 @@
 /* { dg-require-effective-target size32plus } */
+/* { dg-add-options large_stack } */
 
 /* Formerly known as ltrans-8.c */
 
diff --git a/gcc/testsuite/gcc.dg/graphite/run-id-1.c b/gcc/testsuite/gcc.dg/graphite/run-id-1.c
index 7766008..a58c090 100644
--- a/gcc/testsuite/gcc.dg/graphite/run-id-1.c
+++ b/gcc/testsuite/gcc.dg/graphite/run-id-1.c
@@ -1,3 +1,4 @@
+/* { dg-options "-Wl,--stack,12582912" { target *-*-mingw* *-*-cygwin* } } */
 /* { dg-require-effective-target size32plus } */
 
 void abort (void);
diff --git a/gcc/testsuite/gcc.dg/lto/20081222_1.c b/gcc/testsuite/gcc.dg/lto/20081222_1.c
index e8f9254..755eb99 100644
--- a/gcc/testsuite/gcc.dg/lto/20081222_1.c
+++ b/gcc/testsuite/gcc.dg/lto/20081222_1.c
@@ -1,8 +1,12 @@
 #include "20081222_0.h"
 
+#define ASMNAME(cname)  ASMNAME2 (__USER_LABEL_PREFIX__, cname)
+#define ASMNAME2(prefix, cname) STRING (prefix) cname
+#define STRING(x)    #x
+
 /* Actually, call "x" "INT_X", and make it hidden.  */
 extern __typeof (x) x
-	__asm__ ("INT_x")
+	__asm__ (ASMNAME ("INT_x"))
 	__attribute__ ((__visibility__ ("hidden")));
 
 int x ()
@@ -12,5 +16,5 @@ int x ()
 
 /* Make an externally-visible symbol "X" that's an alias for INT_x.  */
 extern __typeof (x) EXT_x
-	__asm__ ("x")
+	__asm__ (ASMNAME ("x"))
 	__attribute__ ((__alias__ ("INT_x")));
diff --git a/gcc/testsuite/gcc.dg/lto/20090210_0.c b/gcc/testsuite/gcc.dg/lto/20090210_0.c
index 2c858a6..681729e 100644
--- a/gcc/testsuite/gcc.dg/lto/20090210_0.c
+++ b/gcc/testsuite/gcc.dg/lto/20090210_0.c
@@ -1,6 +1,6 @@
 /* { dg-lto-do run }  */
 /* { dg-suppress-ld-options {-fPIC} }  */
-/* { dg-require-effective-target tls } */
+/* { dg-require-effective-target tls_runtime } */
 /* { dg-extra-ld-options "-pthread" { target *-*-solaris2.[89] } } */
 int foo (int x)
 {
diff --git a/gcc/testsuite/gcc.dg/lto/pr46940_0.c b/gcc/testsuite/gcc.dg/lto/pr46940_0.c
index 5283495..89cb828 100644
--- a/gcc/testsuite/gcc.dg/lto/pr46940_0.c
+++ b/gcc/testsuite/gcc.dg/lto/pr46940_0.c
@@ -2,10 +2,14 @@
 /* { dg-extra-ld-options "-fuse-linker-plugin" } */
 #include <stdio.h>
 
+#define ASMNAME(cname)  ASMNAME2 (__USER_LABEL_PREFIX__, cname)
+#define ASMNAME2(prefix, cname) STRING (prefix) cname
+#define STRING(x)    #x
+
 extern __attribute__((visibility("hidden"))) void _moz_foo (void);
-extern __typeof (_moz_foo) _moz_foo __asm__ ("" "INT__foo") __attribute__((__visibility__("hidden"))) ;
+extern __typeof (_moz_foo) _moz_foo __asm__ (ASMNAME ("INT__foo")) __attribute__((__visibility__("hidden"))) ;
 void _moz_foo(void)
 {
   printf ("blah\n");
 }
-extern __typeof (_moz_foo) EXT__foo __asm__("" "_moz_foo") __attribute__((__alias__("" "INT__foo")));
+extern __typeof (_moz_foo) EXT__foo __asm__(ASMNAME ("_moz_foo")) __attribute__((__alias__("" "INT__foo")));
diff --git a/gcc/testsuite/gcc.dg/pr47276.c b/gcc/testsuite/gcc.dg/pr47276.c
index 3fa1a0e..9276b1a 100644
--- a/gcc/testsuite/gcc.dg/pr47276.c
+++ b/gcc/testsuite/gcc.dg/pr47276.c
@@ -1,6 +1,11 @@
 /* { dg-do compile } */
 /* { dg-require-alias "" } */
 /* { dg-require-visibility "" } */
+
+#define ASMNAME(cname)  ASMNAME2 (__USER_LABEL_PREFIX__, cname)
+#define ASMNAME2(prefix, cname) STRING (prefix) cname
+#define STRING(x)    #x
+
 extern void syslog (int __pri, __const char *__fmt, ...)
      __attribute__ ((__format__ (__printf__, 2, 3)));
 extern void vsyslog (int __pri, __const char *__fmt, int __ap)
@@ -17,15 +22,15 @@ void
 __vsyslog_chk(int pri, int flag, const char *fmt, int ap)
 {
 }
-extern __typeof (__vsyslog_chk) __EI___vsyslog_chk __asm__("" "__vsyslog_chk"); extern __typeof (__vsyslog_chk) __EI___vsyslog_chk __attribute__((alias ("" "__GI___vsyslog_chk")));
+extern __typeof (__vsyslog_chk) __EI___vsyslog_chk __asm__("" ASMNAME ("__vsyslog_chk")); extern __typeof (__vsyslog_chk) __EI___vsyslog_chk __attribute__((alias ("" "__GI___vsyslog_chk")));
 void
 __syslog(int pri, const char *fmt, ...)
 {
 }
 extern __typeof (__syslog) syslog __attribute__ ((alias ("__syslog")));
-extern __typeof (syslog) __EI_syslog __asm__("" "syslog"); extern __typeof (syslog) __EI_syslog __attribute__((alias ("" "__GI_syslog")));
+extern __typeof (syslog) __EI_syslog __asm__("" ASMNAME ("syslog")); extern __typeof (syslog) __EI_syslog __attribute__((alias ("" "__GI_syslog")));
 extern __typeof (__vsyslog) vsyslog __attribute__ ((alias ("__vsyslog")));
-extern __typeof (vsyslog) __EI_vsyslog __asm__("" "vsyslog"); extern __typeof (vsyslog) __EI_vsyslog __attribute__((alias ("" "__GI_vsyslog")));
-extern __typeof (syslog) syslog __asm__ ("" "__GI_syslog") __attribute__ ((visibility ("hidden")));
-extern __typeof (vsyslog) vsyslog __asm__ ("" "__GI_vsyslog") __attribute__ ((visibility ("hidden")));
-extern __typeof (__vsyslog_chk) __vsyslog_chk __asm__ ("" "__GI___vsyslog_chk") __attribute__ ((visibility ("hidden")));
+extern __typeof (vsyslog) __EI_vsyslog __asm__("" ASMNAME ("vsyslog")); extern __typeof (vsyslog) __EI_vsyslog __attribute__((alias ("" "__GI_vsyslog")));
+extern __typeof (syslog) syslog __asm__ ("" ASMNAME ("__GI_syslog")) __attribute__ ((visibility ("hidden")));
+extern __typeof (vsyslog) vsyslog __asm__ ("" ASMNAME ("__GI_vsyslog")) __attribute__ ((visibility ("hidden")));
+extern __typeof (__vsyslog_chk) __vsyslog_chk __asm__ ("" ASMNAME ("__GI___vsyslog_chk")) __attribute__ ((visibility ("hidden")));
diff --git a/gcc/testsuite/gcc.dg/tls/thr-cse-1.c b/gcc/testsuite/gcc.dg/tls/thr-cse-1.c
index 95d3b45..7542350 100644
--- a/gcc/testsuite/gcc.dg/tls/thr-cse-1.c
+++ b/gcc/testsuite/gcc.dg/tls/thr-cse-1.c
@@ -15,9 +15,10 @@ int foo (int b, int c, int d)
   return a;
 }
 
-/* { dg-final { scan-assembler-not "emutls_get_address.*emutls_get_address.*" { target { ! { "*-wrs-vxworks"  "*-*-darwin8"  "hppa*-*-hpux*" "spu-*-*" } } } } } */
+/* { dg-final { scan-assembler-not "emutls_get_address.*emutls_get_address.*" { target { ! { "*-wrs-vxworks"  "*-*-darwin8"  "hppa*-*-hpux*" "spu-*-*" "i?86-*-mingw*" } } } } } */
 /* { dg-final { scan-assembler-not "call\tL___emutls_get_address.stub.*call\tL___emutls_get_address.stub.*" { target "*-*-darwin8" } } } */
 /* { dg-final { scan-assembler-not "(b,l|bl) __emutls_get_address.*(b,l|bl) __emutls_get_address.*" { target "hppa*-*-hpux*" } } } */
 /* { dg-final { scan-assembler-not "(brsl|brasl)\t__emutls_get_address.*(brsl|brasl)\t__emutls_get_address.*" { target spu-*-* } } } */
 /* { dg-final { scan-assembler-not "tls_lookup.*tls_lookup.*" { target *-wrs-vxworks } } } */
+/* { dg-final { scan-assembler-not "call\t___emutls_get_address.*call\t___emutls_get_address" { target "i?86-*-mingw*" } } } */
 
diff --git a/gcc/testsuite/gcc.dg/torture/arm-fp16-ops-5.c b/gcc/testsuite/gcc.dg/torture/arm-fp16-ops-5.c
index 92bc8a9..738d26d 100644
--- a/gcc/testsuite/gcc.dg/torture/arm-fp16-ops-5.c
+++ b/gcc/testsuite/gcc.dg/torture/arm-fp16-ops-5.c
@@ -13,3 +13,5 @@
 /* { dg-final { scan-assembler-not "\tbl\t__gnu_h\[a-z\]*_ieee" } } */
 /* { dg-final { scan-assembler-not "\tbl\t__gnu_h2f_ieee" } } */
 /* { dg-final { scan-assembler-not "\tbl\t__gnu_f2h_ieee" } } */
+/* { dg-final { scan-assembler-not "\tbl\t__aeabi_h2f" } } */
+/* { dg-final { scan-assembler-not "\tbl\t__aeabi_f2h" } } */
diff --git a/gcc/testsuite/gcc.dg/torture/arm-fp16-ops-6.c b/gcc/testsuite/gcc.dg/torture/arm-fp16-ops-6.c
index ae40b1e..a0e09c8 100644
--- a/gcc/testsuite/gcc.dg/torture/arm-fp16-ops-6.c
+++ b/gcc/testsuite/gcc.dg/torture/arm-fp16-ops-6.c
@@ -13,3 +13,5 @@
 /* { dg-final { scan-assembler-not "\tbl\t__gnu_h\[a-z\]*_ieee" } } */
 /* { dg-final { scan-assembler-not "\tbl\t__gnu_h2f_ieee" } } */
 /* { dg-final { scan-assembler-not "\tbl\t__gnu_f2h_ieee" } } */
+/* { dg-final { scan-assembler-not "\tbl\t__aeabi_h2f" } } */
+/* { dg-final { scan-assembler-not "\tbl\t__aeabi_f2h" } } */
diff --git a/gcc/testsuite/gcc.dg/torture/pr48044.c b/gcc/testsuite/gcc.dg/torture/pr48044.c
index fe3b734..d20a634 100644
--- a/gcc/testsuite/gcc.dg/torture/pr48044.c
+++ b/gcc/testsuite/gcc.dg/torture/pr48044.c
@@ -2,6 +2,10 @@
 /* { dg-do compile } */
 /* { dg-require-alias "" } */
 
-int a __asm__ ("b") = 0;
-extern int c __asm__ ("a") __attribute__ ((alias ("b")));
+#define ASMNAME(cname)  ASMNAME2 (__USER_LABEL_PREFIX__, cname)
+#define ASMNAME2(prefix, cname) STRING (prefix) cname
+#define STRING(x)    #x
+
+int a __asm__ (ASMNAME ("b")) = 0;
+extern int c __asm__ (ASMNAME ("a")) __attribute__ ((alias ("b")));
 extern int d __attribute__ ((weak, alias ("a")));
diff --git a/gcc/testsuite/gcc.dg/torture/stackalign/builtin-apply-2.c b/gcc/testsuite/gcc.dg/torture/stackalign/builtin-apply-2.c
index a1ba20f..11f390e 100644
--- a/gcc/testsuite/gcc.dg/torture/stackalign/builtin-apply-2.c
+++ b/gcc/testsuite/gcc.dg/torture/stackalign/builtin-apply-2.c
@@ -5,6 +5,8 @@
    with pre-pushed arguments (e.g. SPARC).  */
 
 /* { dg-do run } */
+
+/* { dg-skip-if "Variadic funcs use Base AAPCS.  Normal funcs use VFP variant." { "arm*-*-*" } { "-mfloat-abi=hard" } { "" } } */
    
 
 #define INTEGER_ARG  5
diff --git a/gcc/testsuite/gcc.dg/torture/type-generic-1.c b/gcc/testsuite/gcc.dg/torture/type-generic-1.c
index add83c0..6924fed 100644
--- a/gcc/testsuite/gcc.dg/torture/type-generic-1.c
+++ b/gcc/testsuite/gcc.dg/torture/type-generic-1.c
@@ -4,6 +4,7 @@
 /* { dg-do run } */
 /* { dg-skip-if "No Inf/NaN support" { spu-*-* } } */
 /* { dg-add-options ieee } */
+/* { dg-options "-Wl,--defsym=__cs3_mips_float_type=2 -lcs3-mips-cp1 -lcs3-mips-fpemu" { target mips*-*sde*-* } } */
 
 #include "../tg-tests.h"
 
diff --git a/gcc/testsuite/gcc.dg/tree-prof/val-prof-7.c b/gcc/testsuite/gcc.dg/tree-prof/val-prof-7.c
index 664b620..d0f3472 100644
--- a/gcc/testsuite/gcc.dg/tree-prof/val-prof-7.c
+++ b/gcc/testsuite/gcc.dg/tree-prof/val-prof-7.c
@@ -1,7 +1,7 @@
 /* { dg-options "-O2 -fdump-ipa-tree_profile_ipa -mtune=core2" } */
 /* { dg-skip-if "" { ! { i?86-*-* x86_64-*-* } } { "*" } { "" } } */
 
-#include <strings.h>
+extern void bzero (void *, __SIZE_TYPE__);
 
 int foo(int len)
 {
diff --git a/gcc/testsuite/gcc.dg/tree-ssa/20040204-1.c b/gcc/testsuite/gcc.dg/tree-ssa/20040204-1.c
index 45e44a1..8080a4e 100644
--- a/gcc/testsuite/gcc.dg/tree-ssa/20040204-1.c
+++ b/gcc/testsuite/gcc.dg/tree-ssa/20040204-1.c
@@ -33,5 +33,5 @@ void test55 (int x, int y)
    that the && should be emitted (based on BRANCH_COST).  Fix this
    by teaching dom to look through && and register all components
    as true.  */
-/* { dg-final { scan-tree-dump-times "link_error" 0 "optimized" { xfail { ! "alpha*-*-* powerpc*-*-* cris-*-* crisv32-*-* hppa*-*-* i?86-*-* mmix-*-* mips*-*-* m68k*-*-* moxie-*-* sparc*-*-* spu-*-* x86_64-*-*" } } } } */
+/* { dg-final { scan-tree-dump-times "link_error" 0 "optimized" { xfail { ! "alpha*-*-* powerpc*-*-* cris-*-* crisv32-*-* hppa*-*-* i?86-*-* mmix-*-* mips*-*-* m68k*-*-* moxie-*-* sparc*-*-* spu-*-* x86_64-*-* arm*-*-*" } } } } */
 /* { dg-final { cleanup-tree-dump "optimized" } } */
diff --git a/gcc/testsuite/gcc.dg/tree-ssa/foldconst-3.c b/gcc/testsuite/gcc.dg/tree-ssa/foldconst-3.c
index 6132362..9f10886 100644
--- a/gcc/testsuite/gcc.dg/tree-ssa/foldconst-3.c
+++ b/gcc/testsuite/gcc.dg/tree-ssa/foldconst-3.c
@@ -1,5 +1,5 @@
 /* { dg-do compile } */
-/* { dg-options "-O2 -fdump-tree-optimized" } */
+/* { dg-options "-O2 -fdump-tree-optimized -fno-short-enums" } */
 typedef const union tree_node *const_tree;
 typedef struct
 {
diff --git a/gcc/testsuite/gcc.dg/tree-ssa/pr21559.c b/gcc/testsuite/gcc.dg/tree-ssa/pr21559.c
index 34f4a01..117d217 100644
--- a/gcc/testsuite/gcc.dg/tree-ssa/pr21559.c
+++ b/gcc/testsuite/gcc.dg/tree-ssa/pr21559.c
@@ -1,5 +1,5 @@
 /* { dg-do compile } */
-/* { dg-options "-O2 -fdump-tree-vrp1-details" } */
+/* { dg-options "-O2 -fdump-tree-vrp1-details -fno-remove-local-statics" } */
 
 static int blocksize = 4096;
 
diff --git a/gcc/testsuite/gcc.dg/tree-ssa/predcom-1.c b/gcc/testsuite/gcc.dg/tree-ssa/predcom-1.c
index 16bd5c9..a0f02dc 100644
--- a/gcc/testsuite/gcc.dg/tree-ssa/predcom-1.c
+++ b/gcc/testsuite/gcc.dg/tree-ssa/predcom-1.c
@@ -1,6 +1,7 @@
 /* { dg-do compile } */
 /* { dg-do run } */
-/* { dg-options "-O2 -fpredictive-commoning -fdump-tree-pcom-details" } */
+/* { dg-options "-O2 -funroll-loops --param max-unroll-times=8 -fpredictive-com
+moning -fdump-tree-pcom-details" } */
 
 void abort (void);
 
diff --git a/gcc/testsuite/gcc.dg/tree-ssa/predcom-2.c b/gcc/testsuite/gcc.dg/tree-ssa/predcom-2.c
index 7275f28..863bb7b 100644
--- a/gcc/testsuite/gcc.dg/tree-ssa/predcom-2.c
+++ b/gcc/testsuite/gcc.dg/tree-ssa/predcom-2.c
@@ -1,6 +1,7 @@
 /* { dg-do compile } */
 /* { dg-do run } */
-/* { dg-options "-O2 -fpredictive-commoning -fdump-tree-pcom-details" } */
+/* { dg-options "-O2 -funroll-loops --param max-unroll-times=8 -fpredictive-com
+moning -fdump-tree-pcom-details" } */
 
 void abort (void);
 
diff --git a/gcc/testsuite/gcc.dg/tree-ssa/predcom-3.c b/gcc/testsuite/gcc.dg/tree-ssa/predcom-3.c
index d500234..0ad5cd0 100644
--- a/gcc/testsuite/gcc.dg/tree-ssa/predcom-3.c
+++ b/gcc/testsuite/gcc.dg/tree-ssa/predcom-3.c
@@ -1,5 +1,6 @@
 /* { dg-do compile } */
-/* { dg-options "-O2 -fpredictive-commoning -fdump-tree-pcom-details" } */
+/* { dg-options "-O2 -funroll-loops --param max-unroll-times=8 -fpredictive-com
+moning -fdump-tree-pcom-details" } */
 
 int a[1000], b[1000];
 
diff --git a/gcc/testsuite/gcc.dg/tree-ssa/predcom-4.c b/gcc/testsuite/gcc.dg/tree-ssa/predcom-4.c
index 6f06b7f..db7565d 100644
--- a/gcc/testsuite/gcc.dg/tree-ssa/predcom-4.c
+++ b/gcc/testsuite/gcc.dg/tree-ssa/predcom-4.c
@@ -1,6 +1,7 @@
 /* { dg-do compile } */
 /* { dg-do run } */
-/* { dg-options "-O2 -fpredictive-commoning -fdump-tree-pcom-details" } */
+/* { dg-options "-O2 -funroll-loops --param max-unroll-times=8 -fpredictive-com
+moning -fdump-tree-pcom-details" } */
 
 /* Test for predictive commoning of expressions, without reassociation.  */
 
diff --git a/gcc/testsuite/gcc.dg/tree-ssa/predcom-5.c b/gcc/testsuite/gcc.dg/tree-ssa/predcom-5.c
index 134fc37..4b17fbb 100644
--- a/gcc/testsuite/gcc.dg/tree-ssa/predcom-5.c
+++ b/gcc/testsuite/gcc.dg/tree-ssa/predcom-5.c
@@ -1,6 +1,7 @@
 /* { dg-do compile } */
 /* { dg-do run } */
-/* { dg-options "-O2 -fpredictive-commoning -fdump-tree-pcom-details" } */
+/* { dg-options "-O2 -funroll-loops --param max-unroll-times=8 -fpredictive-com
+moning -fdump-tree-pcom-details" } */
 
 /* Test for predictive commoning of expressions, with reassociation.  */
 
diff --git a/gcc/testsuite/gcc.dg/tree-ssa/ssa-dse-6.c b/gcc/testsuite/gcc.dg/tree-ssa/ssa-dse-6.c
index 3d02006..232cfca 100644
--- a/gcc/testsuite/gcc.dg/tree-ssa/ssa-dse-6.c
+++ b/gcc/testsuite/gcc.dg/tree-ssa/ssa-dse-6.c
@@ -1,5 +1,5 @@
 /* { dg-do compile } */
-/* { dg-options "-O2 -fdump-tree-dse1" } */
+/* { dg-options "-O2 -fdump-tree-dse1 -fno-remove-local-statics" } */
 
 int foo11 (int c)
 {
diff --git a/gcc/testsuite/gcc.dg/vect/pr48172.c b/gcc/testsuite/gcc.dg/vect/pr48172.c
index 892aeca..d6985bc 100644
--- a/gcc/testsuite/gcc.dg/vect/pr48172.c
+++ b/gcc/testsuite/gcc.dg/vect/pr48172.c
@@ -1,5 +1,3 @@
-/* { dg-do run } */
-
 extern void *memset(void *s, int c, __SIZE_TYPE__ n);
 extern void abort (void);
 
diff --git a/gcc/testsuite/gcc.dg/vect/pr48377.c b/gcc/testsuite/gcc.dg/vect/pr48377.c
index 86d3531..24846cd 100644
--- a/gcc/testsuite/gcc.dg/vect/pr48377.c
+++ b/gcc/testsuite/gcc.dg/vect/pr48377.c
@@ -1,5 +1,4 @@
 /* PR tree-optimization/48377 */
-/* { dg-do run } */
 /* { dg-require-effective-target non_strict_align } */
 
 typedef unsigned int U __attribute__((__aligned__ (1), __may_alias__));
diff --git a/gcc/testsuite/gcc.dg/vect/pr49038.c b/gcc/testsuite/gcc.dg/vect/pr49038.c
index 91c214f..0759230 100644
--- a/gcc/testsuite/gcc.dg/vect/pr49038.c
+++ b/gcc/testsuite/gcc.dg/vect/pr49038.c
@@ -1,3 +1,5 @@
+/* { dg-require-effective-target mmap } */
+ 
 #include <sys/mman.h>
 #include <stdio.h>
 
diff --git a/gcc/testsuite/gcc.dg/vect/vect-16.c b/gcc/testsuite/gcc.dg/vect/vect-16.c
index 6983705..d55751b 100644
--- a/gcc/testsuite/gcc.dg/vect/vect-16.c
+++ b/gcc/testsuite/gcc.dg/vect/vect-16.c
@@ -34,5 +34,5 @@ int main (void)
 }
 
 /* Requires fast-math.  */
-/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 1 "vect" { xfail *-*-* } } } */
+/* { dg-final { scan-tree-dump-times "vectorized 1 loops" 1 "vect" { xfail { ! "arm*-*-*" } } } } */
 /* { dg-final { cleanup-tree-dump "vect" } } */
diff --git a/gcc/testsuite/gcc.dg/vect/vect-shift-2.c b/gcc/testsuite/gcc.dg/vect/vect-shift-2.c
index 83211eb..641fe1a 100644
--- a/gcc/testsuite/gcc.dg/vect/vect-shift-2.c
+++ b/gcc/testsuite/gcc.dg/vect/vect-shift-2.c
@@ -5,6 +5,7 @@
    correct value is generated.  */
 
 #ifdef TRACE
+#include <stdio.h>
 #endif
 
 #include <stdarg.h>
diff --git a/gcc/testsuite/gcc.dg/vmx/vmx.exp b/gcc/testsuite/gcc.dg/vmx/vmx.exp
index 85c88d8..feee1a4 100644
--- a/gcc/testsuite/gcc.dg/vmx/vmx.exp
+++ b/gcc/testsuite/gcc.dg/vmx/vmx.exp
@@ -31,7 +31,7 @@ if {![istarget powerpc*-*-*]
 # nothing but extensions.
 global DEFAULT_VMXCFLAGS
 if ![info exists DEFAULT_VMXCFLAGS] then {
-    set DEFAULT_VMXCFLAGS "-maltivec -mabi=altivec -std=gnu99 -mno-vsx"
+    set DEFAULT_VMXCFLAGS "-maltivec -std=gnu99 -mno-vsx"
 }
 
 # If the target system supports AltiVec instructions, the default action
diff --git a/gcc/testsuite/gcc.target/arm/pr43698.c b/gcc/testsuite/gcc.target/arm/pr43698.c
index 407cf7e..1fc497c 100644
--- a/gcc/testsuite/gcc.target/arm/pr43698.c
+++ b/gcc/testsuite/gcc.target/arm/pr43698.c
@@ -1,5 +1,5 @@
 /* { dg-do run } */
-/* { dg-options "-Os -march=armv7-a" } */
+/* { dg-options "-Os" } */
 #include <stdint.h>
 #include <stdlib.h>
 
diff --git a/gcc/testsuite/gcc.target/arm/sync-1.c b/gcc/testsuite/gcc.target/arm/sync-1.c
index ad85a04..80cb08f 100644
--- a/gcc/testsuite/gcc.target/arm/sync-1.c
+++ b/gcc/testsuite/gcc.target/arm/sync-1.c
@@ -1,5 +1,5 @@
-/* { dg-do run } */
-/* { dg-options "-O2 -march=armv7-a" } */
+/* { dg-do run { target sync_int_long } } */
+/* { dg-options "-O2" } */
 
 volatile int mem;
 
diff --git a/gcc/testsuite/gcc.target/arm/vfp-1.c b/gcc/testsuite/gcc.target/arm/vfp-1.c
index a020622..d455ea4 100644
--- a/gcc/testsuite/gcc.target/arm/vfp-1.c
+++ b/gcc/testsuite/gcc.target/arm/vfp-1.c
@@ -127,13 +127,13 @@ void test_convert () {
 
 void test_ldst (float f[], double d[]) {
   /* { dg-final { scan-assembler "flds.+ \\\[r0, #1020\\\]" } } */
-  /* { dg-final { scan-assembler "flds.+ \\\[r0, #-1020\\\]" } } */
+  /* { dg-final { scan-assembler "flds.+ \\\[r\[0-9\], #-1020\\\]" { target { arm32 && { ! arm_thumb2_ok } } } } } */
   /* { dg-final { scan-assembler "add.+ r0, #1024" } } */
-  /* { dg-final { scan-assembler "fsts.+ \\\[r0, #0\\\]\n" } } */
+  /* { dg-final { scan-assembler "fsts.+ \\\[r\[0-9\], #0\\\]\n" } } */
   f[256] = f[255] + f[-255];
 
   /* { dg-final { scan-assembler "fldd.+ \\\[r1, #1016\\\]" } } */
-  /* { dg-final { scan-assembler "fldd.+ \\\[r1, #-1016\\\]" } } */
+  /* { dg-final { scan-assembler "fldd.+ \\\[r\[1-9\], #-1016\\\]" { target { arm32 && { ! arm_thumb2_ok } } } } } */
   /* { dg-final { scan-assembler "fstd.+ \\\[r1, #256\\\]" } } */
   d[32] = d[127] + d[-127];
 }
diff --git a/gcc/testsuite/gcc.target/arm/wmul-1.c b/gcc/testsuite/gcc.target/arm/wmul-1.c
index 426c939..ddddd50 100644
--- a/gcc/testsuite/gcc.target/arm/wmul-1.c
+++ b/gcc/testsuite/gcc.target/arm/wmul-1.c
@@ -1,6 +1,6 @@
 /* { dg-do compile } */
 /* { dg-require-effective-target arm_dsp } */
-/* { dg-options "-O2" } */
+/* { dg-options "-O1 -fexpensive-optimizations" } */
 
 int mac(const short *a, const short *b, int sqr, int *sum)
 {
diff --git a/gcc/testsuite/gcc.target/arm/wmul-2.c b/gcc/testsuite/gcc.target/arm/wmul-2.c
index 898b5f0..2ea55f9 100644
--- a/gcc/testsuite/gcc.target/arm/wmul-2.c
+++ b/gcc/testsuite/gcc.target/arm/wmul-2.c
@@ -1,6 +1,6 @@
 /* { dg-do compile } */
 /* { dg-require-effective-target arm_dsp } */
-/* { dg-options "-O2" } */
+/* { dg-options "-O1 -fexpensive-optimizations" } */
 
 void vec_mpy(int y[], const short x[], short scaler)
 {
diff --git a/gcc/testsuite/gcc.target/arm/wmul-3.c b/gcc/testsuite/gcc.target/arm/wmul-3.c
index 83f73fb..144b553 100644
--- a/gcc/testsuite/gcc.target/arm/wmul-3.c
+++ b/gcc/testsuite/gcc.target/arm/wmul-3.c
@@ -1,6 +1,6 @@
 /* { dg-do compile } */
 /* { dg-require-effective-target arm_dsp } */
-/* { dg-options "-O2" } */
+/* { dg-options "-O1 -fexpensive-optimizations" } */
 
 int mac(const short *a, const short *b, int sqr, int *sum)
 {
diff --git a/gcc/testsuite/gcc.target/arm/wmul-4.c b/gcc/testsuite/gcc.target/arm/wmul-4.c
index a297bda..68f9866 100644
--- a/gcc/testsuite/gcc.target/arm/wmul-4.c
+++ b/gcc/testsuite/gcc.target/arm/wmul-4.c
@@ -1,6 +1,6 @@
 /* { dg-do compile } */
 /* { dg-require-effective-target arm_dsp } */
-/* { dg-options "-O2" } */
+/* { dg-options "-O1 -fexpensive-optimizations" } */
 
 int mac(const int *a, const int *b, long long sqr, long long *sum)
 {
diff --git a/gcc/testsuite/lib/lto.exp b/gcc/testsuite/lib/lto.exp
index f4469ee..519509e 100644
--- a/gcc/testsuite/lib/lto.exp
+++ b/gcc/testsuite/lib/lto.exp
@@ -497,7 +497,7 @@ proc lto-execute { src1 sid } {
 	verbose "Testing $testcase, $option"
 
 	# There's a unique name for each executable we generate.
-	set execname "${execbase}-${count}1"
+	set execname "${execbase}-${count}1.exe"
 	incr count
 
 	file_on_host delete $execname
diff --git a/gcc/testsuite/lib/prune.exp b/gcc/testsuite/lib/prune.exp
index 093d4f7..f249877 100644
--- a/gcc/testsuite/lib/prune.exp
+++ b/gcc/testsuite/lib/prune.exp
@@ -72,3 +72,34 @@ if { [info procs prune_warnings] == "" } then {
 	return $text
     }
 }
+
+# Extend prune_warnings (provided by DejaGNU itself) to prune more
+# things.  The prune_gcc_output function above is called only by some
+# tests; prune_warnings is used by all.
+if { [info procs prune_warnings_orig] == "" } {
+    rename prune_warnings prune_warnings_orig
+    
+    proc prune_warnings { text } {
+	set text [prune_warnings_orig $text]
+	
+	if { [ishost "sparc*-*-solaris2*"] } {
+	    # When testing a compiler built for SPARC Solaris 2.9 (or earlier)
+	    # on a host running Solaris 2.10 (or later), we get this warning 
+	    # from the static linker when building with g++:
+	    #
+	    #   libm.so.1, needed by .../libstdc++.so may conflict with
+	    #   libm.so
+	    #
+	    # The warning is issued because libstdc++ is linked against
+	    # libm.so.1 (from the Solaris 2.9 sysroot), whereas Solaris 2.10
+	    # provides both libm.so.2 and libm.so.1.  On Solaris 2.10, libc.so
+	    # depends on libm.so.2, so all programs pull in libm.so.2.
+	    #
+	    # Pulling both libraries must in fact be harmless, as, otherwise,
+	    # programs built for Solaris 2.9 would break on Solaris 2.10.
+	    regsub -all "(^|\n)\[^\n\]*: warning: libm.so.1, needed by \[^\n\]*, may conflict with libm.so.2" $text "" text
+	}
+
+	return $text
+    }
+}
diff --git a/gcc/testsuite/lib/target-supports.exp b/gcc/testsuite/lib/target-supports.exp
index 7da4afd..45177a9 100644
--- a/gcc/testsuite/lib/target-supports.exp
+++ b/gcc/testsuite/lib/target-supports.exp
@@ -691,6 +691,12 @@ proc check_effective_target_fopenmp {} {
     } "-fopenmp"]
 }
 
+# Return 1 if the target supports mmap, 0 otherwise.
+
+proc check_effective_target_mmap {} {
+    return [check_function_available "mmap"]
+}
+
 # Return 1 if compilation with -pthread is error-free for trivial
 # code, 0 otherwise.
 
@@ -1842,6 +1848,15 @@ proc check_effective_target_arm32 { } {
     }]
 }
 
+# Return 1 if this is a little-endian ARM target
+proc check_effective_target_arm_little_endian { } {
+    return [check_no_compiler_messages arm_little_endian assembly {
+	#if !defined(__arm__) || !defined(__ARMEL__)
+	#error FOO
+	#endif
+    }]
+}
+
 # Return 1 if this is an ARM target that only supports aligned vector accesses
 proc check_effective_target_arm_vect_no_misalign { } {
     return [check_no_compiler_messages arm_vect_no_misalign assembly {
@@ -2338,6 +2353,26 @@ proc check_effective_target_vect_shift_scalar { } {
 }
 
 
+# Return 1 if the target supports hardware vector shift operation for char.
+
+proc check_effective_target_vect_shift_char { } {
+    global et_vect_shift_char_saved
+
+    if [info exists et_vect_shift_char_saved] {
+	verbose "check_effective_target_vect_shift_char: using cached result" 2
+    } else {
+	set et_vect_shift_char_saved 0
+	if { ([istarget powerpc*-*-*]
+             && ![istarget powerpc-*-linux*paired*])
+	     || [check_effective_target_arm32] } {
+	   set et_vect_shift_char_saved 1
+	}
+    }
+
+    verbose "check_effective_target_vect_shift_char: returning $et_vect_shift_char_saved" 2
+    return $et_vect_shift_char_saved
+}
+
 # Return 1 if the target supports hardware vectors of long, 0 otherwise.
 #
 # This can change for different subtargets so do not cache the result.
@@ -2816,7 +2851,8 @@ proc check_effective_target_vect_pack_trunc { } {
              || [istarget i?86-*-*]
              || [istarget x86_64-*-*]
              || [istarget spu-*-*]
-             || ([istarget arm*-*-*] && [check_effective_target_arm_neon]) } {
+             || ([istarget arm*-*-*] && [check_effective_target_arm_neon]
+		 && [check_effective_target_arm_little_endian]) } {
             set et_vect_pack_trunc_saved 1
         }
     }
@@ -2841,7 +2877,8 @@ proc check_effective_target_vect_unpack { } {
              || [istarget x86_64-*-*] 
              || [istarget spu-*-*]
              || [istarget ia64-*-*]
-             || ([istarget arm*-*-*] && [check_effective_target_arm_neon]) } {
+             || ([istarget arm*-*-*] && [check_effective_target_arm_neon]
+		 && [check_effective_target_arm_little_endian]) } {
             set et_vect_unpack_saved 1
         }
     }
@@ -3006,8 +3043,9 @@ proc check_effective_target_vector_alignment_reachable { } {
     if [info exists et_vector_alignment_reachable_saved] {
         verbose "check_effective_target_vector_alignment_reachable: using cached result" 2
     } else {
-        if { [check_effective_target_vect_aligned_arrays]
-             || [check_effective_target_natural_alignment_32] } {
+        if { ([check_effective_target_vect_aligned_arrays]
+              || [check_effective_target_natural_alignment_32])
+	     && !([istarget arm*-*-*] && [check_effective_target_arm_neon]) } {
             set et_vector_alignment_reachable_saved 1
         } else {
             set et_vector_alignment_reachable_saved 0
@@ -3047,7 +3085,8 @@ proc check_effective_target_vect_element_align { } {
 	verbose "check_effective_target_vect_element_align: using cached result" 2
     } else {
 	set et_vect_element_align 0
-	if { [istarget arm*-*-*]
+	if { ([istarget arm*-*-*]
+	      && ![check_effective_target_arm_vect_no_misalign])
 	     || [check_effective_target_vect_hw_misalign] } {
 	   set et_vect_element_align 1
 	}
@@ -3949,6 +3988,7 @@ proc check_vect_support_and_set_flags { } {
         # default to avoid loss of precision.  We must pass -ffast-math to test
         # vectorization of float operations.
         lappend DEFAULT_VECTCFLAGS "-ffast-math"
+	lappend DEFAULT_VECTCFLAGS "-fno-unroll-loops"
         if [is-effective-target arm_neon_hw] {
             set dg-do-what-default run
         } else {
@@ -3961,6 +4001,17 @@ proc check_vect_support_and_set_flags { } {
     return 1
 }
 
+# Add options to increase stack size, when that is a tunable parameter at
+# link time (e.g. uClinux).  Use for test cases which need a lot of stack
+# space.
+
+proc add_options_for_large_stack { flags } {
+    if { [istarget *-*-uclinux*] } {
+        return "$flags -Wl,-elf2flt=-s1500000"
+    }
+    return $flags
+}
+
 proc check_effective_target_non_strict_align {} {
     return [check_no_compiler_messages non_strict_align assembly {
 	char *y;
diff --git a/gcc/timevar.def b/gcc/timevar.def
index ec800aa..91eaa0c 100644
--- a/gcc/timevar.def
+++ b/gcc/timevar.def
@@ -159,6 +159,7 @@ DEFTIMEVAR (TV_TREE_LOOP_IVOPTS	     , "tree iv optimization")
 DEFTIMEVAR (TV_PREDCOM		     , "predictive commoning")
 DEFTIMEVAR (TV_TREE_LOOP_INIT	     , "tree loop init")
 DEFTIMEVAR (TV_TREE_LOOP_FINI	     , "tree loop fini")
+DEFTIMEVAR (TV_TREE_LOOP_PROMOTE     , "tree loop index promotion")
 DEFTIMEVAR (TV_TREE_CH		     , "tree copy headers")
 DEFTIMEVAR (TV_TREE_SSA_UNCPROP	     , "tree SSA uncprop")
 DEFTIMEVAR (TV_TREE_SSA_TO_NORMAL    , "tree SSA to normal")
@@ -166,6 +167,7 @@ DEFTIMEVAR (TV_TREE_NRV		     , "tree NRV optimization")
 DEFTIMEVAR (TV_TREE_COPY_RENAME	     , "tree rename SSA copies")
 DEFTIMEVAR (TV_TREE_SSA_VERIFY       , "tree SSA verifier")
 DEFTIMEVAR (TV_TREE_STMT_VERIFY      , "tree STMT verifier")
+DEFTIMEVAR (TV_TREE_RLS              , "tree local static removal")
 DEFTIMEVAR (TV_TREE_SWITCH_CONVERSION, "tree switch initialization conversion")
 DEFTIMEVAR (TV_CGRAPH_VERIFY         , "callgraph verifier")
 DEFTIMEVAR (TV_DOM_FRONTIERS         , "dominance frontiers")
@@ -180,6 +182,7 @@ DEFTIMEVAR (TV_POST_EXPAND	     , "post expand cleanups")
 DEFTIMEVAR (TV_VARCONST              , "varconst")
 DEFTIMEVAR (TV_LOWER_SUBREG	     , "lower subreg")
 DEFTIMEVAR (TV_JUMP                  , "jump")
+DEFTIMEVAR (TV_EE                    , "extension elimination")
 DEFTIMEVAR (TV_FWPROP                , "forward prop")
 DEFTIMEVAR (TV_CSE                   , "CSE")
 DEFTIMEVAR (TV_DCE                   , "dead code elimination")
diff --git a/gcc/toplev.c b/gcc/toplev.c
index 2b0f06d..c2d75da 100644
--- a/gcc/toplev.c
+++ b/gcc/toplev.c
@@ -1333,6 +1333,13 @@ process_options (void)
       flag_strict_volatile_bitfields = 0;
     }
 
+  if (flag_strict_volatile_bitfields > 0 && !abi_version_at_least (2))
+    {
+      warning (0, "-fstrict-volatile-bitfield disabled; "
+	       "it is incompatible with ABI versions < 2");
+      flag_strict_volatile_bitfields = 0;
+    }
+
   /* Unrolling all loops implies that standard loop unrolling must also
      be done.  */
   if (flag_unroll_all_loops)
diff --git a/gcc/tree-cfg.c b/gcc/tree-cfg.c
index d56650d..8e70df7 100644
--- a/gcc/tree-cfg.c
+++ b/gcc/tree-cfg.c
@@ -45,6 +45,7 @@ along with GCC; see the file COPYING3.  If not see
 #include "value-prof.h"
 #include "pointer-set.h"
 #include "tree-inline.h"
+#include "target.h"
 
 /* This file contains functions for building the Control Flow Graph (CFG)
    for a function tree.  */
@@ -7345,6 +7346,9 @@ execute_warn_function_return (void)
   edge e;
   edge_iterator ei;
 
+  if (!targetm.warn_func_result())
+    return 0;
+
   /* If we have a path to EXIT, then we do return.  */
   if (TREE_THIS_VOLATILE (cfun->decl)
       && EDGE_COUNT (EXIT_BLOCK_PTR->preds) > 0)
diff --git a/gcc/tree-eh.c b/gcc/tree-eh.c
index fab4a3a..5163529 100644
--- a/gcc/tree-eh.c
+++ b/gcc/tree-eh.c
@@ -3129,7 +3129,10 @@ lower_eh_dispatch (basic_block src, gimple stmt)
 
 	    c->label = NULL;
 	    tp_node = c->type_list;
-	    flt_node = c->filter_list;
+	    if (TARGET_COMPACT_EH)
+	      flt_node = c->compact_filter_list;
+	    else
+	      flt_node = c->filter_list;
 
 	    if (tp_node == NULL)
 	      {
diff --git a/gcc/tree-pass.h b/gcc/tree-pass.h
index d7b122b..0534ac6 100644
--- a/gcc/tree-pass.h
+++ b/gcc/tree-pass.h
@@ -376,6 +376,7 @@ extern struct gimple_opt_pass pass_empty_loop;
 extern struct gimple_opt_pass pass_record_bounds;
 extern struct gimple_opt_pass pass_graphite;
 extern struct gimple_opt_pass pass_graphite_transforms;
+extern struct gimple_opt_pass pass_promote_indices;
 extern struct gimple_opt_pass pass_if_conversion;
 extern struct gimple_opt_pass pass_loop_distribution;
 extern struct gimple_opt_pass pass_vectorize;
@@ -446,6 +447,7 @@ extern struct gimple_opt_pass pass_tracer;
 extern struct gimple_opt_pass pass_warn_unused_result;
 extern struct gimple_opt_pass pass_split_functions;
 extern struct gimple_opt_pass pass_feedback_split_functions;
+extern struct gimple_opt_pass pass_remove_local_statics;
 
 /* IPA Passes */
 extern struct simple_ipa_opt_pass pass_ipa_lower_emutls;
@@ -483,6 +485,7 @@ extern struct rtl_opt_pass pass_rtl_eh;
 extern struct rtl_opt_pass pass_initial_value_sets;
 extern struct rtl_opt_pass pass_unshare_all_rtl;
 extern struct rtl_opt_pass pass_instantiate_virtual_regs;
+extern struct rtl_opt_pass pass_ee;
 extern struct rtl_opt_pass pass_rtl_fwprop;
 extern struct rtl_opt_pass pass_rtl_fwprop_addr;
 extern struct rtl_opt_pass pass_jump2;
@@ -547,6 +550,7 @@ extern struct rtl_opt_pass pass_peephole2;
 extern struct rtl_opt_pass pass_if_after_reload;
 extern struct rtl_opt_pass pass_regrename;
 extern struct rtl_opt_pass pass_cprop_hardreg;
+extern struct rtl_opt_pass pass_cprop_hardreg2;
 extern struct rtl_opt_pass pass_reorder_blocks;
 extern struct rtl_opt_pass pass_branch_target_load_optimize2;
 extern struct rtl_opt_pass pass_leaf_regs;
@@ -576,6 +580,7 @@ extern struct gimple_opt_pass pass_inline_parameters;
 extern struct gimple_opt_pass pass_all_early_optimizations;
 extern struct gimple_opt_pass pass_update_address_taken;
 extern struct gimple_opt_pass pass_convert_switch;
+extern struct gimple_opt_pass pass_if_to_switch;
 
 /* The root of the compilation pass tree, once constructed.  */
 extern struct opt_pass *all_passes, *all_small_ipa_passes, *all_lowering_passes,
diff --git a/gcc/tree-ssa-loop-ivopts.c b/gcc/tree-ssa-loop-ivopts.c
index 479b46f..9b6f86e 100644
--- a/gcc/tree-ssa-loop-ivopts.c
+++ b/gcc/tree-ssa-loop-ivopts.c
@@ -171,6 +171,7 @@ struct cost_pair
 			   the final value of the iv.  For iv elimination,
 			   the new bound to compare with.  */
   int inv_expr_id;      /* Loop invariant expression id.  */
+  enum tree_code comp;	/* For iv elimination, the comparison.  */
 };
 
 /* Use.  */
@@ -291,6 +292,9 @@ struct ivopts_data
 
   /* Whether the loop body includes any function calls.  */
   bool body_includes_call;
+
+  /* Whether the loop body can only be exited via single exit.  */
+  bool loop_single_exit_p;
 };
 
 /* An assignment of iv candidates to uses.  */
@@ -377,6 +381,127 @@ struct iv_ca_delta
 
 static VEC(tree,heap) *decl_rtl_to_reset;
 
+/* Detects whether A is of POINTER_TYPE, and modifies CODE and B to make
+   A CODE B type-safe.  */
+
+static inline void
+robust_plus (enum tree_code *code, tree a, tree *b)
+{
+  tree a_type = TREE_TYPE (a);
+  tree b_type = TREE_TYPE (*b);
+
+  if (POINTER_TYPE_P (a_type))
+    {
+      switch (*code)
+        {
+        case MINUS_EXPR:
+          *b = fold_build1 (NEGATE_EXPR, b_type, *b);
+
+          /* Fall-through.  */
+        case PLUS_EXPR:
+          *code = POINTER_PLUS_EXPR;
+          break;
+        default:
+          gcc_unreachable ();
+        }
+    }
+  else
+    *b = fold_convert (a_type, *b);
+}
+
+/* Returns (TREE_TYPE (A))(A CODE B), where CODE is either PLUS_EXPR or
+   MINUS_EXPR.  Handles the case that A is a pointer robustly.  */
+
+static inline tree
+fold_build_plus (enum tree_code code, tree a, tree b)
+{
+  robust_plus (&code, a, &b);
+  return fold_build2 (code, TREE_TYPE (a), a, b);
+}
+
+/* Folds (TREE_TYPE (A))(A CODE B), where CODE is either PLUS_EXPR or
+   MINUS_EXPR.  Returns the folded expression if folding is successful.
+   Otherwise, return NULL_TREE.  Handles the case that A is a pointer
+   robustly.  */
+
+static inline tree
+fold_plus (enum tree_code code, tree a, tree b)
+{
+  tree a_type = TREE_TYPE (a);
+  tree res;
+
+  STRIP_NOPS (a);
+  robust_plus (&code, a, &b);
+
+  res = fold_binary (code, TREE_TYPE (a), a, b);
+  if (res == NULL_TREE)
+    return NULL_TREE;
+
+  return fold_convert (a_type, res);
+}
+
+/* Folds (TREE_TYPE (A))(A - B), possibly using the defining stmt of A.  Returns
+   the folded expression if folding is successful and resulted in an ssa_name.
+   Otherwise, return NULL_TREE.  */
+
+static inline tree
+fold_diff_to_ssa_name (tree a, tree b)
+{
+  tree a_type = TREE_TYPE (a);
+  tree res, a0, a1;
+  gimple def_stmt;
+
+  res = fold_plus (MINUS_EXPR, a, b);
+  if (res != NULL_TREE)
+    {
+      STRIP_NOPS (res);
+      if (TREE_CODE (res) == SSA_NAME)
+	return fold_convert (a_type, res);
+    }
+
+  STRIP_NOPS (a);
+  STRIP_NOPS (b);
+
+  if (TREE_CODE (a) == PLUS_EXPR && TREE_CODE (b) == PLUS_EXPR
+      && tree_int_cst_equal (TREE_OPERAND (a, 1), TREE_OPERAND (b, 1)))
+    {
+      a = TREE_OPERAND (a, 0);
+      b = TREE_OPERAND (b, 0);
+
+      STRIP_NOPS (a);
+      STRIP_NOPS (b);
+
+      res = fold_plus (MINUS_EXPR, a, b);
+      if (res != NULL_TREE)
+	{
+	  STRIP_NOPS (res);
+	  if (TREE_CODE (res) == SSA_NAME)
+	    return fold_convert (a_type, res);
+	}
+    }
+
+  if (TREE_CODE (a) != SSA_NAME)
+    return NULL_TREE;
+
+  def_stmt = SSA_NAME_DEF_STMT (a);
+  if (!is_gimple_assign (def_stmt)
+      || (gimple_assign_rhs_code (def_stmt) != PLUS_EXPR
+	  && gimple_assign_rhs_code (def_stmt) != POINTER_PLUS_EXPR))
+    return NULL_TREE;
+  a0 = gimple_assign_rhs1 (def_stmt);
+  a1 = gimple_assign_rhs2 (def_stmt);
+
+  res = fold_plus (MINUS_EXPR, fold_build_plus (PLUS_EXPR, a0, a1), b);
+  if (res != NULL_TREE)
+    {
+      STRIP_NOPS (res);
+      if (TREE_CODE (res) == SSA_NAME)
+	return fold_convert (a_type, res);
+    }
+
+  return NULL_TREE;
+}
+
 /* Number of uses recorded in DATA.  */
 
 static inline unsigned
@@ -783,17 +908,25 @@ niter_for_exit (struct ivopts_data *data, edge exit,
 
   if (!slot)
     {
-      /* Try to determine number of iterations.  We must know it
-	 unconditionally (i.e., without possibility of # of iterations
-	 being zero).  Also, we cannot safely work with ssa names that
-	 appear in phi nodes on abnormal edges, so that we do not create
-	 overlapping life ranges for them (PR 27283).  */
+      /* Try to determine number of iterations.  We cannot safely work with ssa
+         names that appear in phi nodes on abnormal edges, so that we do not
+         create overlapping life ranges for them (PR 27283).  */
       desc = XNEW (struct tree_niter_desc);
       if (number_of_iterations_exit (data->current_loop,
 				     exit, desc, true)
-	  && integer_zerop (desc->may_be_zero)
      	  && !contains_abnormal_ssa_name_p (desc->niter))
-	niter = desc->niter;
+	{
+	  if (!integer_zerop (desc->may_be_zero))
+            /* Construct COND_EXPR that describes the number of iterations.
+               Either the COND_EXPR is not too expensive, and we can use it as
+               loop bound, or we can deduce a LT_EXPR bound from it.  */
+	    niter
+	      = build3 (COND_EXPR, TREE_TYPE (desc->niter), desc->may_be_zero,
+			build_int_cst_type (TREE_TYPE (desc->niter), 0),
+			desc->niter);
+	  else
+	    niter = desc->niter;
+	}
       else
 	niter = NULL_TREE;
 
@@ -2351,18 +2484,7 @@ add_autoinc_candidates (struct ivopts_data *data, tree base, tree step,
   if ((HAVE_PRE_INCREMENT && GET_MODE_SIZE (mem_mode) == cstepi)
       || (HAVE_PRE_DECREMENT && GET_MODE_SIZE (mem_mode) == -cstepi))
     {
-      enum tree_code code = MINUS_EXPR;
-      tree new_base;
-      tree new_step = step;
-
-      if (POINTER_TYPE_P (TREE_TYPE (base)))
-	{
-	  new_step = fold_build1 (NEGATE_EXPR, TREE_TYPE (step), step);
-	  code = POINTER_PLUS_EXPR;
-	}
-      else
-	new_step = fold_convert (TREE_TYPE (base), new_step);
-      new_base = fold_build2 (code, TREE_TYPE (base), base, new_step);
+      tree new_base = fold_build_plus (MINUS_EXPR, base, step);
       add_candidate_1 (data, new_base, step, important, IP_BEFORE_USE, use,
 		       use->stmt);
     }
@@ -2654,13 +2776,12 @@ infinite_cost_p (comp_cost cost)
 
 /* Sets cost of (USE, CANDIDATE) pair to COST and record that it depends
    on invariants DEPENDS_ON and that the value used in expressing it
-   is VALUE.  */
-
+   is VALUE, and in case of iv elimination the comparison operator is COMP.  */
 static void
 set_use_iv_cost (struct ivopts_data *data,
 		 struct iv_use *use, struct iv_cand *cand,
 		 comp_cost cost, bitmap depends_on, tree value,
-                 int inv_expr_id)
+                 int inv_expr_id, enum tree_code comp)
 {
   unsigned i, s;
 
@@ -2677,6 +2798,7 @@ set_use_iv_cost (struct ivopts_data *data,
       use->cost_map[cand->id].depends_on = depends_on;
       use->cost_map[cand->id].value = value;
       use->cost_map[cand->id].inv_expr_id = inv_expr_id;
+      use->cost_map[cand->id].comp = comp;
       return;
     }
 
@@ -2697,6 +2819,7 @@ found:
   use->cost_map[i].depends_on = depends_on;
   use->cost_map[i].value = value;
   use->cost_map[i].inv_expr_id = inv_expr_id;
+  use->cost_map[i].comp = comp;
 }
 
 /* Gets cost of (USE, CANDIDATE) pair.  */
@@ -4208,7 +4331,8 @@ determine_use_iv_cost_generic (struct ivopts_data *data,
   if (cand->pos == IP_ORIGINAL
       && cand->incremented_at == use->stmt)
     {
-      set_use_iv_cost (data, use, cand, zero_cost, NULL, NULL_TREE, -1);
+      set_use_iv_cost (data, use, cand, zero_cost, NULL, NULL_TREE, -1,
+		       ERROR_MARK);
       return true;
     }
 
@@ -4216,7 +4340,7 @@ determine_use_iv_cost_generic (struct ivopts_data *data,
                                NULL, &inv_expr_id);
 
   set_use_iv_cost (data, use, cand, cost, depends_on, NULL_TREE,
-                   inv_expr_id);
+                   inv_expr_id, ERROR_MARK);
 
   return !infinite_cost_p (cost);
 }
@@ -4244,7 +4368,7 @@ determine_use_iv_cost_address (struct ivopts_data *data,
 	cost = infinite_cost;
     }
   set_use_iv_cost (data, use, cand, cost, depends_on, NULL_TREE,
-                   inv_expr_id);
+                   inv_expr_id, ERROR_MARK);
 
   return !infinite_cost_p (cost);
 }
@@ -4319,12 +4443,152 @@ iv_elimination_compare (struct ivopts_data *data, struct iv_use *use)
   return (exit->flags & EDGE_TRUE_VALUE ? EQ_EXPR : NE_EXPR);
 }
 
+/* Get the loop bound and comparison operator of USE->iv, and store them in
+   BOUND_P and COMP_P.  Returns false if unsuccessful.  */
+
+static bool
+get_use_lt_bound (struct iv_use *use, tree use_iv, tree *bound_p,
+		  enum tree_code *comp_p)
+{
+  gimple stmt = use->stmt;
+
+  if (gimple_code (stmt) != GIMPLE_COND
+      || gimple_cond_lhs (stmt) != use_iv)
+    return false;
+
+  *comp_p = gimple_cond_code (stmt);
+  *bound_p = gimple_cond_rhs (stmt);
+
+  return true;
+}
+
+/* Tries to replace loop exit test USE, by one formulated in terms of a LT_EXPR
+   comparison with CAND.  Stores the resulting comparison in COMP_P and bound in
+   BOUND_P.  */
+
+static bool
+iv_elimination_compare_lt (struct ivopts_data *data, struct iv_use *use,
+                           struct iv_cand *cand, tree *bound_p,
+			   enum tree_code *comp_p)
+{
+  enum tree_code use_comp, canon_comp;
+  tree base_ptr, use_lt_bound, bound, *use_iv, base_cand_at_use;
+  bool ok;
+  tree use_type, cand_type, cand_iv_base = cand->iv->base;
+
+  /* Initialize cand_type, use_iv and use_type.  */
+  STRIP_NOPS (cand_iv_base);
+  cand_type = TREE_TYPE (cand_iv_base);
+  ok = extract_cond_operands (data, use->stmt, &use_iv, NULL, NULL, NULL);
+  gcc_assert (ok);
+  use_type = TREE_TYPE (*use_iv);
+
+  /* We're trying to replace 'i < n' with 'p < base + n' in
+
+     void
+     f1 (char *base, unsigned long int s, unsigned long int n)
+     {
+       unsigned long int i = s;
+       char *p = base + s;
+       do
+         {
+	   *p = '\0';
+	   p++;
+	   i++;
+	 }
+       while (i < n);
+     }
+
+     Overflow of base + n can't happen because either:
+     - s < n, and i will step to n, and p will step to base + n, or
+     - s >= n, so base + n < base + s, and assuming pointer arithmetic
+       doesn't overflow, base + s doesn't overflow, so base + n won't.
+
+     This transformation is not valid if i and n are signed, because
+     base + n might underflow.
+  */
+
+  /* Use should be an unsigned integral.  */
+  if (!INTEGRAL_TYPE_P (use_type) || !TYPE_UNSIGNED (use_type))
+    return false;
+
+  /* Cand should be a pointer, and pointer overflow should be undefined.  */
+  if (!POINTER_TYPE_P (cand_type) || !POINTER_TYPE_OVERFLOW_UNDEFINED)
+    return false;
+
+  /* Make sure that the loop iterates till the loop bound is hit.  */
+  if (!data->loop_single_exit_p)
+    return false;
+
+  /* We only handle this case for the moment.  */
+  if (!tree_int_cst_equal (use->iv->step, cand->iv->step))
+    return false;
+
+  if (cand->pos == IP_ORIGINAL)
+    {
+      /* If cand is a src level ptr iv, we know that
+	 cand->iv->base + nit * cand->iv->step won't overflow.  */
+
+      /* Now get the base of var_at_stmt (cand, use->stmt).  */
+      if (stmt_after_increment (data->current_loop, cand, use->stmt))
+	/* Get the base of var_after.  */
+	base_cand_at_use = fold_build_plus (PLUS_EXPR, cand->iv->base,
+					     cand->iv->step);
+      else
+	/* Get the base of var_before.  */
+	base_cand_at_use = cand->iv->base;
+    }
+  else
+    {
+      /* It is possible to also allow other pointer cands, provided we can prove
+	 cand->iv->base + nit * cand->iv->step doesn't overflow.  Return false
+	 for now.  */
+      return false;
+    }
+
+  /* Detect p = base + s.  */
+  base_ptr = fold_diff_to_ssa_name (base_cand_at_use,
+				    fold_convert (sizetype, use->iv->base));
+  if (base_ptr == NULL_TREE)
+    return false;
+  STRIP_NOPS (base_ptr);
+  /* Get the bound of the iv of the use.  */
+  if (!get_use_lt_bound (use, *use_iv, &use_lt_bound, &use_comp))
+    return false;
+
+  /* Determine canon_comp.  */
+  if (*comp_p == NE_EXPR)
+    canon_comp = use_comp;
+  else if (*comp_p == EQ_EXPR)
+    canon_comp = invert_tree_comparison (use_comp, false);
+  else
+    gcc_unreachable ();
+
+  /* Allow positive and negative step, and inclusive and exclusive bound.
+     To trigger inclusive bound, we need -funsafe-loop-optimizations.  */
+  if (canon_comp != LT_EXPR && canon_comp != GT_EXPR
+      && canon_comp != LE_EXPR && canon_comp != GE_EXPR)
+    return false;
+
+  /* Calculate bound.  */
+  bound = fold_build_plus (PLUS_EXPR, base_ptr,
+                           fold_convert (sizetype, use_lt_bound));
+  if (expression_expensive_p (bound))
+    return false;
+
+  *comp_p = use_comp;
+  *bound_p = bound;
+  return true;
+}
+
 /* Check whether it is possible to express the condition in USE by comparison
-   of candidate CAND.  If so, store the value compared with to BOUND.  */
+   of candidate CAND.  If so, store the value compared with to BOUND, and the
+   comparison operator to COMP.  */
 
 static bool
 may_eliminate_iv (struct ivopts_data *data,
-		  struct iv_use *use, struct iv_cand *cand, tree *bound)
+		  struct iv_use *use, struct iv_cand *cand, tree *bound,
+		  enum tree_code *comp)
 {
   basic_block ex_bb;
   edge exit;
@@ -4362,17 +4626,8 @@ may_eliminate_iv (struct ivopts_data *data,
   /* If the number of iterations is constant, compare against it directly.  */
   if (TREE_CODE (nit) == INTEGER_CST)
     {
-      /* See cand_value_at.  */
-      if (stmt_after_increment (loop, cand, use->stmt))
-        {
-          if (!tree_int_cst_lt (nit, period))
-            return false;
-        }
-      else
-        {
-          if (tree_int_cst_lt (period, nit))
-            return false;
-        }
+      if (tree_int_cst_lt (period, nit))
+        return false;
     }
 
   /* If not, and if this is the only possible exit of the loop, see whether
@@ -4383,8 +4638,6 @@ may_eliminate_iv (struct ivopts_data *data,
       double_int period_value, max_niter;
 
       max_niter = desc->max;
-      if (stmt_after_increment (loop, cand, use->stmt))
-        max_niter = double_int_add (max_niter, double_int_one);
       period_value = tree_to_double_int (period);
       if (double_int_ucmp (max_niter, period_value) > 0)
         {
@@ -4393,7 +4646,6 @@ may_eliminate_iv (struct ivopts_data *data,
             {
               if (!estimated_loop_iterations (loop, true, &max_niter))
                 return false;
-              /* The loop bound is already adjusted by adding 1.  */
               if (double_int_ucmp (max_niter, period_value) > 0)
                 return false;
             }
@@ -4405,6 +4657,23 @@ may_eliminate_iv (struct ivopts_data *data,
   cand_value_at (loop, cand, use->stmt, nit, &bnd);
 
   *bound = aff_combination_to_tree (&bnd);
+  *comp = iv_elimination_compare (data, use);
+
+  /* Try to implement nit using a '<' instead.  */
+  if (TREE_CODE (nit) == COND_EXPR)
+    {
+      if (iv_elimination_compare_lt (data, use, cand, bound, comp))
+        return true;
+
+      /* We could try to see if the non-lt bound is not too expensive, but the
+         cost infrastructure needs tuning for that first.  Even though
+         expression_expensive_p always returns true for COND_EXPRs, it happens
+         that the bound is folded into a MAX_EXPR, which is approved by
+         expression_expensive_p, but attributed a too low cost by force_var_cost
+         in case the MAX_EXPR would expand into control flow.  */
+      return false;
+    }
+
   /* It is unlikely that computing the number of iterations using division
      would be more profitable than keeping the original induction variable.  */
   if (expression_expensive_p (*bound))
@@ -4426,16 +4695,18 @@ determine_use_iv_cost_condition (struct ivopts_data *data,
   bool ok;
   int inv_expr_id = -1;
   tree *control_var, *bound_cst;
+  enum tree_code comp;
 
   /* Only consider real candidates.  */
   if (!cand->iv)
     {
-      set_use_iv_cost (data, use, cand, infinite_cost, NULL, NULL_TREE, -1);
+      set_use_iv_cost (data, use, cand, infinite_cost, NULL, NULL_TREE, -1,
+		       ERROR_MARK);
       return false;
     }
 
   /* Try iv elimination.  */
-  if (may_eliminate_iv (data, use, cand, &bound))
+  if (may_eliminate_iv (data, use, cand, &bound, &comp))
     {
       elim_cost = force_var_cost (data, bound, &depends_on_elim);
       /* The bound is a loop invariant, so it will be only computed
@@ -4482,9 +4753,10 @@ determine_use_iv_cost_condition (struct ivopts_data *data,
       depends_on = depends_on_express;
       depends_on_express = NULL;
       bound = NULL_TREE;
+      comp = ERROR_MARK;
     }
 
-  set_use_iv_cost (data, use, cand, cost, depends_on, bound, inv_expr_id);
+  set_use_iv_cost (data, use, cand, cost, depends_on, bound, inv_expr_id, comp);
 
   if (depends_on_elim)
     BITMAP_FREE (depends_on_elim);
@@ -6119,7 +6391,7 @@ rewrite_use_compare (struct ivopts_data *data,
           fprintf (dump_file, "Replacing exit test: ");
           print_gimple_stmt (dump_file, use->stmt, 0, TDF_SLIM);
         }
-      compare = iv_elimination_compare (data, use);
+      compare = cp->comp;
       bound = unshare_expr (fold_convert (var_type, bound));
       op = force_gimple_operand (bound, &stmts, true, NULL_TREE);
       if (stmts)
@@ -6351,7 +6623,7 @@ tree_ssa_iv_optimize_loop (struct ivopts_data *data, struct loop *loop)
 {
   bool changed = false;
   struct iv_ca *iv_ca;
-  edge exit;
+  edge exit = single_dom_exit (loop);
   basic_block *body;
 
   gcc_assert (!data->niters);
@@ -6362,7 +6634,6 @@ tree_ssa_iv_optimize_loop (struct ivopts_data *data, struct loop *loop)
     {
       fprintf (dump_file, "Processing loop %d\n", loop->num);
 
-      exit = single_dom_exit (loop);
       if (exit)
 	{
 	  fprintf (dump_file, "  single exit %d -> %d, exit condition ",
@@ -6379,6 +6650,8 @@ tree_ssa_iv_optimize_loop (struct ivopts_data *data, struct loop *loop)
   renumber_gimple_stmt_uids_in_blocks (body, loop->num_nodes);
   free (body);
 
+  data->loop_single_exit_p = exit != NULL && loop_only_exit_p (loop, exit);
+
   /* For each ssa name determines whether it behaves as an induction variable
      in some loop.  */
   if (!find_induction_variables (data))
diff --git a/gcc/tree-ssa-pre.c b/gcc/tree-ssa-pre.c
index 2753fd7..40c346c 100644
--- a/gcc/tree-ssa-pre.c
+++ b/gcc/tree-ssa-pre.c
@@ -3495,6 +3495,47 @@ insert_into_preds_of_block (basic_block block, unsigned int exprnum,
 }
 
 
+/* Indicate if, when optimizing for speed, it is appropriate to make
+   INSERTS_NEEDED insertions in order to make EXPR in BLOCK redundant.  */
+static bool
+ppre_n_insert_for_speed_p (pre_expr expr, basic_block block,
+			   unsigned int inserts_needed)
+{
+  /* The more expensive EXPR is, the more we should be prepared to insert
+     in the predecessors of BLOCK to make EXPR fully redundant.
+     For now, only recognize AND, OR, XOR, PLUS and MINUS of a multiple-use
+     SSA_NAME with a constant as cheap.  */
+  int cost;
+
+  if (flag_tree_pre_partial_partial_obliviously)
+    return true;
+  if (expr->kind == NARY)
+    {
+      vn_nary_op_t nary = PRE_EXPR_NARY (expr);
+      switch (nary->opcode)
+	{
+	  tree name, cnst;
+	case BIT_AND_EXPR: case BIT_IOR_EXPR: case BIT_XOR_EXPR:
+	case PLUS_EXPR: case MINUS_EXPR:
+
+	  gcc_assert (nary->length == 2);
+	  name = nary->op[0];
+	  cnst = nary->op[1];
+	  if (TREE_CODE (name) != SSA_NAME || has_single_use (name))
+	    return true;
+	  if (!is_gimple_min_invariant (cnst))
+	    return true;
+	  cost = 1;
+	  break;
+	default:
+	  return true;
+	}
+    }
+  else
+    return true;
+  return EDGE_COUNT (block->preds) * cost >= inserts_needed;
+
+}
 
 /* Perform insertion of partially redundant values.
    For BLOCK, do the following:
@@ -3749,10 +3790,23 @@ do_partial_partial_insertion (basic_block block, basic_block dom)
 	  if (!cant_insert && by_all && dbg_cnt (treepre_insert))
 	    {
 	      pre_stats.pa_insert++;
-	      if (insert_into_preds_of_block (block, get_expression_id (expr),
-					      avail))
-		new_stuff = true;
-	    }
+	      /* Assuming the expression is 50% anticipatable, we have
+		 to multiply the number of insertions needed by two for a cost
+		 comparison.  */
+	      if (!optimize_function_for_speed_p (cfun)
+		  || ppre_n_insert_for_speed_p (expr, block,
+						2 * EDGE_COUNT (block->preds)))
+		{
+		  if (insert_into_preds_of_block (block,
+						  get_expression_id (expr),
+						  avail))
+		    new_stuff = true;
+		}  
+	      else if (dump_file && (dump_flags & TDF_DETAILS))
+		fprintf (dump_file, "Not inserting (optimizing for %s)\n",
+			 optimize_function_for_speed_p (cfun)
+			 ? "speed" : "size");
+	    }	    
 	  free (avail);
 	}
     }
@@ -4838,7 +4892,8 @@ execute_pre (bool do_fre)
 {
   unsigned int todo = 0;
 
-  do_partial_partial = optimize > 2 && optimize_function_for_speed_p (cfun);
+  do_partial_partial =
+    flag_tree_pre_partial_partial && optimize_function_for_speed_p (cfun);
 
   /* This has to happen before SCCVN runs because
      loop_optimizer_init may create new phis, etc.  */
diff --git a/gcc/tree-ssa-sccvn.c b/gcc/tree-ssa-sccvn.c
index 6ed076d..a30e2b0 100644
--- a/gcc/tree-ssa-sccvn.c
+++ b/gcc/tree-ssa-sccvn.c
@@ -684,6 +684,7 @@ copy_reference_ops_from_ref (tree ref, VEC(vn_reference_op_s, heap) **result)
 	case COMPLEX_CST:
 	case VECTOR_CST:
 	case REAL_CST:
+	case FIXED_CST:
 	case CONSTRUCTOR:
 	case VAR_DECL:
 	case PARM_DECL:
diff --git a/gcc/tree-ssa-sink.c b/gcc/tree-ssa-sink.c
index 962c295..f9267a1 100644
--- a/gcc/tree-ssa-sink.c
+++ b/gcc/tree-ssa-sink.c
@@ -478,6 +478,7 @@ sink_code_in_bb (basic_block bb)
 	  last = false;
 	  continue;
 	}
+
       if (dump_file)
 	{
 	  fprintf (dump_file, "Sinking ");
diff --git a/gcc/tree-ssa-uninit.c b/gcc/tree-ssa-uninit.c
index 39785d4..02b166a 100644
--- a/gcc/tree-ssa-uninit.c
+++ b/gcc/tree-ssa-uninit.c
@@ -1605,6 +1605,157 @@ is_superset_of (VEC(use_pred_info_t, heap) **preds1,
   return true;
 }
 
+/* Comparison function used by qsort. It is used to
+   sort predicate chains to allow predicate
+   simplification.  */
+
+static int
+pred_chain_length_cmp (const void *p1, const void *p2)
+{
+  use_pred_info_t i1, i2;
+  VEC(use_pred_info_t, heap) * const *chain1
+      = (VEC(use_pred_info_t, heap) * const *)p1;
+  VEC(use_pred_info_t, heap) * const *chain2
+      = (VEC(use_pred_info_t, heap) * const *)p2;
+
+  if (VEC_length (use_pred_info_t, *chain1)
+      != VEC_length (use_pred_info_t, *chain2))
+    return (VEC_length (use_pred_info_t, *chain1)
+            - VEC_length (use_pred_info_t, *chain2));
+
+  i1 = VEC_index (use_pred_info_t, *chain1, 0);
+  i2 = VEC_index (use_pred_info_t, *chain2, 0);
+
+  /* Allow predicates with similar prefix come together.  */
+  if (!i1->invert && i2->invert)
+    return -1;
+  else if (i1->invert && !i2->invert)
+    return 1;
+
+  return gimple_uid (i1->cond) - gimple_uid (i2->cond);
+}
+
+/* x OR (!x AND y) is equivalent to x OR y.
+   This function normalizes x1 OR (!x1 AND x2) OR (!x1 AND !x2 AND x3)
+   into x1 OR x2 OR x3.  PREDS is the predicate chains, and N is
+   the number of chains. Returns true if normalization happens.  */
+
+static bool
+normalize_preds (VEC(use_pred_info_t, heap) **preds, size_t *n)
+{
+  size_t i, j, ll;
+  VEC(use_pred_info_t, heap) *pred_chain;
+  VEC(use_pred_info_t, heap) *x = 0;
+  use_pred_info_t xj = 0, nxj = 0;
+
+  if (*n < 2)
+    return false;
+
+  /* First sort the chains in ascending order of lengths.  */
+  qsort (preds, *n, sizeof (void *), pred_chain_length_cmp);
+  pred_chain = preds[0];
+  ll = VEC_length (use_pred_info_t, pred_chain);
+  if (ll != 1)
+   {
+     if (ll == 2)
+       {
+         use_pred_info_t xx, yy, xx2, nyy;
+         VEC(use_pred_info_t, heap) *pred_chain2 = preds[1];
+         if (VEC_length (use_pred_info_t, pred_chain2) != 2)
+           return false;
+
+         /* See if simplification x AND y OR x AND !y is possible.  */
+         xx = VEC_index (use_pred_info_t, pred_chain, 0);
+         yy = VEC_index (use_pred_info_t, pred_chain, 1);
+         xx2 = VEC_index (use_pred_info_t, pred_chain2, 0);
+         nyy = VEC_index (use_pred_info_t, pred_chain2, 1);
+         if (gimple_cond_lhs (xx->cond) != gimple_cond_lhs (xx2->cond)
+             || gimple_cond_rhs (xx->cond) != gimple_cond_rhs (xx2->cond)
+             || gimple_cond_code (xx->cond) != gimple_cond_code (xx2->cond)
+             || (xx->invert != xx2->invert))
+           return false;
+         if (gimple_cond_lhs (yy->cond) != gimple_cond_lhs (nyy->cond)
+             || gimple_cond_rhs (yy->cond) != gimple_cond_rhs (nyy->cond)
+             || gimple_cond_code (yy->cond) != gimple_cond_code (nyy->cond)
+             || (yy->invert == nyy->invert))
+           return false;
+
+         /* Now merge the first two chains.  */
+         free (yy);
+         free (nyy);
+         free (xx2);
+         VEC_free (use_pred_info_t, heap, pred_chain);
+         VEC_free (use_pred_info_t, heap, pred_chain2);
+         pred_chain = 0;
+         VEC_safe_push (use_pred_info_t, heap, pred_chain, xx);
+         preds[0] = pred_chain;
+         for (i = 1; i < *n - 1; i++)
+           preds[i] = preds[i + 1];
+
+         preds[*n - 1] = 0;
+         *n = *n - 1;
+       }
+     else
+       return false;
+   }
+
+  VEC_safe_push (use_pred_info_t, heap, x,
+                 VEC_index (use_pred_info_t, pred_chain, 0));
+
+  /* The loop extracts x1, x2, x3, etc from chains
+     x1 OR (!x1 AND x2) OR (!x1 AND !x2 AND x3) OR ...  */
+  for (i = 1; i < *n; i++)
+    {
+      pred_chain = preds[i];
+      if (VEC_length (use_pred_info_t, pred_chain) != i + 1)
+        return false;
+
+      for (j = 0; j < i; j++)
+        {
+          xj = VEC_index (use_pred_info_t, x, j);
+          nxj = VEC_index (use_pred_info_t, pred_chain, j);
+
+          /* Check if nxj is !xj  */
+          if (gimple_cond_lhs (xj->cond) != gimple_cond_lhs (nxj->cond)
+              || gimple_cond_rhs (xj->cond) != gimple_cond_rhs (nxj->cond)
+              || gimple_cond_code (xj->cond) != gimple_cond_code (nxj->cond)
+              || (xj->invert == nxj->invert))
+            return false;
+        }
+
+      VEC_safe_push (use_pred_info_t, heap, x,
+                     VEC_index (use_pred_info_t, pred_chain, i));
+    }
+
+  /* Now normalize the pred chains using the extraced x1, x2, x3 etc.  */
+  for (j = 0; j < *n; j++)
+    {
+      use_pred_info_t t;
+      xj = VEC_index (use_pred_info_t, x, j);
+
+      t = XNEW (struct use_pred_info);
+      *t = *xj;
+
+      VEC_replace (use_pred_info_t, x, j, t);
+    }
+
+  for (i = 0; i < *n; i++)
+    {
+      pred_chain = preds[i];
+      for (j = 0; j < VEC_length (use_pred_info_t, pred_chain); j++)
+        free (VEC_index (use_pred_info_t, pred_chain, j));
+      VEC_free (use_pred_info_t, heap, pred_chain);
+      pred_chain = 0;
+      /* A new chain.  */
+      VEC_safe_push (use_pred_info_t, heap, pred_chain,
+                     VEC_index (use_pred_info_t, x, i));
+      preds[i] = pred_chain;
+    }
+  return true;
+}
+
+
+
 /* Computes the predicates that guard the use and checks
    if the incoming paths that have empty (or possibly
    empty) defintion can be pruned/filtered. The function returns
@@ -1658,9 +1809,18 @@ is_use_properly_guarded (gimple use_stmt,
 
   if (has_valid_preds)
     {
+      bool normed;
       if (dump_file)
         dump_predicates (phi, num_def_preds, def_preds,
                          "Operand defs of phi ");
+
+      normed = normalize_preds (def_preds, &num_def_preds);
+      if (normed && dump_file)
+        {
+          fprintf (dump_file, "\nNormalized to\n");
+          dump_predicates (phi, num_def_preds, def_preds,
+                           "Operand defs of phi ");
+        }
       is_properly_guarded =
           is_superset_of (def_preds, num_def_preds,
                           preds, num_preds);
diff --git a/gcc/tree.c b/gcc/tree.c
index 83a0711..e92ec53 100644
--- a/gcc/tree.c
+++ b/gcc/tree.c
@@ -9447,6 +9447,10 @@ build_common_builtin_nodes (void)
 	const char *p;
 	enum built_in_function mcode, dcode;
 	tree type, inner_type;
+	const char *prefix = "__";
+
+	if (targetm.libfunc_gnu_prefix)
+	  prefix = "__gnu_";
 
 	type = lang_hooks.types.type_for_mode ((enum machine_mode) mode, 0);
 	if (type == NULL)
@@ -9465,13 +9469,17 @@ build_common_builtin_nodes (void)
 	  *q = TOLOWER (*p);
 	*q = '\0';
 
-	built_in_names[mcode] = concat ("__mul", mode_name_buf, "3", NULL);
+	built_in_names[mcode] = concat (prefix, "mul", mode_name_buf, "3",
+					NULL);
         local_define_builtin (built_in_names[mcode], ftype, mcode,
-			      built_in_names[mcode], ECF_CONST | ECF_NOTHROW | ECF_LEAF);
+			      built_in_names[mcode],
+			      ECF_CONST | ECF_NOTHROW | ECF_LEAF);
 
-	built_in_names[dcode] = concat ("__div", mode_name_buf, "3", NULL);
+	built_in_names[dcode] = concat (prefix, "div", mode_name_buf, "3",
+					NULL);
         local_define_builtin (built_in_names[dcode], ftype, dcode,
-			      built_in_names[dcode], ECF_CONST | ECF_NOTHROW | ECF_LEAF);
+			      built_in_names[dcode],
+			      ECF_CONST | ECF_NOTHROW | ECF_LEAF);
       }
   }
 }
@@ -10997,7 +11005,7 @@ tree
 lhd_gcc_personality (void)
 {
   if (!gcc_eh_personality_decl)
-    gcc_eh_personality_decl = build_personality_function ("gcc");
+    gcc_eh_personality_decl = build_personality_function ("gcc", false);
   return gcc_eh_personality_decl;
 }
 
diff --git a/gcc/tree.h b/gcc/tree.h
index ece68b4..64d4558 100644
--- a/gcc/tree.h
+++ b/gcc/tree.h
@@ -2653,6 +2653,9 @@ struct GTY(()) tree_decl_minimal {
 #define DECL_FUNCTION_PERSONALITY(NODE) \
   (FUNCTION_DECL_CHECK (NODE)->function_decl.personality)
 
+#define DECL_FUNCTION_PERSONALITY2(NODE) \
+  (FUNCTION_DECL_CHECK (NODE)->function_decl.personality2)
+
 /* Nonzero for a given ..._DECL node means that the name of this node should
    be ignored for symbolic debug purposes.  For a TYPE_DECL, this means that
    the associated type should be ignored.  For a FUNCTION_DECL, the body of
@@ -3374,6 +3377,11 @@ struct GTY(()) tree_function_decl {
   /* The personality function. Used for stack unwinding. */
   tree personality;
 
+  /* An alternative personality function for the compact-encoding scheme.
+     The decision on whether to use this version will be deferred
+     until final.  */
+  tree personality2;
+
   /* Function specific options that are used by this function.  */
   tree function_specific_target;	/* target options */
   tree function_specific_optimization;	/* optimization options */
@@ -5595,7 +5603,7 @@ extern unsigned HOST_WIDE_INT compute_builtin_object_size (tree, int);
 
 /* In expr.c.  */
 extern unsigned HOST_WIDE_INT highest_pow2_factor (const_tree);
-extern tree build_personality_function (const char *);
+extern tree build_personality_function (const char *, bool);
 
 /* In tree-inline.c.  */
 
diff --git a/gcc/unwind-compat.c b/gcc/unwind-compat.c
index 5b41f24..2fbd69d 100644
--- a/gcc/unwind-compat.c
+++ b/gcc/unwind-compat.c
@@ -207,4 +207,14 @@ _Unwind_SetIP (struct _Unwind_Context *context, _Unwind_Ptr val)
   return __libunwind_Unwind_SetIP (context, val);
 }
 symver (_Unwind_SetIP, GCC_3.0);
+
+extern unsigned char __libunwind_Unwind_GetEhEncoding
+  (struct _Unwind_Context *);
+
+unsigned char
+_Unwind_GetEhEncoding (struct _Unwind_Context *context)
+{
+  return __libunwind_Unwind_GetEhEncoding (context, val);
+}
+symver (_Unwind_GetEhEncoding, GCC_3.0);
 #endif
diff --git a/gcc/unwind-dw2-fde-glibc.c b/gcc/unwind-dw2-fde-glibc.c
index d8e3c0e..400d7c0 100644
--- a/gcc/unwind-dw2-fde-glibc.c
+++ b/gcc/unwind-dw2-fde-glibc.c
@@ -45,6 +45,9 @@
 #include "unwind-dw2-fde.h"
 #include "unwind-compat.h"
 #include "gthr.h"
+#ifdef MD_HAVE_COMPACT_EH
+#include "unwind-compact.h"
+#endif
 
 #if !defined(inhibit_libc) && defined(HAVE_LD_EH_FRAME_HDR) \
     && (__GLIBC__ > 2 || (__GLIBC__ == 2 && __GLIBC_MINOR__ > 2) \
@@ -72,11 +75,11 @@
 # define __RELOC_POINTER(ptr, base) ((ptr) + (base))
 #endif
 
-static const fde * _Unwind_Find_registered_FDE (void *pc, struct dwarf_eh_bases *bases);
+static enum compact_entry_type
+_Unwind_Find_registered_Index (void *pc, struct compact_eh_bases *bases);
 
-#define _Unwind_Find_FDE _Unwind_Find_registered_FDE
+#define _Unwind_Find_registered_Index _Unwind_Find_registered_Index
 #include "unwind-dw2-fde.c"
-#undef _Unwind_Find_FDE
 
 #ifndef PT_GNU_EH_FRAME
 #define PT_GNU_EH_FRAME (PT_LOOS + 0x474e550)
@@ -85,10 +88,8 @@ static const fde * _Unwind_Find_registered_FDE (void *pc, struct dwarf_eh_bases
 struct unw_eh_callback_data
 {
   _Unwind_Ptr pc;
-  void *tbase;
-  void *dbase;
-  void *func;
-  const fde *ret;
+  struct compact_eh_bases *bases;
+  enum compact_entry_type type;
   int check_cache;
 };
 
@@ -131,9 +132,9 @@ base_from_cb_data (unsigned char encoding, struct unw_eh_callback_data *data)
       return 0;
 
     case DW_EH_PE_textrel:
-      return (_Unwind_Ptr) data->tbase;
+      return (_Unwind_Ptr) data->bases->tbase;
     case DW_EH_PE_datarel:
-      return (_Unwind_Ptr) data->dbase;
+      return (_Unwind_Ptr) data->bases->dbase;
     default:
       gcc_unreachable ();
     }
@@ -155,6 +156,7 @@ _Unwind_IteratePhdrCallback (struct dl_phdr_info *info, size_t size, void *ptr)
   _Unwind_Ptr eh_frame;
   struct object ob;
   _Unwind_Ptr pc_low = 0, pc_high = 0;
+  const fde *f;
 
   struct ext_dl_phdr_info
     {
@@ -303,12 +305,10 @@ _Unwind_IteratePhdrCallback (struct dl_phdr_info *info, size_t size, void *ptr)
   /* Read .eh_frame_hdr header.  */
   hdr = (const struct unw_eh_frame_hdr *)
     __RELOC_POINTER (p_eh_frame_hdr->p_vaddr, load_base);
-  if (hdr->version != 1)
-    return 1;
 
 #ifdef CRT_GET_RFIB_DATA
-# ifdef __i386__
-  data->dbase = NULL;
+# if defined(__i386__) || defined(__mips__)
+  data->bases->dbase = NULL;
   if (p_dynamic)
     {
       /* For dynamically linked executables and shared libraries,
@@ -318,19 +318,22 @@ _Unwind_IteratePhdrCallback (struct dl_phdr_info *info, size_t size, void *ptr)
       for (; dyn->d_tag != DT_NULL ; dyn++)
 	if (dyn->d_tag == DT_PLTGOT)
 	  {
-	    data->dbase = (void *) dyn->d_un.d_ptr;
-#if defined __linux__
+	    data->bases->dbase = (void *) dyn->d_un.d_ptr;
+#if defined(__mips__)
+	    /* On MIPS we need to relocate the value and offset.  */
+	    data->bases->dbase += load_base + 0x7ff0;
+#elif defined __linux__
 	    /* On IA-32 Linux, _DYNAMIC is writable and GLIBC has
 	       relocated it.  */
 #elif defined __sun__ && defined __svr4__
 	    /* On Solaris 2/x86, we need to do this ourselves.  */
-	    data->dbase += load_base;
+	    data->bases->dbase += load_base;
 #endif
 	    break;
 	  }
     }
 # elif defined __FRV_FDPIC__ && defined __linux__
-  data->dbase = load_base.got_value;
+  data->bases->dbase = load_base.got_value;
 # elif defined __x86_64__ && defined __sun__ && defined __svr4__
   /* While CRT_GET_RFIB_DATA is also defined for 64-bit Solaris 10+/x86, it
      doesn't apply since it uses DW_EH_PE_pcrel encoding.  */
@@ -339,6 +342,18 @@ _Unwind_IteratePhdrCallback (struct dl_phdr_info *info, size_t size, void *ptr)
 # endif
 #endif
 
+#ifdef MD_HAVE_COMPACT_EH
+  if (hdr->version == 2)
+    {
+      data->type = _Unwind_Search_Compact_eh_hdr ((void *)data->pc,
+	  (const unsigned char *) hdr, data->bases);
+      return 1;
+    }
+#endif
+
+  if (hdr->version != 1)
+    return 1;
+
   p = read_encoded_value_with_base (hdr->eh_frame_ptr_enc,
 				    base_from_cb_data (hdr->eh_frame_ptr_enc,
 						       data),
@@ -369,7 +384,6 @@ _Unwind_IteratePhdrCallback (struct dl_phdr_info *info, size_t size, void *ptr)
 	  const struct fde_table *table = (const struct fde_table *) p;
 	  size_t lo, hi, mid;
 	  _Unwind_Ptr data_base = (_Unwind_Ptr) hdr;
-	  fde *f;
 	  unsigned int f_enc, f_enc_size;
 	  _Unwind_Ptr range;
 
@@ -401,8 +415,11 @@ _Unwind_IteratePhdrCallback (struct dl_phdr_info *info, size_t size, void *ptr)
 	  read_encoded_value_with_base (f_enc & 0x0f, 0,
 					&f->pc_begin[f_enc_size], &range);
 	  if (data->pc < table[mid].initial_loc + data_base + range)
-	    data->ret = f;
-	  data->func = (void *) (table[mid].initial_loc + data_base);
+	    {
+	      data->bases->entry = f;
+	      data->type = CET_FDE;
+	    }
+	  data->bases->func = (void *) (table[mid].initial_loc + data_base);
 	  return 1;
 	}
     }
@@ -411,58 +428,55 @@ _Unwind_IteratePhdrCallback (struct dl_phdr_info *info, size_t size, void *ptr)
      As soon as GLIBC will provide API so to notify that a library has been
      removed, we could cache this (and thus use search_object).  */
   ob.pc_begin = NULL;
-  ob.tbase = data->tbase;
-  ob.dbase = data->dbase;
+  ob.tbase = data->bases->tbase;
+  ob.dbase = data->bases->dbase;
   ob.u.single = (fde *) eh_frame;
   ob.s.i = 0;
   ob.s.b.mixed_encoding = 1;  /* Need to assume worst case.  */
-  data->ret = linear_search_fdes (&ob, (fde *) eh_frame, (void *) data->pc);
-  if (data->ret != NULL)
+  f = linear_search_fdes (&ob, (fde *) eh_frame, (void *) data->pc);
+  data->bases->entry = f;
+  if (f != NULL)
     {
       _Unwind_Ptr func;
-      unsigned int encoding = get_fde_encoding (data->ret);
+      unsigned int encoding = get_fde_encoding (f);
 
       read_encoded_value_with_base (encoding,
 				    base_from_cb_data (encoding, data),
-				    data->ret->pc_begin, &func);
-      data->func = (void *) func;
+				    f->pc_begin, &func);
+      data->bases->func = (void *) func;
+      data->type = CET_FDE;
     }
   return 1;
 }
 
-const fde *
-_Unwind_Find_FDE (void *pc, struct dwarf_eh_bases *bases)
+enum compact_entry_type
+_Unwind_Find_Index (void *pc, struct compact_eh_bases *bases)
 {
   struct unw_eh_callback_data data;
-  const fde *ret;
 
-  ret = _Unwind_Find_registered_FDE (pc, bases);
-  if (ret != NULL)
-    return ret;
+  data.type = _Unwind_Find_registered_Index (pc, bases);
+  if (data.type != CET_not_found)
+    return data.type;
 
   data.pc = (_Unwind_Ptr) pc;
-  data.tbase = NULL;
-  data.dbase = NULL;
-  data.func = NULL;
-  data.ret = NULL;
+  data.bases = bases;
+  data.type = CET_not_found;
   data.check_cache = 1;
 
   if (dl_iterate_phdr (_Unwind_IteratePhdrCallback, &data) < 0)
-    return NULL;
+    return CET_not_found;
 
-  if (data.ret)
-    {
-      bases->tbase = data.tbase;
-      bases->dbase = data.dbase;
-      bases->func = data.func;
-    }
-  return data.ret;
+  if (data.type != CET_not_found)
+    return data.type;
+
+  return CET_not_found;
 }
 
-#else
-/* Prevent multiple include of header files.  */
-#define _Unwind_Find_FDE _Unwind_Find_FDE
+#else /* !USE_PT_GNU_EH_FRAME */
+
+#define _Unwind_Find_registered_Index _Unwind_Find_Index
 #include "unwind-dw2-fde.c"
+
 #endif
 
 #if defined (USE_GAS_SYMVER) && defined (SHARED) && defined (USE_LIBUNWIND_EXCEPTIONS)
diff --git a/gcc/unwind-dw2-fde.c b/gcc/unwind-dw2-fde.c
index 93d4271..e38311d 100644
--- a/gcc/unwind-dw2-fde.c
+++ b/gcc/unwind-dw2-fde.c
@@ -24,7 +24,8 @@ a copy of the GCC Runtime Library Exception along with this program;
 see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
 <http://www.gnu.org/licenses/>.  */
 
-#ifndef _Unwind_Find_FDE
+#ifndef _Unwind_Find_registered_Index
+#define _Unwind_Find_registered_Index _Unwind_Find_Index
 #include "tconfig.h"
 #include "tsystem.h"
 #include "coretypes.h"
@@ -35,6 +36,9 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
 #include "unwind-pe.h"
 #include "unwind-dw2-fde.h"
 #include "gthr.h"
+#ifdef MD_HAVE_COMPACT_EH
+#include "unwind-compact.h"
+#endif
 #endif
 
 /* The unseen_objects list contains objects that have been registered
@@ -67,22 +71,20 @@ init_object_mutex_once (void)
 #define init_object_mutex_once()
 #endif
 
-/* Called from crtbegin.o to register the unwind info for an object.  */
+/* Initialize OB and add to list of objects.  */
 
-void
-__register_frame_info_bases (const void *begin, struct object *ob,
-			     void *tbase, void *dbase)
+static void
+__register_frame_info_1 (const void *begin, struct object *ob,
+			  void *tbase, void *dbase, int header)
 {
-  /* If .eh_frame is empty, don't register at all.  */
-  if ((const uword *) begin == 0 || *(const uword *) begin == 0)
-    return;
-
   ob->pc_begin = (void *)-1;
   ob->tbase = tbase;
   ob->dbase = dbase;
   ob->u.single = begin;
   ob->s.i = 0;
   ob->s.b.encoding = DW_EH_PE_omit;
+  ob->s.b.header = header;
+  ob->s.b.mixed_encoding = header;
 #ifdef DWARF2_OBJECT_END_PTR_EXTENSION
   ob->fde_end = NULL;
 #endif
@@ -96,6 +98,34 @@ __register_frame_info_bases (const void *begin, struct object *ob,
   __gthread_mutex_unlock (&object_mutex);
 }
 
+/* Called from crtbegin.o to register the unwind info for an object.  */
+
+void
+__register_frame_info_header_bases (const void *begin, struct object *ob,
+				    void *tbase, void *dbase)
+{
+  /* Only register compact EH frame headers.  */
+  if (begin == NULL || *(const char *) begin != 2)
+    return;
+
+#ifdef MD_HAVE_COMPACT_EH
+  __register_frame_info_1 (begin, ob, tbase, dbase, 1);
+#endif
+}
+
+/* Called from crtbegin.o to register the unwind info for an object.  */
+
+void
+__register_frame_info_bases (const void *begin, struct object *ob,
+			     void *tbase, void *dbase)
+{
+  /* If .eh_frame is empty, don't register at all.  */
+  if ((const uword *) begin == 0 || *(const uword *) begin == 0)
+    return;
+
+  __register_frame_info_1 (begin, ob, tbase, dbase, 0);
+}
+
 void
 __register_frame_info (const void *begin, struct object *ob)
 {
@@ -947,9 +977,25 @@ binary_search_mixed_encoding_fdes (struct object *ob, void *pc)
   return NULL;
 }
 
-static const fde *
-search_object (struct object* ob, void *pc)
+static enum compact_entry_type
+search_object (struct object* ob, void *pc, struct compact_eh_bases *bases)
 {
+  const fde *f = NULL;
+
+#ifdef MD_HAVE_COMPACT_EH
+  if (ob->s.b.header)
+    {
+      const unsigned char *hdr = (const unsigned char *) ob->u.single;
+      if (ob->pc_begin == (void *) -1)
+	{
+	  const sword *first = (const sword *) (hdr + 8);
+	  ob->pc_begin = (void *)((*first & ~1) + (_Unwind_Ptr) first);
+	}
+
+      return _Unwind_Search_Compact_eh_hdr (pc, hdr, bases);
+    }
+#endif
+
   /* If the data hasn't been sorted, try to do this now.  We may have
      more memory available than last time we tried.  */
   if (! ob->s.b.sorted)
@@ -960,17 +1006,17 @@ search_object (struct object* ob, void *pc)
 	 that we've not processed this object before.  A quick range
 	 check is in order.  */
       if (pc < ob->pc_begin)
-	return NULL;
+	return CET_not_found;
     }
 
   if (ob->s.b.sorted)
     {
       if (ob->s.b.mixed_encoding)
-	return binary_search_mixed_encoding_fdes (ob, pc);
+	f = binary_search_mixed_encoding_fdes (ob, pc);
       else if (ob->s.b.encoding == DW_EH_PE_absptr)
-	return binary_search_unencoded_fdes (ob, pc);
+	f = binary_search_unencoded_fdes (ob, pc);
       else
-	return binary_search_single_encoding_fdes (ob, pc);
+	f = binary_search_single_encoding_fdes (ob, pc);
     }
   else
     {
@@ -980,22 +1026,24 @@ search_object (struct object* ob, void *pc)
 	  fde **p;
 	  for (p = ob->u.array; *p ; p++)
 	    {
-	      const fde *f = linear_search_fdes (ob, *p, pc);
+	      f = linear_search_fdes (ob, *p, pc);
 	      if (f)
-		return f;
+		break;
 	    }
-	  return NULL;
 	}
       else
-	return linear_search_fdes (ob, ob->u.single, pc);
+	f = linear_search_fdes (ob, ob->u.single, pc);
     }
+
+  bases->entry = f;
+  return f ? CET_FDE : CET_not_found;
 }
 
-const fde *
-_Unwind_Find_FDE (void *pc, struct dwarf_eh_bases *bases)
+enum compact_entry_type
+_Unwind_Find_registered_Index (void *pc, struct compact_eh_bases *bases)
 {
   struct object *ob;
-  const fde *f = NULL;
+  enum compact_entry_type t;
 
   init_object_mutex_once ();
   __gthread_mutex_lock (&object_mutex);
@@ -1006,8 +1054,8 @@ _Unwind_Find_FDE (void *pc, struct dwarf_eh_bases *bases)
   for (ob = seen_objects; ob; ob = ob->next)
     if (pc >= ob->pc_begin)
       {
-	f = search_object (ob, pc);
-	if (f)
+	t = search_object (ob, pc, bases);
+	if (t != CET_not_found)
 	  goto fini;
 	break;
       }
@@ -1018,7 +1066,7 @@ _Unwind_Find_FDE (void *pc, struct dwarf_eh_bases *bases)
       struct object **p;
 
       unseen_objects = ob->next;
-      f = search_object (ob, pc);
+      t = search_object (ob, pc, bases);
 
       /* Insert the object into the classified list.  */
       for (p = &seen_objects; *p ; p = &(*p)->next)
@@ -1027,21 +1075,26 @@ _Unwind_Find_FDE (void *pc, struct dwarf_eh_bases *bases)
       ob->next = *p;
       *p = ob;
 
-      if (f)
+      if (t != CET_not_found)
 	goto fini;
     }
 
  fini:
   __gthread_mutex_unlock (&object_mutex);
 
-  if (f)
+  if (t == CET_not_found)
+    return CET_not_found;
+
+  bases->tbase = ob->tbase;
+  bases->dbase = ob->dbase;
+
+  if (t == CET_FDE)
     {
+      const fde *f;
       int encoding;
       _Unwind_Ptr func;
 
-      bases->tbase = ob->tbase;
-      bases->dbase = ob->dbase;
-
+      f = bases->entry;
       encoding = ob->s.b.encoding;
       if (ob->s.b.mixed_encoding)
 	encoding = get_fde_encoding (f);
@@ -1050,5 +1103,21 @@ _Unwind_Find_FDE (void *pc, struct dwarf_eh_bases *bases)
       bases->func = (void *) func;
     }
 
-  return f;
+  return t;
+}
+
+const fde *
+_Unwind_Find_FDE (void *pc, struct dwarf_eh_bases *bases)
+{
+  enum compact_entry_type type;
+  struct compact_eh_bases data;
+
+  type = _Unwind_Find_Index (pc, &data);
+  if (type != CET_FDE)
+    return NULL;
+
+  bases->tbase = data.tbase;
+  bases->dbase = data.dbase;
+  bases->func = data.func;
+  return (const fde *) data.entry;
 }
diff --git a/gcc/unwind-dw2-fde.h b/gcc/unwind-dw2-fde.h
index 6d7a8de..dfc5bfe 100644
--- a/gcc/unwind-dw2-fde.h
+++ b/gcc/unwind-dw2-fde.h
@@ -54,10 +54,11 @@ struct object
       unsigned long sorted : 1;
       unsigned long from_array : 1;
       unsigned long mixed_encoding : 1;
+      unsigned long header : 1;
       unsigned long encoding : 8;
       /* ??? Wish there was an easy way to detect a 64-bit host here;
 	 we've got 32 bits left to play with...  */
-      unsigned long count : 21;
+      unsigned long count : 20;
     } b;
     size_t i;
   } s;
@@ -88,9 +89,12 @@ struct dwarf_eh_bases
   void *tbase;
   void *dbase;
   void *func;
+  unsigned char eh_encoding;
 };
 
 
+extern void __register_frame_info_header_bases (const void *, struct object *,
+						void *, void *);
 extern void __register_frame_info_bases (const void *, struct object *,
 					 void *, void *);
 extern void __register_frame_info (const void *, struct object *);
@@ -166,6 +170,23 @@ next_fde (const fde *f)
 
 extern const fde * _Unwind_Find_FDE (void *, struct dwarf_eh_bases *);
 
+struct compact_eh_bases {
+    void *tbase;
+    void *dbase;
+    void *func;
+    const void *entry;
+    unsigned char eh_encoding;
+};
+enum compact_entry_type {
+    CET_not_found,
+    CET_FDE,
+    CET_inline,
+    CET_outline
+};
+
+extern enum compact_entry_type _Unwind_Find_Index (void *,
+    struct compact_eh_bases *);
+
 static inline int
 last_fde (struct object *obj __attribute__ ((__unused__)), const fde *f)
 {
diff --git a/gcc/unwind-dw2.c b/gcc/unwind-dw2.c
index 2ea9adb..498da41 100644
--- a/gcc/unwind-dw2.c
+++ b/gcc/unwind-dw2.c
@@ -327,6 +327,12 @@ _Unwind_GetTextRelBase (struct _Unwind_Context *context)
 {
   return (_Unwind_Ptr) context->bases.tbase;
 }
+
+unsigned char
+_Unwind_GetEhEncoding (struct _Unwind_Context *context)
+{
+  return context->bases.eh_encoding;
+}
 #endif
 
 #ifdef MD_UNWIND_SUPPORT
@@ -1115,6 +1121,79 @@ execute_cfa_program (const unsigned char *insn_ptr,
     }
 }
 
+#ifdef MD_HAVE_COMPACT_EH
+static _Unwind_Reason_Code
+__gnu_compact_pr1 (int version ATTRIBUTE_UNUSED,
+		  _Unwind_Action actions ATTRIBUTE_UNUSED,
+		  _Unwind_Exception_Class exception_class ATTRIBUTE_UNUSED,
+		  struct _Unwind_Exception *ue_header ATTRIBUTE_UNUSED,
+		  struct _Unwind_Context *context ATTRIBUTE_UNUSED)
+{
+  return _URC_CONTINUE_UNWIND;
+}
+
+/* The C++ EH routines need to live in the C++ runtime.  These should
+   be pulled in via the unwinding tables when needed.  */
+extern _Unwind_Reason_Code __gnu_compact_pr2 (int,
+    _Unwind_Action, _Unwind_Exception_Class, struct _Unwind_Exception *,
+    struct _Unwind_Context *) TARGET_ATTRIBUTE_WEAK;
+extern _Unwind_Reason_Code __gnu_compact_pr3 (int,
+    _Unwind_Action, _Unwind_Exception_Class, struct _Unwind_Exception *,
+    struct _Unwind_Context *) TARGET_ATTRIBUTE_WEAK;
+
+static _Unwind_Reason_Code
+uw_frame_state_compact (struct _Unwind_Context *context,
+			_Unwind_FrameState *fs,
+			enum compact_entry_type entry_type,
+		       	struct compact_eh_bases *bases)
+{
+  const unsigned char *p;
+  unsigned int pr_index;
+  _Unwind_Ptr personality;
+  unsigned char buf[4];
+  _Unwind_Reason_Code rc;
+
+  p = bases->entry;
+  pr_index = *(p++);
+  switch (pr_index) {
+  case 0:
+      p = read_encoded_value (context, bases->eh_encoding, p, &personality);
+      fs->personality = (_Unwind_Personality_Fn)personality;
+      break;
+  case 1:
+      fs->personality = __gnu_compact_pr1;
+      break;
+  case 2:
+      fs->personality = __gnu_compact_pr2;
+      break;
+  case 3:
+      fs->personality = __gnu_compact_pr3;
+      break;
+  default:
+      fs->personality = NULL;
+  }
+
+  if (!fs->personality)
+    return _URC_FATAL_PHASE1_ERROR;
+
+  if (entry_type == CET_inline)
+    {
+      memcpy(buf, p, 3);
+      buf[3] = 0x5c;
+      p = buf;
+    }
+
+  rc = md_unwind_compact (context, fs, &p);
+  if (rc != _URC_NO_REASON)
+      return rc;
+
+  if (entry_type == CET_outline)
+    context->lsda = (void *)(_Unwind_Internal_Ptr)p;
+
+  return _URC_NO_REASON;
+}
+#endif
+
 /* Given the _Unwind_Context CONTEXT for a stack frame, look up the FDE for
    its caller and decode it into FS.  This function also sets the
    args_size and lsda members of CONTEXT, as they are really information
@@ -1134,8 +1213,27 @@ uw_frame_state_for (struct _Unwind_Context *context, _Unwind_FrameState *fs)
   if (context->ra == 0)
     return _URC_END_OF_STACK;
 
+#ifdef MD_HAVE_COMPACT_EH
+    {
+      struct compact_eh_bases bases;
+      enum compact_entry_type type;
+      type = _Unwind_Find_Index (context->ra + _Unwind_IsSignalFrame (context)
+				 - 1, &bases);
+      context->bases.tbase = bases.tbase;
+      context->bases.dbase = bases.dbase;
+      context->bases.func = bases.func;
+      context->bases.eh_encoding = bases.eh_encoding;
+      if (type == CET_inline || type == CET_outline)
+	return uw_frame_state_compact (context, fs, type, &bases);
+      if (type == CET_FDE)
+	fde = bases.entry;
+      else
+	fde = NULL;
+    }
+#else
   fde = _Unwind_Find_FDE (context->ra + _Unwind_IsSignalFrame (context) - 1,
 			  &context->bases);
+#endif
   if (fde == NULL)
     {
 #ifdef MD_FALLBACK_FRAME_STATE_FOR
@@ -1423,16 +1521,12 @@ uw_advance_context (struct _Unwind_Context *context, _Unwind_FrameState *fs)
 /* Fill in CONTEXT for top-of-stack.  The only valid registers at this
    level will be the return address and the CFA.  */
 
-#define uw_init_context(CONTEXT)					   \
-  do									   \
-    {									   \
-      /* Do any necessary initialization to access arbitrary stack frames. \
-	 On the SPARC, this means flushing the register windows.  */	   \
-      __builtin_unwind_init ();						   \
-      uw_init_context_1 (CONTEXT, __builtin_dwarf_cfa (),		   \
-			 __builtin_return_address (0));			   \
-    }									   \
-  while (0)
+#define uw_init_context(CONTEXT)					\
+  /* Do any necessary initialization to access arbitrary stack frames.	\
+     On the SPARC, this means flushing the register windows.  */	\
+  (__builtin_unwind_init (),						\
+   uw_init_context_1 ((CONTEXT), __builtin_dwarf_cfa (),		\
+		      __builtin_return_address (0)))
 
 static inline void
 init_dwarf_reg_size_table (void)
@@ -1440,7 +1534,7 @@ init_dwarf_reg_size_table (void)
   __builtin_init_dwarf_reg_size_table (dwarf_reg_size_table);
 }
 
-static void __attribute__((noinline))
+static _Unwind_Reason_Code __attribute__((noinline))
 uw_init_context_1 (struct _Unwind_Context *context,
 		   void *outer_cfa, void *outer_ra)
 {
@@ -1454,7 +1548,8 @@ uw_init_context_1 (struct _Unwind_Context *context,
   context->flags = EXTENDED_CONTEXT_BIT;
 
   code = uw_frame_state_for (context, &fs);
-  gcc_assert (code == _URC_NO_REASON);
+  if (code != _URC_NO_REASON)
+    return code;
 
 #if __GTHREADS
   {
@@ -1480,6 +1575,8 @@ uw_init_context_1 (struct _Unwind_Context *context,
      initialization context, then we can't see it in the given
      call frame data.  So have the initialization context tell us.  */
   context->ra = __builtin_extract_return_addr (outer_ra);
+
+  return _URC_NO_REASON;
 }
 
 static void _Unwind_DebugHook (void *, void *)
@@ -1598,6 +1695,7 @@ alias (_Unwind_Resume);
 alias (_Unwind_Resume_or_Rethrow);
 alias (_Unwind_SetGR);
 alias (_Unwind_SetIP);
+alias (_Unwind_GetEhEncoding);
 #endif
 
 #endif /* !USING_SJLJ_EXCEPTIONS */
diff --git a/gcc/unwind-generic.h b/gcc/unwind-generic.h
index 4ff9017..667ffb9 100644
--- a/gcc/unwind-generic.h
+++ b/gcc/unwind-generic.h
@@ -164,6 +164,8 @@ extern _Unwind_Ptr _Unwind_GetIP (struct _Unwind_Context *);
 extern _Unwind_Ptr _Unwind_GetIPInfo (struct _Unwind_Context *, int *);
 extern void _Unwind_SetIP (struct _Unwind_Context *, _Unwind_Ptr);
 
+extern unsigned char _Unwind_GetEhEncoding (struct _Unwind_Context *);
+
 /* @@@ Retrieve the CFA of the given context.  */
 extern _Unwind_Word _Unwind_GetCFA (struct _Unwind_Context *);
 
diff --git a/gcc/unwind-sjlj.c b/gcc/unwind-sjlj.c
index c71e798..4ef81a6 100644
--- a/gcc/unwind-sjlj.c
+++ b/gcc/unwind-sjlj.c
@@ -292,10 +292,11 @@ uw_advance_context (struct _Unwind_Context *context, _Unwind_FrameState *fs)
   uw_update_context (context, fs);
 }
 
-static inline void
+static inline _Unwind_Reason_Code
 uw_init_context (struct _Unwind_Context *context)
 {
   context->fc = _Unwind_SjLj_GetContext ();
+  return _URC_NO_REASON;
 }
 
 static void __attribute__((noreturn))
diff --git a/gcc/unwind.inc b/gcc/unwind.inc
index 5e2ec29..562f6b1 100644
--- a/gcc/unwind.inc
+++ b/gcc/unwind.inc
@@ -85,7 +85,8 @@ _Unwind_RaiseException(struct _Unwind_Exception *exc)
   _Unwind_Reason_Code code;
 
   /* Set up this_context to describe the current stack frame.  */
-  uw_init_context (&this_context);
+  code = uw_init_context (&this_context);
+  gcc_assert (code == _URC_NO_REASON);
   cur_context = this_context;
 
   /* Phase 1: Search.  Unwind the stack, calling the personality routine
@@ -198,7 +199,8 @@ _Unwind_ForcedUnwind (struct _Unwind_Exception *exc,
   struct _Unwind_Context this_context, cur_context;
   _Unwind_Reason_Code code;
 
-  uw_init_context (&this_context);
+  code = uw_init_context (&this_context);
+  gcc_assert (code == _URC_NO_REASON);
   cur_context = this_context;
 
   exc->private_1 = (_Unwind_Ptr) stop;
@@ -221,7 +223,8 @@ _Unwind_Resume (struct _Unwind_Exception *exc)
   struct _Unwind_Context this_context, cur_context;
   _Unwind_Reason_Code code;
 
-  uw_init_context (&this_context);
+  code = uw_init_context (&this_context);
+  gcc_assert (code == _URC_NO_REASON);
   cur_context = this_context;
 
   /* Choose between continuing to process _Unwind_RaiseException
@@ -251,7 +254,8 @@ _Unwind_Resume_or_Rethrow (struct _Unwind_Exception *exc)
   if (exc->private_1 == 0)
     return _Unwind_RaiseException (exc);
 
-  uw_init_context (&this_context);
+  code = uw_init_context (&this_context);
+  gcc_assert (code == _URC_NO_REASON);
   cur_context = this_context;
 
   code = _Unwind_ForcedUnwind_Phase2 (exc, &cur_context);
@@ -280,7 +284,9 @@ _Unwind_Backtrace(_Unwind_Trace_Fn trace, void * trace_argument)
   struct _Unwind_Context context;
   _Unwind_Reason_Code code;
 
-  uw_init_context (&context);
+  code = uw_init_context (&context);
+  if (code != _URC_NO_REASON)
+    return _URC_FATAL_PHASE1_ERROR;
 
   while (1)
     {
diff --git a/gcc/varasm.c b/gcc/varasm.c
index 977ca40..0e92027 100644
--- a/gcc/varasm.c
+++ b/gcc/varasm.c
@@ -989,11 +989,14 @@ align_variable (tree decl, bool dont_output_data)
     {
 #ifdef DATA_ALIGNMENT
       unsigned int data_align = DATA_ALIGNMENT (TREE_TYPE (decl), align);
+#else
+      unsigned int data_align = align;
+#endif
+      data_align = alignment_for_aligned_arrays (TREE_TYPE (decl), data_align);
       /* Don't increase alignment too much for TLS variables - TLS space
 	 is too precious.  */
       if (! DECL_THREAD_LOCAL_P (decl) || data_align <= BITS_PER_WORD)
 	align = data_align;
-#endif
 #ifdef CONSTANT_ALIGNMENT
       if (DECL_INITIAL (decl) != 0 && DECL_INITIAL (decl) != error_mark_node)
 	{
diff --git a/libgcc/Makefile.in b/libgcc/Makefile.in
index b57aeb6..52ce1c5 100644
--- a/libgcc/Makefile.in
+++ b/libgcc/Makefile.in
@@ -250,6 +250,11 @@ gcc_s_compile = $(gcc_compile) -DSHARED
 objects = $(filter %$(objext),$^)
 
 # Collect any host-specific information from Makefile fragments.
+
+LIBGCC_VER_GNU_PREFIX = __
+LIBGCC_VER_FIXEDPOINT_GNU_PREFIX = __
+LIBGCC_VER_SYMBOLS_PREFIX =
+
 tmake_file = @tmake_file@
 include $(srcdir)/empty.mk $(tmake_file)
 
@@ -407,18 +412,24 @@ libgcc-s-objects += $(patsubst %,%_s$(objext),$(sifuncs) $(difuncs) $(tifuncs))
 endif
 endif
 
+ifeq ($(LIB2_DIVMOD_EXCEPTION_FLAGS),)
+# Provide default flags for compiling divmod functions, if they haven't been
+# set already by a target-specific Makefile fragment.
+LIB2_DIVMOD_EXCEPTION_FLAGS := -fexceptions -fnon-call-exceptions
+endif
+
 # Build LIB2_DIVMOD_FUNCS.
 lib2-divmod-o = $(patsubst %,%$(objext),$(LIB2_DIVMOD_FUNCS))
 $(lib2-divmod-o): %$(objext): $(gcc_srcdir)/libgcc2.c
 	$(gcc_compile) -DL$* -c $(gcc_srcdir)/libgcc2.c \
-	  -fexceptions -fnon-call-exceptions $(vis_hide)
+	  $(LIB2_DIVMOD_EXCEPTION_FLAGS) $(vis_hide)
 libgcc-objects += $(lib2-divmod-o)
 
 ifeq ($(enable_shared),yes)
 lib2-divmod-s-o = $(patsubst %,%_s$(objext),$(LIB2_DIVMOD_FUNCS))
 $(lib2-divmod-s-o): %_s$(objext): $(gcc_srcdir)/libgcc2.c
 	$(gcc_s_compile) -DL$* -c $(gcc_srcdir)/libgcc2.c \
-	  -fexceptions -fnon-call-exceptions
+	  $(LIB2_DIVMOD_EXCEPTION_FLAGS)
 libgcc-s-objects += $(lib2-divmod-s-o)
 endif
 
@@ -795,6 +806,11 @@ libgcc_s$(SHLIB_EXT): libgcc.map
 mapfile = libgcc.map
 endif
 
+libgcc-std.ver: $(srcdir)/libgcc-std.ver.in
+	sed -e 's/__PFX__/$(LIBGCC_VER_GNU_PREFIX)/g' \
+	    -e 's/__FIXPTPFX__/$(LIBGCC_VER_FIXEDPOINT_GNU_PREFIX)/g' < $< > $@
+
+
 libgcc_s$(SHLIB_EXT): $(libgcc-s-objects) $(extra-parts) libgcc.a
 	# @multilib_flags@ is still needed because this may use
 	# $(GCC_FOR_TARGET) and $(LIBGCC2_CFLAGS) directly.
diff --git a/libgcc/config.host b/libgcc/config.host
index 25e949e..169e5f0 100644
--- a/libgcc/config.host
+++ b/libgcc/config.host
@@ -202,12 +202,15 @@ arm*-*-netbsdelf*)
 	;;
 arm*-*-ecos-elf)
 	;;
 arm*-*-eabi* | arm*-*-symbianelf* )
+	tmake_file="${tmake_file} arm/t-divmod-ef t-fixedpoint-gnu-prefix"
 	;;
 arm*-*-rtems*)
 	;;
diff --git a/libgcc/config/arm/bpabi-lib.h b/libgcc/config/arm/bpabi-lib.h
index fc0e595..ebb1775 100644
--- a/libgcc/config/arm/bpabi-lib.h
+++ b/libgcc/config/arm/bpabi-lib.h
@@ -71,3 +71,8 @@
 #ifdef L_floatundisf
 #define DECLARE_LIBRARY_RENAMES RENAME_LIBRARY (floatundisf, ul2f)
 #endif
+
+/* For ARM bpabi, we only want to use a "__gnu_" prefix for the fixed-point
+   helper functions - not everything in libgcc - in the interests of
+   maintaining backward compatibility.  */
+#define LIBGCC2_FIXEDBIT_GNU_PREFIX
diff --git a/libgcc/config/s390/t-linux b/libgcc/config/s390/t-linux
index a0f10cb..b767082 100644
--- a/libgcc/config/s390/t-linux
+++ b/libgcc/config/s390/t-linux
@@ -2,6 +2,6 @@ DFP_ENABLE = true
 
 # Override t-slibgcc-elf-ver to export some libgcc symbols with
 # the symbol versions that glibc used.
-SHLIB_MAPFILES = $(gcc_srcdir)/libgcc-std.ver $(srcdir)/config/s390/libgcc-glibc.ver
+SHLIB_MAPFILES = libgcc-std.ver $(srcdir)/config/s390/libgcc-glibc.ver
 
 HOST_LIBGCC2_CFLAGS += -mlong-double-128
\ No newline at end of file
diff --git a/libgcc/shared-object.mk b/libgcc/shared-object.mk
index 65171b6..ea49dc2 100644
--- a/libgcc/shared-object.mk
+++ b/libgcc/shared-object.mk
@@ -8,11 +8,13 @@ base := $(basename $(notdir $o))
 
 ifeq ($(suffix $o),.c)
 
+c_flags-$(base)$(objext) := $(c_flags)
 $(base)$(objext): $o
-	$(gcc_compile) $(c_flags) -c $< $(vis_hide)
+	$(gcc_compile) $(c_flags-$@) -c $< $(vis_hide)
 
+c_flags-$(base)_s$(objext) := $(c_flags)
 $(base)_s$(objext): $o
-	$(gcc_s_compile) $(c_flags) -c $<
+	$(gcc_s_compile) $(c_flags-$@) -c $<
 
 else
 
diff --git a/libgcc/static-object.mk b/libgcc/static-object.mk
index ab75d32..3a168c4 100644
--- a/libgcc/static-object.mk
+++ b/libgcc/static-object.mk
@@ -8,8 +8,9 @@ base := $(basename $(notdir $o))
 
 ifeq ($(suffix $o),.c)
 
+c_flags-$(base)$(objext) := $(c_flags)
 $(base)$(objext): $o
-	$(gcc_compile) $(c_flags) -c $< $(vis_hide)
+	$(gcc_compile) $(c_flags-$@) -c $< $(vis_hide)
 
 else
 
diff --git a/libjava/Makefile.am b/libjava/Makefile.am
index 52d0995..55e06e2 100644
--- a/libjava/Makefile.am
+++ b/libjava/Makefile.am
@@ -221,9 +221,14 @@ endif
 
 dbexec_LTLIBRARIES = libjvm.la
 
-pkgconfigdir = $(libdir)/pkgconfig
+# Install the pkgconfig file in a target-specific directory, since the
+# libraries it indicates
 
-jardir = $(datadir)/java
+pkgconfigdir = $(toolexeclibdir)/pkgconfig
+
+# We install the JAR in a target-specific directory so that toolchains
+# build from different sources can be installed in the same directory.
+jardir = $(prefix)/$(target_noncanonical)/share/java
 jar_DATA = libgcj-$(gcc_version).jar libgcj-tools-$(gcc_version).jar
 if INSTALL_ECJ_JAR
 jar_DATA += $(ECJ_BUILD_JAR)
diff --git a/libjava/Makefile.in b/libjava/Makefile.in
index bf5ab9d..6a73245 100644
--- a/libjava/Makefile.in
+++ b/libjava/Makefile.in
@@ -937,7 +937,10 @@ CORE_PACKAGE_SOURCE_FILES_LO = $(filter-out $(LOWER_PACKAGE_FILES_LO),$(ALL_PACK
 @BUILD_SUBLIBS_TRUE@LIBJAVA_CORE_EXTRA = @LIBGCJ_SUBLIB_CORE_EXTRA_DEPS@
 dbexec_LTLIBRARIES = libjvm.la
 pkgconfigdir = $(libdir)/pkgconfig
-jardir = $(datadir)/java
+
+# We install the JAR in a target-specific directory so that toolchains
+# build from different sources can be installed in the same directory.
+jardir = $(prefix)/$(target_noncanonical)/share/java
 jar_DATA = libgcj-$(gcc_version).jar libgcj-tools-$(gcc_version).jar \
 	$(am__append_5)
 @JAVA_HOME_SET_FALSE@JAVA_HOME_DIR = $(prefix)
diff --git a/libjava/classpath/Makefile.in b/libjava/classpath/Makefile.in
index ad20cf6..2877d9a 100644
--- a/libjava/classpath/Makefile.in
+++ b/libjava/classpath/Makefile.in
@@ -303,9 +303,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/configure b/libjava/classpath/configure
index a25f5f7..4ef830f 100755
--- a/libjava/classpath/configure
+++ b/libjava/classpath/configure
@@ -745,6 +745,9 @@ CREATE_JNI_HEADERS_TRUE
 glibjdir
 nativeexeclibdir
 toolexeclibdir
+toolexecmainlibdir
+toolexecdir
+target_noncanonical
 CREATE_GJDOC_FALSE
 CREATE_GJDOC_TRUE
 CREATE_PLUGIN_FALSE
@@ -898,6 +901,7 @@ enable_qt_peer
 enable_plugin
 enable_gmp
 enable_gjdoc
+enable_version_specific_runtime_libs
 with_native_libdir
 with_glibj_dir
 with_antlr_jar
@@ -1615,6 +1619,9 @@ Optional Features:
                           (disabled by --disable-gmp) default=yes
   --disable-gjdoc         compile GJDoc (disabled by --disable-gjdoc)
                           default=yes
+  --enable-version-specific-runtime-libs
+                          specify that runtime libraries should be installed
+                          in a compiler-specific directory
   --enable-regen-headers  automatically regenerate JNI headers default=yes if
                           headers don't exist
   --enable-regen-gjdoc-parser
@@ -5519,12 +5526,58 @@ else
 fi
 
 
+# Check whether --enable-version-specific-runtime-libs was given.
+if test "${enable_version_specific_runtime_libs+set}" = set; then :
+  enableval=$enable_version_specific_runtime_libs; case "$enableval" in
+      yes) version_specific_libs=yes ;;
+      no)  version_specific_libs=no ;;
+      *)   as_fn_error "Unknown argument to enable/disable version-specific libs" "$LINENO" 5;;
+     esac
+else
+  version_specific_libs=no
+
+fi
+
 
-  multi_os_directory=`$CC -print-multi-os-directory`
-  case $multi_os_directory in
-    .) toolexeclibdir=${libdir} ;; # Avoid trailing /.
-    *) toolexeclibdir=${libdir}/${multi_os_directory} ;;
+  case ${host_alias} in
+    "") host_noncanonical=${build_noncanonical} ;;
+    *) host_noncanonical=${host_alias} ;;
   esac
+  case ${target_alias} in
+    "") target_noncanonical=${host_noncanonical} ;;
+    *) target_noncanonical=${target_alias} ;;
+  esac
+
+
+  case ${version_specific_libs} in
+    yes)
+      # Need the gcc compiler version to know where to install libraries
+      # and header files if --enable-version-specific-runtime-libs option
+      # is selected.
+      includedir='$(libdir)/gcc/$(target_noncanonical)/$(gcc_version)/include/'
+      toolexecdir='$(libdir)/gcc/$(target_noncanonical)'
+      toolexecmainlibdir='$(toolexecdir)/$(gcc_version)$(MULTISUBDIR)'
+      toolexeclibdir=$toolexecmainlibdir
+      ;;
+    no)
+      if test -n "$with_cross_host" &&
+	 test x"$with_cross_host" != x"no"; then
+	# Install a library built with a cross compiler in tooldir, not libdir.
+	toolexecdir='$(exec_prefix)/$(target_noncanonical)'
+	toolexecmainlibdir='$(toolexecdir)/lib'
+      else
+	toolexecdir='$(libdir)/gcc-lib/$(target_noncanonical)'
+	toolexecmainlibdir='$(libdir)'
+      fi
+      multi_os_directory=`$CC -print-multi-os-directory`
+      case $multi_os_directory in
+	.) toolexeclibdir=$toolexecmainlibdir ;; # Avoid trailing /.
+	*) toolexeclibdir=$toolexecmainlibdir/$multi_os_directory ;;
+      esac
+      ;;
+  esac
+
+
 
 
 
@@ -11811,7 +11864,7 @@ else
   lt_dlunknown=0; lt_dlno_uscore=1; lt_dlneed_uscore=2
   lt_status=$lt_dlunknown
   cat > conftest.$ac_ext <<_LT_EOF
-#line 11814 "configure"
+#line 11867 "configure"
 #include "confdefs.h"
 
 #if HAVE_DLFCN_H
@@ -11917,7 +11970,7 @@ else
   lt_dlunknown=0; lt_dlno_uscore=1; lt_dlneed_uscore=2
   lt_status=$lt_dlunknown
   cat > conftest.$ac_ext <<_LT_EOF
-#line 11920 "configure"
+#line 11973 "configure"
 #include "confdefs.h"
 
 #if HAVE_DLFCN_H
@@ -23814,7 +23867,7 @@ else
 JAVA_TEST=Object.java
 CLASS_TEST=Object.class
 cat << \EOF > $JAVA_TEST
-/* #line 23817 "configure" */
+/* #line 23870 "configure" */
 package java.lang;
 
 public class Object
@@ -23907,7 +23960,7 @@ EOF
 if uudecode$EXEEXT Test.uue; then
         ac_cv_prog_uudecode_base64=yes
 else
-        echo "configure: 23910: uudecode had trouble decoding base 64 file 'Test.uue'" >&5
+        echo "configure: 23963: uudecode had trouble decoding base 64 file 'Test.uue'" >&5
         echo "configure: failed file was:" >&5
         cat Test.uue >&5
         ac_cv_prog_uudecode_base64=no
@@ -23935,7 +23988,7 @@ JAVA_TEST=Test.java
 CLASS_TEST=Test.class
 TEST=Test
 cat << \EOF > $JAVA_TEST
-/* [#]line 23938 "configure" */
+/* [#]line 23991 "configure" */
 public class Test {
 public static void main (String args[]) {
         System.exit (0);
@@ -24143,7 +24196,7 @@ if test "x${use_glibj_zip}" = xfalse || \
   JAVA_TEST=Test.java
   CLASS_TEST=Test.class
   cat << \EOF > $JAVA_TEST
-  /* #line 24146 "configure" */
+  /* #line 24199 "configure" */
   public class Test
   {
     public static void main(String args)
diff --git a/libjava/classpath/configure.ac b/libjava/classpath/configure.ac
index 49bd667..6bf71d8 100644
--- a/libjava/classpath/configure.ac
+++ b/libjava/classpath/configure.ac
@@ -317,6 +317,16 @@ dnl defined to the same value for all multilibs.  We define toolexeclibdir
 dnl so that we can refer to the multilib installation directories from
 dnl classpath's build files.
 dnl -----------------------------------------------------------
+AC_ARG_ENABLE(version-specific-runtime-libs,
+  AS_HELP_STRING([--enable-version-specific-runtime-libs],    
+                 [specify that runtime libraries should be installed in a compiler-specific directory]),
+    [case "$enableval" in
+      yes) version_specific_libs=yes ;;
+      no)  version_specific_libs=no ;;
+      *)   AC_MSG_ERROR([Unknown argument to enable/disable version-specific libs]);;
+     esac],
+    [version_specific_libs=no]
+)
 CLASSPATH_TOOLEXECLIBDIR
 
 dnl -----------------------------------------------------------
diff --git a/libjava/classpath/doc/Makefile.in b/libjava/classpath/doc/Makefile.in
index 781f601..d44283b 100644
--- a/libjava/classpath/doc/Makefile.in
+++ b/libjava/classpath/doc/Makefile.in
@@ -321,9 +321,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/doc/api/Makefile.in b/libjava/classpath/doc/api/Makefile.in
index b5c7bec..d6c555e 100644
--- a/libjava/classpath/doc/api/Makefile.in
+++ b/libjava/classpath/doc/api/Makefile.in
@@ -272,9 +272,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/examples/Makefile.in b/libjava/classpath/examples/Makefile.in
index be2e2ce..7634c71 100644
--- a/libjava/classpath/examples/Makefile.in
+++ b/libjava/classpath/examples/Makefile.in
@@ -295,9 +295,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/external/Makefile.in b/libjava/classpath/external/Makefile.in
index 355e685..d2030cf 100644
--- a/libjava/classpath/external/Makefile.in
+++ b/libjava/classpath/external/Makefile.in
@@ -284,9 +284,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/external/jsr166/Makefile.in b/libjava/classpath/external/jsr166/Makefile.in
index 7a85895..b4efe55 100644
--- a/libjava/classpath/external/jsr166/Makefile.in
+++ b/libjava/classpath/external/jsr166/Makefile.in
@@ -270,9 +270,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/external/relaxngDatatype/Makefile.in b/libjava/classpath/external/relaxngDatatype/Makefile.in
index f909137..6c950cf 100644
--- a/libjava/classpath/external/relaxngDatatype/Makefile.in
+++ b/libjava/classpath/external/relaxngDatatype/Makefile.in
@@ -270,9 +270,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/external/sax/Makefile.in b/libjava/classpath/external/sax/Makefile.in
index a4a0595..b9d8323 100644
--- a/libjava/classpath/external/sax/Makefile.in
+++ b/libjava/classpath/external/sax/Makefile.in
@@ -270,9 +270,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/external/w3c_dom/Makefile.in b/libjava/classpath/external/w3c_dom/Makefile.in
index 68eecef..79b1bfd 100644
--- a/libjava/classpath/external/w3c_dom/Makefile.in
+++ b/libjava/classpath/external/w3c_dom/Makefile.in
@@ -270,9 +270,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/include/Makefile.in b/libjava/classpath/include/Makefile.in
index 58582e8..76796f0 100644
--- a/libjava/classpath/include/Makefile.in
+++ b/libjava/classpath/include/Makefile.in
@@ -271,9 +271,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/lib/Makefile.in b/libjava/classpath/lib/Makefile.in
index eba0729..833e854 100644
--- a/libjava/classpath/lib/Makefile.in
+++ b/libjava/classpath/lib/Makefile.in
@@ -275,9 +275,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/m4/acinclude.m4 b/libjava/classpath/m4/acinclude.m4
index 65cb8fc..7dbf324 100644
--- a/libjava/classpath/m4/acinclude.m4
+++ b/libjava/classpath/m4/acinclude.m4
@@ -247,11 +247,45 @@ dnl GCJ LOCAL: Calculate toolexeclibdir
 dnl -----------------------------------------------------------
 AC_DEFUN([CLASSPATH_TOOLEXECLIBDIR],
 [
-  multi_os_directory=`$CC -print-multi-os-directory`
-  case $multi_os_directory in
-    .) toolexeclibdir=${libdir} ;; # Avoid trailing /.
-    *) toolexeclibdir=${libdir}/${multi_os_directory} ;;
+  case ${host_alias} in
+    "") host_noncanonical=${build_noncanonical} ;;
+    *) host_noncanonical=${host_alias} ;;
   esac
+  case ${target_alias} in
+    "") target_noncanonical=${host_noncanonical} ;;
+    *) target_noncanonical=${target_alias} ;;
+  esac
+  AC_SUBST(target_noncanonical)
+
+  case ${version_specific_libs} in
+    yes)
+      # Need the gcc compiler version to know where to install libraries
+      # and header files if --enable-version-specific-runtime-libs option
+      # is selected.
+      includedir='$(libdir)/gcc/$(target_noncanonical)/$(gcc_version)/include/'
+      toolexecdir='$(libdir)/gcc/$(target_noncanonical)'
+      toolexecmainlibdir='$(toolexecdir)/$(gcc_version)$(MULTISUBDIR)'
+      toolexeclibdir=$toolexecmainlibdir
+      ;;
+    no)
+      if test -n "$with_cross_host" &&
+	 test x"$with_cross_host" != x"no"; then
+	# Install a library built with a cross compiler in tooldir, not libdir.
+	toolexecdir='$(exec_prefix)/$(target_noncanonical)'
+	toolexecmainlibdir='$(toolexecdir)/lib'
+      else
+	toolexecdir='$(libdir)/gcc-lib/$(target_noncanonical)'
+	toolexecmainlibdir='$(libdir)'
+      fi
+      multi_os_directory=`$CC -print-multi-os-directory`
+      case $multi_os_directory in
+	.) toolexeclibdir=$toolexecmainlibdir ;; # Avoid trailing /.
+	*) toolexeclibdir=$toolexecmainlibdir/$multi_os_directory ;;
+      esac
+      ;;
+  esac
+  AC_SUBST(toolexecdir)
+  AC_SUBST(toolexecmainlibdir)
   AC_SUBST(toolexeclibdir)
 ])
 
diff --git a/libjava/classpath/native/Makefile.in b/libjava/classpath/native/Makefile.in
index 4d6e581..94b6e9e 100644
--- a/libjava/classpath/native/Makefile.in
+++ b/libjava/classpath/native/Makefile.in
@@ -283,9 +283,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/fdlibm/Makefile.in b/libjava/classpath/native/fdlibm/Makefile.in
index ff34451..5c53030 100644
--- a/libjava/classpath/native/fdlibm/Makefile.in
+++ b/libjava/classpath/native/fdlibm/Makefile.in
@@ -299,9 +299,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jawt/Makefile.in b/libjava/classpath/native/jawt/Makefile.in
index 6ca1639..82f64d9 100644
--- a/libjava/classpath/native/jawt/Makefile.in
+++ b/libjava/classpath/native/jawt/Makefile.in
@@ -316,9 +316,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/Makefile.in b/libjava/classpath/native/jni/Makefile.in
index db80131..07573ab 100644
--- a/libjava/classpath/native/jni/Makefile.in
+++ b/libjava/classpath/native/jni/Makefile.in
@@ -283,9 +283,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/classpath/Makefile.in b/libjava/classpath/native/jni/classpath/Makefile.in
index 8067936..c0ad647 100644
--- a/libjava/classpath/native/jni/classpath/Makefile.in
+++ b/libjava/classpath/native/jni/classpath/Makefile.in
@@ -290,9 +290,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/gconf-peer/Makefile.in b/libjava/classpath/native/jni/gconf-peer/Makefile.in
index 691afd7..2662b60 100644
--- a/libjava/classpath/native/jni/gconf-peer/Makefile.in
+++ b/libjava/classpath/native/jni/gconf-peer/Makefile.in
@@ -316,9 +316,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/gstreamer-peer/Makefile.in b/libjava/classpath/native/jni/gstreamer-peer/Makefile.in
index 4985ff6..1d2aada 100644
--- a/libjava/classpath/native/jni/gstreamer-peer/Makefile.in
+++ b/libjava/classpath/native/jni/gstreamer-peer/Makefile.in
@@ -318,9 +318,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/gtk-peer/Makefile.in b/libjava/classpath/native/jni/gtk-peer/Makefile.in
index 3fc2933..d28f139 100644
--- a/libjava/classpath/native/jni/gtk-peer/Makefile.in
+++ b/libjava/classpath/native/jni/gtk-peer/Makefile.in
@@ -354,9 +354,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/java-io/Makefile.in b/libjava/classpath/native/jni/java-io/Makefile.in
index beb9ce6..da6c7c2 100644
--- a/libjava/classpath/native/jni/java-io/Makefile.in
+++ b/libjava/classpath/native/jni/java-io/Makefile.in
@@ -315,9 +315,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/java-lang/Makefile.in b/libjava/classpath/native/jni/java-lang/Makefile.in
index 148ba7f..340d3c5 100644
--- a/libjava/classpath/native/jni/java-lang/Makefile.in
+++ b/libjava/classpath/native/jni/java-lang/Makefile.in
@@ -327,9 +327,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/java-math/Makefile.in b/libjava/classpath/native/jni/java-math/Makefile.in
index 3b70725..9fab7c2 100644
--- a/libjava/classpath/native/jni/java-math/Makefile.in
+++ b/libjava/classpath/native/jni/java-math/Makefile.in
@@ -316,9 +316,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/java-net/Makefile.in b/libjava/classpath/native/jni/java-net/Makefile.in
index ca3e163..64e5a9a 100644
--- a/libjava/classpath/native/jni/java-net/Makefile.in
+++ b/libjava/classpath/native/jni/java-net/Makefile.in
@@ -321,9 +321,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/java-nio/Makefile.in b/libjava/classpath/native/jni/java-nio/Makefile.in
index 427d072..7d9ffb0 100644
--- a/libjava/classpath/native/jni/java-nio/Makefile.in
+++ b/libjava/classpath/native/jni/java-nio/Makefile.in
@@ -323,9 +323,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/java-util/Makefile.in b/libjava/classpath/native/jni/java-util/Makefile.in
index 7d06551..fb70f77 100644
--- a/libjava/classpath/native/jni/java-util/Makefile.in
+++ b/libjava/classpath/native/jni/java-util/Makefile.in
@@ -312,9 +312,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/midi-alsa/Makefile.in b/libjava/classpath/native/jni/midi-alsa/Makefile.in
index eb46acb..e0d1913 100644
--- a/libjava/classpath/native/jni/midi-alsa/Makefile.in
+++ b/libjava/classpath/native/jni/midi-alsa/Makefile.in
@@ -318,9 +318,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/midi-dssi/Makefile.in b/libjava/classpath/native/jni/midi-dssi/Makefile.in
index 7eed5df..f266ba8 100644
--- a/libjava/classpath/native/jni/midi-dssi/Makefile.in
+++ b/libjava/classpath/native/jni/midi-dssi/Makefile.in
@@ -318,9 +318,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/native-lib/Makefile.in b/libjava/classpath/native/jni/native-lib/Makefile.in
index 4f58ded..de1dca3 100644
--- a/libjava/classpath/native/jni/native-lib/Makefile.in
+++ b/libjava/classpath/native/jni/native-lib/Makefile.in
@@ -290,9 +290,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/qt-peer/Makefile.in b/libjava/classpath/native/jni/qt-peer/Makefile.in
index d65474e..53fe9cc 100644
--- a/libjava/classpath/native/jni/qt-peer/Makefile.in
+++ b/libjava/classpath/native/jni/qt-peer/Makefile.in
@@ -320,9 +320,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/jni/xmlj/Makefile.in b/libjava/classpath/native/jni/xmlj/Makefile.in
index 2fe972a..e3f81da 100644
--- a/libjava/classpath/native/jni/xmlj/Makefile.in
+++ b/libjava/classpath/native/jni/xmlj/Makefile.in
@@ -314,9 +314,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/native/plugin/Makefile.in b/libjava/classpath/native/plugin/Makefile.in
index 798bc6d..4466eae 100644
--- a/libjava/classpath/native/plugin/Makefile.in
+++ b/libjava/classpath/native/plugin/Makefile.in
@@ -316,9 +316,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/resource/Makefile.in b/libjava/classpath/resource/Makefile.in
index 822a935..61a1ffa 100644
--- a/libjava/classpath/resource/Makefile.in
+++ b/libjava/classpath/resource/Makefile.in
@@ -294,9 +294,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/scripts/Makefile.in b/libjava/classpath/scripts/Makefile.in
index 4a0a339..859d3cf 100644
--- a/libjava/classpath/scripts/Makefile.in
+++ b/libjava/classpath/scripts/Makefile.in
@@ -271,9 +271,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/classpath/tools/Makefile.in b/libjava/classpath/tools/Makefile.in
index c8b0a63..2a0311a 100644
--- a/libjava/classpath/tools/Makefile.in
+++ b/libjava/classpath/tools/Makefile.in
@@ -427,9 +427,12 @@ sysconfdir = @sysconfdir@
 target = @target@
 target_alias = @target_alias@
 target_cpu = @target_cpu@
+target_noncanonical = @target_noncanonical@
 target_os = @target_os@
 target_vendor = @target_vendor@
+toolexecdir = @toolexecdir@
 toolexeclibdir = @toolexeclibdir@
+toolexecmainlibdir = @toolexecmainlibdir@
 top_build_prefix = @top_build_prefix@
 top_builddir = @top_builddir@
 top_srcdir = @top_srcdir@
diff --git a/libjava/configure b/libjava/configure
index d660561..47a0f89 100755
--- a/libjava/configure
+++ b/libjava/configure
@@ -23822,10 +23822,10 @@ gcjsubdir=gcj-$gcjversion-$libgcj_soversion
 multi_os_directory=`$CC -print-multi-os-directory`
 case $multi_os_directory in
   .)
-   dbexecdir='$(libdir)/'$gcjsubdir # Avoid /.
+   dbexecdir='$(toolexeclibdir)/'$gcjsubdir # Avoid /.
    ;;
   *)
-   dbexecdir='$(libdir)/'$multi_os_directory/$gcjsubdir
+   dbexecdir='$(toolexeclibdir)/'$multi_os_directory/$gcjsubdir
    ;;
 esac
 
diff --git a/libjava/configure.ac b/libjava/configure.ac
index 20dbe96..215d9ec 100644
--- a/libjava/configure.ac
+++ b/libjava/configure.ac
@@ -1602,10 +1602,10 @@ gcjsubdir=gcj-$gcjversion-$libgcj_soversion
 multi_os_directory=`$CC -print-multi-os-directory`
 case $multi_os_directory in
   .)
-   dbexecdir='$(libdir)/'$gcjsubdir # Avoid /.
+   dbexecdir='$(toolexeclibdir)/'$gcjsubdir # Avoid /.
    ;;
   *)
-   dbexecdir='$(libdir)/'$multi_os_directory/$gcjsubdir
+   dbexecdir='$(toolexeclibdir)/'$multi_os_directory/$gcjsubdir
    ;;
 esac
 AC_SUBST(dbexecdir)
diff --git a/libstdc++-v3/acinclude.m4 b/libstdc++-v3/acinclude.m4
index 788966c..785c71e 100644
--- a/libstdc++-v3/acinclude.m4
+++ b/libstdc++-v3/acinclude.m4
@@ -3265,6 +3265,23 @@ AC_DEFUN([GLIBCXX_GTHREADS_CXX11_COPY_ASSIGN], [
   AC_LANG_RESTORE
 ])
 
+# Check whether or not we are using CSLIBC.  If using CSLIBC, you
+# should configure with --with-cslibc *in addition to* --with-newlib.
+AC_DEFUN([GLIBCXX_WITH_CSLIBC], [
+  AC_ARG_WITH([cslibc],
+    AC_HELP_STRING([--with-cslibc],
+                   [assume CSLIBC as system C library]),,
+    [with_cslibc=no])
+  if test "$with_cslibc" = yes; then
+    cslibc_define=1
+  else
+    cslibc_define=0
+  fi
+  GLIBCXX_CONDITIONAL(GLIBCXX_CSLIBC, test $with_cslibc = yes)
+  AC_DEFINE_UNQUOTED(_GLIBCXX_CSLIBC, $cslibc_define,
+    [Define to 1 if building for use with CSLIBC, or 0 otherwise.])
+])
+
 # Macros from the top-level gcc directory.
 m4_include([../config/gc++filt.m4])
 m4_include([../config/tls.m4])
diff --git a/libstdc++-v3/config.h.in b/libstdc++-v3/config.h.in
index 2a8afaf..25cc9fc 100644
--- a/libstdc++-v3/config.h.in
+++ b/libstdc++-v3/config.h.in
@@ -689,6 +689,9 @@
 /* Define to use concept checking code from the boost libraries. */
 #undef _GLIBCXX_CONCEPT_CHECKS
 
+/* Define to 1 if building for use with CSLIBC, or 0 otherwise. */
+#undef _GLIBCXX_CSLIBC
+
 /* Define if a fully dynamic basic_string is wanted. */
 #undef _GLIBCXX_FULLY_DYNAMIC_STRING
 
diff --git a/libstdc++-v3/config/abi/pre/gnu-versioned-namespace.ver b/libstdc++-v3/config/abi/pre/gnu-versioned-namespace.ver
index 32bcf67..0bd8cc0 100644
--- a/libstdc++-v3/config/abi/pre/gnu-versioned-namespace.ver
+++ b/libstdc++-v3/config/abi/pre/gnu-versioned-namespace.ver
@@ -177,6 +177,8 @@ CXXABI_2.0 {
     __cxa_vec_new;
     __gxx_personality_v0;
     __gxx_personality_sj0;
+    __gnu_compact_pr2;
+    __gnu_compact_pr3;
     __dynamic_cast;
 
     # std::exception_ptr
diff --git a/libstdc++-v3/config/abi/pre/gnu.ver b/libstdc++-v3/config/abi/pre/gnu.ver
index 57744a4..b5a9871 100644
--- a/libstdc++-v3/config/abi/pre/gnu.ver
+++ b/libstdc++-v3/config/abi/pre/gnu.ver
@@ -1315,6 +1315,8 @@ CXXABI_1.3 {
     __cxa_vec_new;
     __gxx_personality_v0;
     __gxx_personality_sj0;
+    __gnu_compact_pr2;
+    __gnu_compact_pr3;
     __dynamic_cast;
 
     # *_type_info classes, ctor and dtor
diff --git a/libstdc++-v3/config/locale/gnu/c_locale.cc b/libstdc++-v3/config/locale/gnu/c_locale.cc
index adfeb62..6d0f774 100644
--- a/libstdc++-v3/config/locale/gnu/c_locale.cc
+++ b/libstdc++-v3/config/locale/gnu/c_locale.cc
@@ -212,3 +212,21 @@ _GLIBCXX_END_NAMESPACE_VERSION
   extern "C" void ldbl (void) __attribute__ ((alias (#dbl)))
 _GLIBCXX_LDBL_COMPAT(_ZSt14__convert_to_vIdEvPKcRT_RSt12_Ios_IostateRKP15__locale_struct, _ZSt14__convert_to_vIeEvPKcRT_RSt12_Ios_IostateRKP15__locale_struct);
 #endif // _GLIBCXX_LONG_DOUBLE_COMPAT
+
+/* Because of a bad cross-compilation fallback in a configure test,
+   Sourcery G++ toolchains for GNU/Linux targets formerly used the
+   "generic" locale model in libstdc++.  Improve compatibility with
+   those toolchains by exporting symbol aliases under the "generic"
+   names for the "gnu" functions.  */
+#define _GLIBCXX_LOCALE_COMPAT(generic, gnu) \
+  extern "C" void generic (void) __attribute__ ((alias (#gnu), weak))
+
+_GLIBCXX_LOCALE_COMPAT (_ZNSt6locale5facet17_S_clone_c_localeERPi, _ZNSt6locale5facet17_S_clone_c_localeERP15__locale_struct);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt6locale5facet18_S_create_c_localeERPiPKcS1_, _ZNSt6locale5facet18_S_create_c_localeERP15__locale_structPKcS2_);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt6locale5facet19_S_destroy_c_localeERPi, _ZNSt6locale5facet19_S_destroy_c_localeERP15__locale_struct);
+_GLIBCXX_LOCALE_COMPAT (_ZSt14__convert_to_vIdEvPKcRT_RSt12_Ios_IostateRKPi, _ZSt14__convert_to_vIdEvPKcRT_RSt12_Ios_IostateRKP15__locale_struct);
+_GLIBCXX_LOCALE_COMPAT (_ZSt14__convert_to_vIfEvPKcRT_RSt12_Ios_IostateRKPi, _ZSt14__convert_to_vIfEvPKcRT_RSt12_Ios_IostateRKP15__locale_struct);
+_GLIBCXX_LOCALE_COMPAT (_ZSt14__convert_to_vIeEvPKcRT_RSt12_Ios_IostateRKPi, _ZSt14__convert_to_vIeEvPKcRT_RSt12_Ios_IostateRKP15__locale_struct);
+#ifdef _GLIBCXX_LONG_DOUBLE_COMPAT
+_GLIBCXX_LOCALE_COMPAT (_ZSt14__convert_to_vIgEvPKcRT_RSt12_Ios_IostateRKPi, _ZSt14__convert_to_vIgEvPKcRT_RSt12_Ios_IostateRKP15__locale_struct);
+#endif
diff --git a/libstdc++-v3/config/locale/gnu/monetary_members.cc b/libstdc++-v3/config/locale/gnu/monetary_members.cc
index 214c4af..e8e7499 100644
--- a/libstdc++-v3/config/locale/gnu/monetary_members.cc
+++ b/libstdc++-v3/config/locale/gnu/monetary_members.cc
@@ -933,3 +933,18 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION
 
 _GLIBCXX_END_NAMESPACE_VERSION
 } // namespace
+
+/* Because of a bad cross-compilation fallback in a configure test,
+   Sourcery G++ toolchains for GNU/Linux targets formerly used the
+   "generic" locale model in libstdc++.  Improve compatibility with
+   those toolchains by exporting symbol aliases under the "generic"
+   names for the "gnu" functions.  */
+#define _GLIBCXX_LOCALE_COMPAT(generic, gnu) \
+  extern "C" void generic (void) __attribute__ ((alias (#gnu), weak))
+
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIcLb0EE24_M_initialize_moneypunctEPiPKc, _ZNSt10moneypunctIcLb0EE24_M_initialize_moneypunctEP15__locale_structPKc);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIcLb1EE24_M_initialize_moneypunctEPiPKc, _ZNSt10moneypunctIcLb1EE24_M_initialize_moneypunctEP15__locale_structPKc);
+#ifdef _GLIBCXX_USE_WCHAR_T
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIwLb0EE24_M_initialize_moneypunctEPiPKc, _ZNSt10moneypunctIwLb0EE24_M_initialize_moneypunctEP15__locale_structPKc);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIwLb1EE24_M_initialize_moneypunctEPiPKc, _ZNSt10moneypunctIwLb1EE24_M_initialize_moneypunctEP15__locale_structPKc);
+#endif
diff --git a/libstdc++-v3/config/locale/gnu/numeric_members.cc b/libstdc++-v3/config/locale/gnu/numeric_members.cc
index 934511a..f184919 100644
--- a/libstdc++-v3/config/locale/gnu/numeric_members.cc
+++ b/libstdc++-v3/config/locale/gnu/numeric_members.cc
@@ -216,3 +216,16 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION
 
 _GLIBCXX_END_NAMESPACE_VERSION
 } // namespace
+
+/* Because of a bad cross-compilation fallback in a configure test,
+   Sourcery G++ toolchains for GNU/Linux targets formerly used the
+   "generic" locale model in libstdc++.  Improve compatibility with
+   those toolchains by exporting symbol aliases under the "generic"
+   names for the "gnu" functions.  */
+#define _GLIBCXX_LOCALE_COMPAT(generic, gnu) \
+  extern "C" void generic (void) __attribute__ ((alias (#gnu), weak))
+
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8numpunctIcE22_M_initialize_numpunctEPi, _ZNSt8numpunctIcE22_M_initialize_numpunctEP15__locale_struct);
+#ifdef _GLIBCXX_USE_WCHAR_T
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8numpunctIwE22_M_initialize_numpunctEPi, _ZNSt8numpunctIwE22_M_initialize_numpunctEP15__locale_struct);
+#endif
diff --git a/libstdc++-v3/config/locale/gnu/time_members.cc b/libstdc++-v3/config/locale/gnu/time_members.cc
index 1974746..c275651 100644
--- a/libstdc++-v3/config/locale/gnu/time_members.cc
+++ b/libstdc++-v3/config/locale/gnu/time_members.cc
@@ -399,3 +399,16 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION
 
 _GLIBCXX_END_NAMESPACE_VERSION
 } // namespace
+
+/* Because of a bad cross-compilation fallback in a configure test,
+   Sourcery G++ toolchains for GNU/Linux targets formerly used the
+   "generic" locale model in libstdc++.  Improve compatibility with
+   those toolchains by exporting symbol aliases under the "generic"
+   names for the "gnu" functions.  */
+#define _GLIBCXX_LOCALE_COMPAT(generic, gnu) \
+  extern "C" void generic (void) __attribute__ ((alias (#gnu), weak))
+
+_GLIBCXX_LOCALE_COMPAT (_ZNSt11__timepunctIcE23_M_initialize_timepunctEPi, _ZNSt11__timepunctIcE23_M_initialize_timepunctEP15__locale_struct);
+#ifdef _GLIBCXX_USE_WCHAR_T
+_GLIBCXX_LOCALE_COMPAT (_ZNSt11__timepunctIwE23_M_initialize_timepunctEPi, _ZNSt11__timepunctIwE23_M_initialize_timepunctEP15__locale_struct);
+#endif
diff --git a/libstdc++-v3/configure b/libstdc++-v3/configure
index 84b6ea9..7f0f35f 100755
--- a/libstdc++-v3/configure
+++ b/libstdc++-v3/configure
@@ -729,6 +729,8 @@ LD
 FGREP
 SED
 LIBTOOL
+GLIBCXX_CSLIBC_FALSE
+GLIBCXX_CSLIBC_TRUE
 EGREP
 GREP
 CPP
@@ -834,6 +836,7 @@ with_target_subdir
 with_cross_host
 with_newlib
 enable_maintainer_mode
+with_cslibc
 enable_shared
 enable_static
 with_pic
@@ -1565,6 +1568,7 @@ Optional Packages:
                           configuring in a subdirectory
   --with-cross-host=HOST  configuring with a cross compiler
   --with-newlib           assume newlib as a system C library
+  --with-cslibc           assume CSLIBC as system C library
   --with-pic              try to use only PIC/non-PIC objects [default=use
                           both]
   --with-gnu-ld           assume the C compiler uses GNU ld [default=no]
@@ -5267,6 +5271,29 @@ $as_echo "$as_me: OS config directory is $os_include_dir" >&6;}
 
 
 
+# Check to see if we are using CSLIBC.
+
+
+# Check whether --with-cslibc was given.
+if test "${with_cslibc+set}" = set; then :
+  withval=$with_cslibc;
+else
+  with_cslibc=no
+fi
+
+  if test "$with_cslibc" = yes; then
+    cslibc_define=1
+  else
+    cslibc_define=0
+  fi
+
+
+cat >>confdefs.h <<_ACEOF
+#define _GLIBCXX_CSLIBC $cslibc_define
+_ACEOF
+
+
+
 if test "x${with_newlib}" != "xyes"; then
   enable_dlopen=yes
 
@@ -6346,6 +6377,10 @@ solaris*)
   lt_cv_deplibs_check_method=pass_all
   ;;
 
+symbian*)
+  lt_cv_deplibs_check_method=pass_all
+  ;;
+
 sysv5* | sco3.2v5* | sco5v6* | unixware* | OpenUNIX* | sysv4*uw2*)
   lt_cv_deplibs_check_method=pass_all
   ;;
@@ -10956,6 +11006,14 @@ sunos4*)
   need_version=yes
   ;;
 
+symbian*)
+  version_type=windows
+  shrext_cmds=".dll"
+  need_version=no
+  need_lib_prefix=no
+  library_names_spec='${libname}.dll'
+  ;;
+
 sysv4 | sysv4.3*)
   version_type=linux
   library_names_spec='${libname}${release}${shared_ext}$versuffix ${libname}${release}${shared_ext}$major $libname${shared_ext}'
@@ -11484,7 +11542,7 @@ else
   lt_dlunknown=0; lt_dlno_uscore=1; lt_dlneed_uscore=2
   lt_status=$lt_dlunknown
   cat > conftest.$ac_ext <<_LT_EOF
-#line 11487 "configure"
+#line 11545 "configure"
 #include "confdefs.h"
 
 #if HAVE_DLFCN_H
@@ -11590,7 +11648,7 @@ else
   lt_dlunknown=0; lt_dlno_uscore=1; lt_dlneed_uscore=2
   lt_status=$lt_dlunknown
   cat > conftest.$ac_ext <<_LT_EOF
-#line 11593 "configure"
+#line 11651 "configure"
 #include "confdefs.h"
 
 #if HAVE_DLFCN_H
@@ -13438,6 +13496,10 @@ $as_echo_n "checking for $compiler option to produce PIC... " >&6; }
       # Interix 3.x gcc -fpic/-fPIC options generate broken code.
       # Instead, we relocate shared libraries at runtime.
       ;;
+    symbian*)
+      # symbian does not have PIC, the loader relocates non-pic shared objects
+      lt_prog_compiler_pic_CXX=
+      ;;
     sysv4*MP*)
       if test -d /usr/nec; then
 	lt_prog_compiler_pic_CXX=-Kconform_pic
@@ -14639,6 +14716,14 @@ sunos4*)
   need_version=yes
   ;;
 
+symbian*)
+  version_type=windows
+  shrext_cmds=".dll"
+  need_version=no
+  need_lib_prefix=no
+  library_names_spec='${libname}.dll'
+  ;;
+
 sysv4 | sysv4.3*)
   version_type=linux
   library_names_spec='${libname}${release}${shared_ext}$versuffix ${libname}${release}${shared_ext}$major $libname${shared_ext}'
@@ -14948,7 +15033,7 @@ fi
     #
     # Fake what AC_TRY_COMPILE does.  XXX Look at redoing this new-style.
     cat > conftest.$ac_ext << EOF
-#line 14951 "configure"
+#line 15036 "configure"
 struct S { ~S(); };
 void bar();
 void foo()
@@ -15316,7 +15401,7 @@ $as_echo "$glibcxx_cv_atomic_long_long" >&6; }
   # Fake what AC_TRY_COMPILE does.
 
     cat > conftest.$ac_ext << EOF
-#line 15319 "configure"
+#line 15404 "configure"
 int main()
 {
   typedef bool atomic_type;
@@ -15353,7 +15438,7 @@ $as_echo "$glibcxx_cv_atomic_bool" >&6; }
     rm -f conftest*
 
     cat > conftest.$ac_ext << EOF
-#line 15356 "configure"
+#line 15441 "configure"
 int main()
 {
   typedef short atomic_type;
@@ -15390,7 +15475,7 @@ $as_echo "$glibcxx_cv_atomic_short" >&6; }
     rm -f conftest*
 
     cat > conftest.$ac_ext << EOF
-#line 15393 "configure"
+#line 15478 "configure"
 int main()
 {
   // NB: _Atomic_word not necessarily int.
@@ -15428,7 +15513,7 @@ $as_echo "$glibcxx_cv_atomic_int" >&6; }
     rm -f conftest*
 
     cat > conftest.$ac_ext << EOF
-#line 15431 "configure"
+#line 15516 "configure"
 int main()
 {
   typedef long long atomic_type;
@@ -15504,7 +15589,7 @@ $as_echo "$as_me: WARNING: Performance of certain classes will degrade as a resu
   # unnecessary for this test.
 
     cat > conftest.$ac_ext << EOF
-#line 15507 "configure"
+#line 15592 "configure"
 int main()
 {
   _Decimal32 d1;
@@ -18729,6 +18814,7 @@ ac_compiler_gnu=$ac_cv_c_compiler_gnu
 
 
 # For the EOF, SEEK_CUR, and SEEK_END integer constants.
+if test $is_hosted = yes; then
 
 
   { $as_echo "$as_me:${as_lineno-$LINENO}: checking for the value of EOF" >&5
@@ -18797,6 +18883,7 @@ cat >>confdefs.h <<_ACEOF
 _ACEOF
 
 
+fi
 
 # For gettimeofday support.
 
@@ -63870,7 +63957,8 @@ $as_echo_n "checking size of void *... " >&6; }
 if test "${ac_cv_sizeof_void_p+set}" = set; then :
   $as_echo_n "(cached) " >&6
 else
-  if ac_fn_c_compute_int "$LINENO" "(long int) (sizeof (void *))" "ac_cv_sizeof_void_p"        "$ac_includes_default"; then :
+  if ac_fn_c_compute_int "$LINENO" "(long int) (sizeof (void *))" "ac_cv_sizeof_void_p"        "/* no standard headers */
+"; then :
 
 else
   if test "$ac_cv_type_void_p" = yes; then
@@ -63908,7 +63996,8 @@ $as_echo_n "checking size of long... " >&6; }
 if test "${ac_cv_sizeof_long+set}" = set; then :
   $as_echo_n "(cached) " >&6
 else
-  if ac_fn_c_compute_int "$LINENO" "(long int) (sizeof (long))" "ac_cv_sizeof_long"        "$ac_includes_default"; then :
+  if ac_fn_c_compute_int "$LINENO" "(long int) (sizeof (long))" "ac_cv_sizeof_long"        "/* no standard headers */
+"; then :
 
 else
   if test "$ac_cv_type_long" = yes; then
@@ -63946,7 +64035,8 @@ $as_echo_n "checking size of int... " >&6; }
 if test "${ac_cv_sizeof_int+set}" = set; then :
   $as_echo_n "(cached) " >&6
 else
-  if ac_fn_c_compute_int "$LINENO" "(long int) (sizeof (int))" "ac_cv_sizeof_int"        "$ac_includes_default"; then :
+  if ac_fn_c_compute_int "$LINENO" "(long int) (sizeof (int))" "ac_cv_sizeof_int"        "/* no standard headers */
+"; then :
 
 else
   if test "$ac_cv_type_int" = yes; then
@@ -63980,7 +64070,8 @@ $as_echo_n "checking size of short... " >&6; }
 if test "${ac_cv_sizeof_short+set}" = set; then :
   $as_echo_n "(cached) " >&6
 else
-  if ac_fn_c_compute_int "$LINENO" "(long int) (sizeof (short))" "ac_cv_sizeof_short"        "$ac_includes_default"; then :
+  if ac_fn_c_compute_int "$LINENO" "(long int) (sizeof (short))" "ac_cv_sizeof_short"        "/* no standard headers */
+"; then :
 
 else
   if test "$ac_cv_type_short" = yes; then
@@ -64014,7 +64105,8 @@ $as_echo_n "checking size of char... " >&6; }
 if test "${ac_cv_sizeof_char+set}" = set; then :
   $as_echo_n "(cached) " >&6
 else
-  if ac_fn_c_compute_int "$LINENO" "(long int) (sizeof (char))" "ac_cv_sizeof_char"        "$ac_includes_default"; then :
+  if ac_fn_c_compute_int "$LINENO" "(long int) (sizeof (char))" "ac_cv_sizeof_char"        "/* no standard headers */
+"; then :
 
 else
   if test "$ac_cv_type_char" = yes; then
@@ -65412,6 +65504,15 @@ ABI_TWEAKS_SRCDIR=config/${abi_tweaks_dir}
 # Determine cross-compile flags and AM_CONDITIONALs.
 #AC_SUBST(GLIBCXX_IS_NATIVE)
 #AM_CONDITIONAL(CANADIAN, test $CANADIAN = yes)
+    if test $with_cslibc = yes; then
+  GLIBCXX_CSLIBC_TRUE=
+  GLIBCXX_CSLIBC_FALSE='#'
+else
+  GLIBCXX_CSLIBC_TRUE='#'
+  GLIBCXX_CSLIBC_FALSE=
+fi
+
+
     if test $is_hosted = yes; then
   GLIBCXX_HOSTED_TRUE=
   GLIBCXX_HOSTED_FALSE='#'
@@ -65910,6 +66011,10 @@ if test -z "${MAINTAINER_MODE_TRUE}" && test -z "${MAINTAINER_MODE_FALSE}"; then
   as_fn_error "conditional \"MAINTAINER_MODE\" was never defined.
 Usually this means the macro was only invoked conditionally." "$LINENO" 5
 fi
+if test -z "${GLIBCXX_CSLIBC_TRUE}" && test -z "${GLIBCXX_CSLIBC_FALSE}"; then
+  as_fn_error "conditional \"GLIBCXX_CSLIBC\" was never defined.
+Usually this means the macro was only invoked conditionally." "$LINENO" 5
+fi
 if test -z "${GLIBCXX_HOSTED_TRUE}" && test -z "${GLIBCXX_HOSTED_FALSE}"; then
   as_fn_error "conditional \"GLIBCXX_HOSTED\" was never defined.
 Usually this means the macro was only invoked conditionally." "$LINENO" 5
diff --git a/libstdc++-v3/configure.ac b/libstdc++-v3/configure.ac
index 427cf0b..f38687e 100644
--- a/libstdc++-v3/configure.ac
+++ b/libstdc++-v3/configure.ac
@@ -88,6 +88,9 @@ CXXFLAGS="$save_CXXFLAGS"
 # up critical shell variables.
 GLIBCXX_CONFIGURE
 
+# Check to see if we are using CSLIBC.
+GLIBCXX_WITH_CSLIBC
+
 if test "x${with_newlib}" != "xyes"; then
   AC_LIBTOOL_DLOPEN
 fi
@@ -152,7 +155,9 @@ GLIBCXX_CHECK_WRITEV
 GLIBCXX_CHECK_C99_TR1
 
 # For the EOF, SEEK_CUR, and SEEK_END integer constants.
-GLIBCXX_COMPUTE_STDIO_INTEGER_CONSTANTS
+if test $is_hosted = yes; then
+   GLIBCXX_COMPUTE_STDIO_INTEGER_CONSTANTS
+fi
 
 # For gettimeofday support.
 GLIBCXX_CHECK_GETTIMEOFDAY
diff --git a/libstdc++-v3/libsupc++/Makefile.am b/libstdc++-v3/libsupc++/Makefile.am
index 701c2d9..05d87bf 100644
--- a/libstdc++-v3/libsupc++/Makefile.am
+++ b/libstdc++-v3/libsupc++/Makefile.am
@@ -62,6 +62,7 @@ sources = \
 	eh_aux_runtime.cc \
 	eh_call.cc \
 	eh_catch.cc \
+	eh_compact_pr.cc \
 	eh_exception.cc \
 	eh_globals.cc \
 	eh_personality.cc \
@@ -86,7 +87,6 @@ sources = \
 	pbase_type_info.cc \
 	pmem_type_info.cc \
 	pointer_type_info.cc \
-	pure.cc \
 	si_class_type_info.cc \
 	tinfo.cc \
 	tinfo2.cc \
@@ -94,6 +94,13 @@ sources = \
 	vmi_class_type_info.cc \
 	vterminate.cc
 
+if !GLIBCXX_CSLIBC
+# CSLIBC (or, more accurately, CS3) provides its own implementation of
+# __cxa_pure_virtual.
+sources += \
+	pure.cc
+endif
+
 libsupc___la_SOURCES = $(sources) $(c_sources) 
 libsupc__convenience_la_SOURCES = $(sources) $(c_sources)
 
diff --git a/libstdc++-v3/libsupc++/Makefile.in b/libstdc++-v3/libsupc++/Makefile.in
index 18ba840..0442ad0 100644
--- a/libstdc++-v3/libsupc++/Makefile.in
+++ b/libstdc++-v3/libsupc++/Makefile.in
@@ -38,6 +38,12 @@ host_triplet = @host@
 target_triplet = @target@
 DIST_COMMON = $(top_srcdir)/fragment.am $(srcdir)/Makefile.in \
 	$(srcdir)/Makefile.am $(bits_HEADERS) $(std_HEADERS)
+
+# CSLIBC (or, more accurately, CS3) provides its own implementation of
+# __cxa_pure_virtual.
+@GLIBCXX_CSLIBC_FALSE@am__append_1 = \
+@GLIBCXX_CSLIBC_FALSE@	pure.cc
+
 subdir = libsupc++
 ACLOCAL_M4 = $(top_srcdir)/aclocal.m4
 am__aclocal_m4_deps = $(top_srcdir)/../config/acx.m4 \
@@ -90,23 +96,25 @@ am__installdirs = "$(DESTDIR)$(toolexeclibdir)" "$(DESTDIR)$(bitsdir)" \
 	"$(DESTDIR)$(stddir)"
 LTLIBRARIES = $(noinst_LTLIBRARIES) $(toolexeclib_LTLIBRARIES)
 libsupc___la_LIBADD =
-am__objects_1 = array_type_info.lo atexit_arm.lo bad_alloc.lo \
+@GLIBCXX_CSLIBC_FALSE@am__objects_1 = pure.lo
+am__objects_2 = array_type_info.lo atexit_arm.lo bad_alloc.lo \
 	bad_cast.lo bad_typeid.lo class_type_info.lo del_op.lo \
 	del_opnt.lo del_opv.lo del_opvnt.lo dyncast.lo eh_alloc.lo \
 	eh_arm.lo eh_aux_runtime.lo eh_call.lo eh_catch.lo \
-	eh_exception.lo eh_globals.lo eh_personality.lo eh_ptr.lo \
-	eh_term_handler.lo eh_terminate.lo eh_throw.lo eh_type.lo \
-	eh_unex_handler.lo enum_type_info.lo function_type_info.lo \
-	fundamental_type_info.lo guard.lo guard_error.lo hash_bytes.lo \
-	nested_exception.lo new_handler.lo new_op.lo new_opnt.lo \
-	new_opv.lo new_opvnt.lo pbase_type_info.lo pmem_type_info.lo \
-	pointer_type_info.lo pure.lo si_class_type_info.lo tinfo.lo \
-	tinfo2.lo vec.lo vmi_class_type_info.lo vterminate.lo
-@GLIBCXX_HOSTED_TRUE@am__objects_2 = cp-demangle.lo
-am_libsupc___la_OBJECTS = $(am__objects_1) $(am__objects_2)
+	eh_compact_pr.lo eh_exception.lo eh_globals.lo \
+	eh_personality.lo eh_ptr.lo eh_term_handler.lo eh_terminate.lo \
+	eh_throw.lo eh_type.lo eh_unex_handler.lo enum_type_info.lo \
+	function_type_info.lo fundamental_type_info.lo guard.lo \
+	guard_error.lo hash_bytes.lo nested_exception.lo \
+	new_handler.lo new_op.lo new_opnt.lo new_opv.lo new_opvnt.lo \
+	pbase_type_info.lo pmem_type_info.lo pointer_type_info.lo \
+	si_class_type_info.lo tinfo.lo tinfo2.lo vec.lo \
+	vmi_class_type_info.lo vterminate.lo $(am__objects_1)
+@GLIBCXX_HOSTED_TRUE@am__objects_3 = cp-demangle.lo
+am_libsupc___la_OBJECTS = $(am__objects_2) $(am__objects_3)
 libsupc___la_OBJECTS = $(am_libsupc___la_OBJECTS)
 libsupc__convenience_la_LIBADD =
-am_libsupc__convenience_la_OBJECTS = $(am__objects_1) $(am__objects_2)
+am_libsupc__convenience_la_OBJECTS = $(am__objects_2) $(am__objects_3)
 libsupc__convenience_la_OBJECTS =  \
 	$(am_libsupc__convenience_la_OBJECTS)
 DEFAULT_INCLUDES = -I.@am__isrc@ -I$(top_builddir)
@@ -351,55 +359,18 @@ headers = $(std_HEADERS) $(bits_HEADERS)
 @GLIBCXX_HOSTED_TRUE@c_sources = \
 @GLIBCXX_HOSTED_TRUE@	cp-demangle.c 
 
-sources = \
-	array_type_info.cc \
-	atexit_arm.cc \
-	bad_alloc.cc \
-	bad_cast.cc \
-	bad_typeid.cc \
-	class_type_info.cc \
-	del_op.cc \
-	del_opnt.cc \
-	del_opv.cc \
-	del_opvnt.cc \
-	dyncast.cc \
-	eh_alloc.cc \
-	eh_arm.cc \
-	eh_aux_runtime.cc \
-	eh_call.cc \
-	eh_catch.cc \
-	eh_exception.cc \
-	eh_globals.cc \
-	eh_personality.cc \
-	eh_ptr.cc \
-	eh_term_handler.cc \
-	eh_terminate.cc \
-	eh_throw.cc \
-	eh_type.cc \
-	eh_unex_handler.cc \
-	enum_type_info.cc \
-	function_type_info.cc \
-	fundamental_type_info.cc \
-	guard.cc \
-	guard_error.cc \
-	hash_bytes.cc \
-	nested_exception.cc \
-	new_handler.cc \
-	new_op.cc \
-	new_opnt.cc \
-	new_opv.cc \
-	new_opvnt.cc \
-	pbase_type_info.cc \
-	pmem_type_info.cc \
-	pointer_type_info.cc \
-	pure.cc \
-	si_class_type_info.cc \
-	tinfo.cc \
-	tinfo2.cc \
-	vec.cc \
-	vmi_class_type_info.cc \
-	vterminate.cc
-
+sources = array_type_info.cc atexit_arm.cc bad_alloc.cc bad_cast.cc \
+	bad_typeid.cc class_type_info.cc del_op.cc del_opnt.cc \
+	del_opv.cc del_opvnt.cc dyncast.cc eh_alloc.cc eh_arm.cc \
+	eh_aux_runtime.cc eh_call.cc eh_catch.cc eh_compact_pr.cc \
+	eh_exception.cc eh_globals.cc eh_personality.cc eh_ptr.cc \
+	eh_term_handler.cc eh_terminate.cc eh_throw.cc eh_type.cc \
+	eh_unex_handler.cc enum_type_info.cc function_type_info.cc \
+	fundamental_type_info.cc guard.cc guard_error.cc hash_bytes.cc \
+	nested_exception.cc new_handler.cc new_op.cc new_opnt.cc \
+	new_opv.cc new_opvnt.cc pbase_type_info.cc pmem_type_info.cc \
+	pointer_type_info.cc si_class_type_info.cc tinfo.cc tinfo2.cc \
+	vec.cc vmi_class_type_info.cc vterminate.cc $(am__append_1)
 libsupc___la_SOURCES = $(sources) $(c_sources) 
 libsupc__convenience_la_SOURCES = $(sources) $(c_sources)
 
diff --git a/libstdc++-v3/libsupc++/cxxabi.h b/libstdc++-v3/libsupc++/cxxabi.h
index c93085a..9419e14 100644
--- a/libstdc++-v3/libsupc++/cxxabi.h
+++ b/libstdc++-v3/libsupc++/cxxabi.h
@@ -619,6 +619,83 @@ namespace __gnu_cxx
     virtual ~recursive_init_error() throw ();
   };
 }
+
+#if defined(__arm__) && defined(__ARM_EABI__)
+
+// Also include the ARM specific routines.  This ensures they have
+// the correct visibility attributes.
+
+namespace __aeabiv1
+{
+  extern "C" void *
+  __aeabi_vec_ctor_nocookie_nodtor (void *array_address,
+				    abi::__cxa_cdtor_type constructor,
+				    size_t element_size,
+				    size_t element_count);
+  extern "C" void *
+  __aeabi_vec_ctor_cookie_nodtor (void *array_address,
+				  abi::__cxa_cdtor_type constructor,
+				  size_t element_size,
+				  size_t element_count);
+  
+  extern "C" void *
+  __aeabi_vec_cctor_nocookie_nodtor (void *dest_array,
+				     void *src_array, 
+				     size_t element_size, 
+				     size_t element_count,
+				     void *(*constructor) (void *, void *));
+
+  extern "C" void *
+  __aeabi_vec_new_cookie_noctor (size_t element_size, 
+				 size_t element_count);
+
+  extern "C" void *
+  __aeabi_vec_new_nocookie (size_t element_size, 
+			    size_t element_count,
+			    abi::__cxa_cdtor_type constructor);
+
+  extern "C" void *
+  __aeabi_vec_new_cookie_nodtor (size_t element_size, 
+				 size_t element_count,
+				 abi::__cxa_cdtor_type constructor);
+
+  extern "C" void *
+  __aeabi_vec_new_cookie(size_t element_size, 
+			 size_t element_count,
+			 abi::__cxa_cdtor_type constructor,
+			 abi::__cxa_cdtor_type destructor);
+  
+  extern "C" void *
+  __aeabi_vec_dtor (void *array_address, 
+		    abi::__cxa_cdtor_type destructor,
+		    size_t element_size, 
+		    size_t element_count);
+
+  extern "C" void *
+  __aeabi_vec_dtor_cookie (void *array_address, 
+			   abi::__cxa_cdtor_type destructor);
+  
+  extern "C" void
+  __aeabi_vec_delete (void *array_address, 
+		      abi::__cxa_cdtor_type destructor);
+
+  extern "C" void
+  __aeabi_vec_delete3 (void *array_address, 
+		       abi::__cxa_cdtor_type destructor,
+		       void (*dealloc) (void *, size_t));
+
+  extern "C" void
+  __aeabi_vec_delete3_nodtor (void *array_address,
+			      void (*dealloc) (void *, size_t));
+
+  extern "C" int
+  __aeabi_atexit (void *object, 
+		  void (*destructor) (void *),
+		  void *dso_handle) throw ();
+} // namespace __aeabiv1
+
+#endif // defined(__arm__) && defined(__ARM_EABI__)
+
 #endif // __cplusplus
 
 #pragma GCC visibility pop
diff --git a/libstdc++-v3/libsupc++/eh_arm.cc b/libstdc++-v3/libsupc++/eh_arm.cc
index f0acb70..550e0c3 100644
--- a/libstdc++-v3/libsupc++/eh_arm.cc
+++ b/libstdc++-v3/libsupc++/eh_arm.cc
@@ -22,6 +22,7 @@
 // see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see
 // <http://www.gnu.org/licenses/>.
 
+#include <typeinfo>
 #include <cxxabi.h>
 #include "unwind-cxx.h"
 
@@ -29,63 +30,63 @@
 
 using namespace __cxxabiv1;
 
-
-// Given the thrown type THROW_TYPE, pointer to a variable containing a
-// pointer to the exception object THROWN_PTR_P and a type CATCH_TYPE to
-// compare against, return whether or not there is a match and if so,
-// update *THROWN_PTR_P.
+// Given the thrown type THROW_TYPE, exception object UE_HEADER and a
+// type CATCH_TYPE to compare against, return whether or not there is
+// a match and if so, update *THROWN_PTR_P to point to either the
+// type-matched object, or in the case of a pointer type, the object
+// pointed to by the pointer.
 
 extern "C" __cxa_type_match_result
-__cxa_type_match(_Unwind_Exception* ue_header,
-		 const std::type_info* catch_type,
-		 bool is_reference __attribute__((__unused__)),
-		 void** thrown_ptr_p)
+__cxxabiv1::__cxa_type_match(_Unwind_Exception* ue_header,
+			     const std::type_info* catch_type,
+			     bool is_reference __attribute__((__unused__)),
+			     void** thrown_ptr_p)
 {
-  bool forced_unwind = __is_gxx_forced_unwind_class(ue_header->exception_class);
-  bool foreign_exception = !forced_unwind && !__is_gxx_exception_class(ue_header->exception_class);
-  bool dependent_exception =
-    __is_dependent_exception(ue_header->exception_class);
+  bool forced_unwind
+    = __is_gxx_forced_unwind_class(ue_header->exception_class);
+  bool foreign_exception
+    = !forced_unwind && !__is_gxx_exception_class(ue_header->exception_class);
+  bool dependent_exception
+    = __is_dependent_exception(ue_header->exception_class);
   __cxa_exception* xh = __get_exception_header_from_ue(ue_header);
   __cxa_dependent_exception *dx = __get_dependent_exception_from_ue(ue_header);
   const std::type_info* throw_type;
+  void *thrown_ptr = 0;
 
   if (forced_unwind)
     throw_type = &typeid(abi::__forced_unwind);
   else if (foreign_exception)
     throw_type = &typeid(abi::__foreign_exception);
-  else if (dependent_exception)
-    throw_type = __get_exception_header_from_obj
-      (dx->primaryException)->exceptionType;
   else
-    throw_type = xh->exceptionType;
-
-  void* thrown_ptr = *thrown_ptr_p;
+    {
+      if (dependent_exception)
+	xh = __get_exception_header_from_obj (dx->primaryException);
+      throw_type = xh->exceptionType;
+      // We used to require the caller set the target of thrown_ptr_p,
+      // but that's incorrect -- the EHABI makes no such requirement
+      // -- and not all callers will set it.  Fortunately callers that
+      // do initialize will always pass us the value we calculate
+      // here, so there's no backwards compatibility problem.
+      thrown_ptr = __get_object_from_ue (ue_header);
+    }
+  
+  __cxa_type_match_result result = ctm_succeeded;
 
   // Pointer types need to adjust the actual pointer, not
   // the pointer to pointer that is the exception object.
   // This also has the effect of passing pointer types
   // "by value" through the __cxa_begin_catch return value.
   if (throw_type->__is_pointer_p())
-    thrown_ptr = *(void**) thrown_ptr;
+    {
+      thrown_ptr = *(void**) thrown_ptr;
+      // We need to indicate the indirection to our caller.
+      result = ctm_succeeded_with_ptr_to_base;
+    }
 
   if (catch_type->__do_catch(throw_type, &thrown_ptr, 1))
     {
       *thrown_ptr_p = thrown_ptr;
-
-      if (typeid(*catch_type) == typeid (typeid(void*)))
-	{
-	  const __pointer_type_info *catch_pointer_type =
-	    static_cast<const __pointer_type_info *> (catch_type);
-	  const __pointer_type_info *throw_pointer_type =
-	    static_cast<const __pointer_type_info *> (throw_type);
-
-	  if (typeid (*catch_pointer_type->__pointee) != typeid (void)
-	      && (*catch_pointer_type->__pointee != 
-		  *throw_pointer_type->__pointee))
-	    return ctm_succeeded_with_ptr_to_base;
-	}
-
-      return ctm_succeeded;
+      return result;
     }
 
   return ctm_failed;
@@ -93,7 +94,7 @@ __cxa_type_match(_Unwind_Exception* ue_header,
 
 // ABI defined routine called at the start of a cleanup handler.
 extern "C" bool
-__cxa_begin_cleanup(_Unwind_Exception* ue_header)
+__cxxabiv1::__cxa_begin_cleanup(_Unwind_Exception* ue_header)
 {
   __cxa_eh_globals *globals = __cxa_get_globals();
   __cxa_exception *header = __get_exception_header_from_ue(ue_header);
diff --git a/libstdc++-v3/libsupc++/eh_call.cc b/libstdc++-v3/libsupc++/eh_call.cc
index f519f35..c75f59a 100644
--- a/libstdc++-v3/libsupc++/eh_call.cc
+++ b/libstdc++-v3/libsupc++/eh_call.cc
@@ -38,8 +38,10 @@ using namespace __cxxabiv1;
 // terminate.
 
 extern "C" void
-__cxa_call_terminate(_Unwind_Exception* ue_header) throw ()
-{
+__cxxabiv1::__cxa_call_terminate(__gnu_cxa_call_arg exc_obj_in) throw ()
+ {
+  _Unwind_Exception* ue_header
+    = reinterpret_cast<_Unwind_Exception*>(exc_obj_in);
 
   if (ue_header)
     {
@@ -65,7 +67,7 @@ __cxa_call_terminate(_Unwind_Exception* ue_header) throw ()
 // The ARM EABI __cxa_call_unexpected has the same semantics as the generic
 // routine, but the exception specification has a different format.
 extern "C" void
-__cxa_call_unexpected(void* exc_obj_in)
+__cxxabiv1::__cxa_call_unexpected(_Unwind_Control_Block* exc_obj_in)
 {
   _Unwind_Exception* exc_obj
     = reinterpret_cast<_Unwind_Exception*>(exc_obj_in);
diff --git a/libstdc++-v3/libsupc++/eh_personality.cc b/libstdc++-v3/libsupc++/eh_personality.cc
index 19c2044..3a59cff 100644
--- a/libstdc++-v3/libsupc++/eh_personality.cc
+++ b/libstdc++-v3/libsupc++/eh_personality.cc
@@ -247,6 +247,33 @@ get_adjusted_ptr (const std::type_info *catch_type,
   return false;
 }
 
+// Return true if THROW_TYPE matches one of the exception spec entries.
+static bool
+check_compact_exception_spec (_throw_typet* throw_type, void* thrown_ptr,
+			      const unsigned char *xh_lsda,
+			      _Unwind_Sword xh_switch_value,
+			      _Unwind_Ptr base)
+
+{
+  int i;
+  _uleb128_t no_of_types;
+  _Unwind_Ptr type_ptr;
+  const std::type_info* type_entry;
+  unsigned char encoding = xh_switch_value & 0xff;
+  const unsigned char *p = (const unsigned char *)
+			    xh_lsda + (xh_switch_value >> 8);
+
+  p = read_uleb128 (p, &no_of_types);
+  for (i = 0; i < (int) no_of_types; i++)
+    {
+      p = read_encoded_value_with_base (encoding, base, p, &type_ptr);
+      type_entry = reinterpret_cast<const std::type_info *>(type_ptr);
+      if (get_adjusted_ptr (type_entry, throw_type, &thrown_ptr))
+	return true;
+    }
+  return false;
+}
+
 // Return true if THROW_TYPE matches one if the filter types.
 
 static bool
@@ -347,6 +374,8 @@ namespace __cxxabiv1
 #define PERSONALITY_FUNCTION	__gxx_personality_v0
 #endif
 
+#pragma GCC visibility push(default)
+
 extern "C" _Unwind_Reason_Code
 #ifdef __ARM_EABI_UNWINDER__
 PERSONALITY_FUNCTION (_Unwind_State state,
@@ -384,6 +413,8 @@ PERSONALITY_FUNCTION (int version,
   switch (state & _US_ACTION_MASK)
     {
     case _US_VIRTUAL_UNWIND_FRAME:
+      if (state & _US_FORCE_UNWIND)
+	CONTINUE_UNWINDING;
       actions = _UA_SEARCH_PHASE;
       break;
 
@@ -718,6 +749,8 @@ PERSONALITY_FUNCTION (int version,
   return _URC_INSTALL_CONTEXT;
 }
 
+#pragma GCC visibility pop
+
 /* The ARM EABI implementation of __cxa_call_unexpected is in a
    different file so that the personality routine (PR) can be used
    standalone.  The generic routine shared datastructures with the PR
@@ -762,22 +795,41 @@ __cxa_call_unexpected (void *exc_obj_in)
       __cxa_exception *new_xh = globals->caughtExceptions;
       void *new_ptr = __get_object_from_ambiguous_exception (new_xh);
 
-      // We don't quite have enough stuff cached; re-parse the LSDA.
-      parse_lsda_header (0, xh_lsda, &info);
-
-      // If this new exception meets the exception spec, allow it.
-      if (check_exception_spec (&info, __get_exception_header_from_obj
-                                  (new_ptr)->exceptionType,
-				new_ptr, xh_switch_value))
-	__throw_exception_again;
+      if (xh_switch_value > 0)
+	{
+	  _throw_typet *new_type = __get_exception_header_from_obj
+				    (new_ptr)->exceptionType;
+	  if (check_compact_exception_spec (new_type, new_ptr, xh_lsda,
+					    xh_switch_value, info.ttype_base))
+	      __throw_exception_again;
+	}
+      else
+	{
+	  // We don't quite have enough stuff cached; re-parse the LSDA.
+	  parse_lsda_header (0, xh_lsda, &info);
+
+          // If this new exception meets the exception spec, allow it.
+	  if (check_exception_spec (&info, __get_exception_header_from_obj
+						(new_ptr)->exceptionType,
+						new_ptr, xh_switch_value))
+	    __throw_exception_again;
+	}
 
-      // If the exception spec allows std::bad_exception, throw that.
-      // We don't have a thrown object to compare against, but since
-      // bad_exception doesn't have virtual bases, that's OK; just pass 0.
+// If the exception spec allows std::bad_exception, throw that.
+// We don't have a thrown object to compare against, but since
+// bad_exception doesn't have virtual bases, that's OK; just pass 0.
 #if defined(__EXCEPTIONS) && defined(__GXX_RTTI)
       const std::type_info &bad_exc = typeid (std::bad_exception);
-      if (check_exception_spec (&info, &bad_exc, 0, xh_switch_value))
-	throw std::bad_exception();
+      if (xh_switch_value > 0)
+	{
+	  if (check_compact_exception_spec (&bad_exc, 0, xh_lsda,
+					    xh_switch_value, info.ttype_base))
+	    throw std::bad_exception();
+	}
+      else if (check_exception_spec (&info, &bad_exc, 0, xh_switch_value))
+	{
+	  throw std::bad_exception();
+	}
 #endif   
 
       // Otherwise, die.
diff --git a/libstdc++-v3/libsupc++/eh_ptr.cc b/libstdc++-v3/libsupc++/eh_ptr.cc
index 94c2842..5a5e762 100644
--- a/libstdc++-v3/libsupc++/eh_ptr.cc
+++ b/libstdc++-v3/libsupc++/eh_ptr.cc
@@ -203,8 +203,8 @@ std::rethrow_exception(std::exception_ptr ep)
   dep->primaryException = obj;
   __sync_add_and_fetch (&eh->referenceCount, 1);
 
-  dep->unexpectedHandler = __unexpected_handler;
-  dep->terminateHandler = __terminate_handler;
+  dep->unexpectedHandler = __get_unexpected_handler ();
+  dep->terminateHandler = __get_terminate_handler ();
   __GXX_INIT_DEPENDENT_EXCEPTION_CLASS(dep->unwindHeader.exception_class);
   dep->unwindHeader.exception_cleanup = __gxx_dependent_exception_cleanup;
 
diff --git a/libstdc++-v3/libsupc++/eh_term_handler.cc b/libstdc++-v3/libsupc++/eh_term_handler.cc
index 52a0745..4bf590a 100644
--- a/libstdc++-v3/libsupc++/eh_term_handler.cc
+++ b/libstdc++-v3/libsupc++/eh_term_handler.cc
@@ -36,11 +36,21 @@
 # include <cstdlib>
 #endif
 
-/* The current installed user handler.  */
-std::terminate_handler __cxxabiv1::__terminate_handler =
 #if _GLIBCXX_HOSTED
-	__gnu_cxx::__verbose_terminate_handler;
+#define DEFAULT_TERMINATE_HANDLER __gnu_cxx::__verbose_terminate_handler
 #else
-	std::abort;
+#define DEFAULT_TERMINATE_HANDLER std::abort
 #endif
+/* The current installed user handler.  */
+#ifdef __symbian__
+/* SymbianOS does not allow initialized data, so use a constructor function.  */
+std::terminate_handler __cxxabiv1::__terminate_handler;
 
+void __cxxabiv1::__init_terminate_handler(void)
+{
+  __cxxabiv1::__terminate_handler = DEFAULT_TERMINATE_HANDLER;
+}
+#else /* !__symbian__ */
+std::terminate_handler __cxxabiv1::__terminate_handler =
+  DEFAULT_TERMINATE_HANDLER;
+#endif
diff --git a/libstdc++-v3/libsupc++/eh_terminate.cc b/libstdc++-v3/libsupc++/eh_terminate.cc
index 87359d0..bd43820 100644
--- a/libstdc++-v3/libsupc++/eh_terminate.cc
+++ b/libstdc++-v3/libsupc++/eh_terminate.cc
@@ -47,7 +47,7 @@ __cxxabiv1::__terminate (std::terminate_handler handler) throw ()
 void
 std::terminate () throw()
 {
-  __terminate (__terminate_handler);
+  __terminate (__get_terminate_handler ());
 }
 
 void
@@ -60,7 +60,7 @@ __cxxabiv1::__unexpected (std::unexpected_handler handler)
 void
 std::unexpected ()
 {
-  __unexpected (__unexpected_handler);
+  __unexpected (__get_unexpected_handler ());
 }
 
 std::terminate_handler
diff --git a/libstdc++-v3/libsupc++/eh_throw.cc b/libstdc++-v3/libsupc++/eh_throw.cc
index 78cfc13..45f17d1 100644
--- a/libstdc++-v3/libsupc++/eh_throw.cc
+++ b/libstdc++-v3/libsupc++/eh_throw.cc
@@ -67,8 +67,8 @@ __cxxabiv1::__cxa_throw (void *obj, std::type_info *tinfo,
   header->referenceCount = 1;
   header->exc.exceptionType = tinfo;
   header->exc.exceptionDestructor = dest;
-  header->exc.unexpectedHandler = __unexpected_handler;
-  header->exc.terminateHandler = __terminate_handler;
+  header->exc.unexpectedHandler = __get_unexpected_handler ();
+  header->exc.terminateHandler = __get_terminate_handler ();
   __GXX_INIT_PRIMARY_EXCEPTION_CLASS(header->exc.unwindHeader.exception_class);
   header->exc.unwindHeader.exception_cleanup = __gxx_exception_cleanup;
 
diff --git a/libstdc++-v3/libsupc++/eh_unex_handler.cc b/libstdc++-v3/libsupc++/eh_unex_handler.cc
index e4f3906..d048d12 100644
--- a/libstdc++-v3/libsupc++/eh_unex_handler.cc
+++ b/libstdc++-v3/libsupc++/eh_unex_handler.cc
@@ -25,5 +25,14 @@
 #include "unwind-cxx.h"
 
 /* The current installed user handler.  */
-std::unexpected_handler __cxxabiv1::__unexpected_handler = std::terminate;
+#ifdef __symbian__
+/* SymbianOS does not allow initialized data, so use a constructor function.  */
+std::unexpected_handler __cxxabiv1::__unexpected_handler;
 
+void __cxxabiv1::__init_unexpected_handler (void)
+{
+  __cxxabiv1::__unexpected_handler = std::terminate;
+}
+#else /* !__symbian__ */
+std::unexpected_handler __cxxabiv1::__unexpected_handler = std::terminate;
+#endif
diff --git a/libstdc++-v3/libsupc++/guard.cc b/libstdc++-v3/libsupc++/guard.cc
index 6e3d415..8e90424 100644
--- a/libstdc++-v3/libsupc++/guard.cc
+++ b/libstdc++-v3/libsupc++/guard.cc
@@ -203,7 +203,10 @@ namespace __cxxabiv1
   static inline void
   throw_recursive_init_exception()
   {
-#ifdef __EXCEPTIONS
+    // When building for use with CSLIBC, avoid the overhead of throwing
+    // an exception.  Recursive initialization is undefined behavior, so
+    // crashing is acceptable.
+#if defined(__EXCEPTIONS) && !_GLIBCXX_CSLIBC
 	throw __gnu_cxx::recursive_init_error();
 #else
 	// Use __builtin_trap so we don't require abort().
diff --git a/libstdc++-v3/libsupc++/unwind-cxx.h b/libstdc++-v3/libsupc++/unwind-cxx.h
index e62ea7c..90746b9 100644
--- a/libstdc++-v3/libsupc++/unwind-cxx.h
+++ b/libstdc++-v3/libsupc++/unwind-cxx.h
@@ -186,13 +186,6 @@ extern "C" void __cxa_bad_typeid () __attribute__((__noreturn__));
 
 // @@@ These are not directly specified by the IA-64 C++ ABI.
 
-// Handles re-checking the exception specification if unexpectedHandler
-// throws, and if bad_exception needs to be thrown.  Called from the
-// compiler.
-extern "C" void __cxa_call_unexpected (void *) __attribute__((__noreturn__));
-extern "C" void __cxa_call_terminate (_Unwind_Exception*) throw ()
-  __attribute__((__noreturn__));
-
 #ifdef __ARM_EABI_UNWINDER__
 // Arm EABI specified routines.
 typedef enum {
@@ -200,13 +193,24 @@ typedef enum {
   ctm_succeeded = 1,
   ctm_succeeded_with_ptr_to_base = 2
 } __cxa_type_match_result;
-extern "C" __cxa_type_match_result __cxa_type_match(_Unwind_Exception*,
-						    const std::type_info*,
-						    bool, void**);
+extern "C" __cxa_type_match_result
+__cxa_type_match (_Unwind_Exception*, const std::type_info*, bool, void**);
 extern "C" bool __cxa_begin_cleanup (_Unwind_Exception*);
 extern "C" void __cxa_end_cleanup (void);
+#define __gnu_cxa_call_arg _Unwind_Control_Block*
+#else
+#define __gnu_cxa_call_arg void*
 #endif
 
+// Handles re-checking the exception specification if unexpectedHandler
+// throws, and if bad_exception needs to be thrown.  Called from the
+// compiler.
+#define __cxa_call_arg
+extern "C" void
+__cxa_call_unexpected (__gnu_cxa_call_arg) __attribute__((noreturn));
+extern "C" void
+__cxa_call_terminate (__gnu_cxa_call_arg) throw () __attribute__((noreturn));
+
 // Invokes given handler, dying appropriately if the user handler was
 // so inconsiderate as to return.
 extern void __terminate(std::terminate_handler) throw () 
@@ -218,6 +222,29 @@ extern void __unexpected(std::unexpected_handler)
 extern std::terminate_handler __terminate_handler;
 extern std::unexpected_handler __unexpected_handler;
 
+#ifdef __symbian__
+extern "C" void __init_terminate_handler(void);
+extern "C" void __init_unexpected_handler(void);
+#endif
+
+static inline std::terminate_handler __get_terminate_handler(void)
+{
+#ifdef __symbian__
+  if (!__terminate_handler)
+    __init_terminate_handler();
+#endif
+  return __terminate_handler;
+}
+
+static inline std::unexpected_handler __get_unexpected_handler(void)
+{
+#ifdef __symbian__
+  if (!__unexpected_handler)
+    __init_unexpected_handler();
+#endif
+  return __unexpected_handler;
+}
+
 // These are explicitly GNU C++ specific.
 
 // Acquire the C++ exception header from the C++ object.
@@ -383,6 +410,16 @@ extern "C" _Unwind_Reason_Code __gxx_personality_v0
      (int, _Unwind_Action, _Unwind_Exception_Class,
       struct _Unwind_Exception *, struct _Unwind_Context *);
 
+// GNU C++ compact eh personality routine.
+extern "C" _Unwind_Reason_Code __gnu_compact_pr2
+     (int, _Unwind_Action, _Unwind_Exception_Class,
+      struct _Unwind_Exception *, struct _Unwind_Context *);
+
+// GNU C++ compact eh personality routine.
+extern "C" _Unwind_Reason_Code __gnu_compact_pr3
+     (int, _Unwind_Action, _Unwind_Exception_Class,
+      struct _Unwind_Exception *, struct _Unwind_Context *);
+
 // GNU C++ sjlj personality routine, Version 0.
 extern "C" _Unwind_Reason_Code __gxx_personality_sj0
      (int, _Unwind_Action, _Unwind_Exception_Class,
diff --git a/libstdc++-v3/python/Makefile.am b/libstdc++-v3/python/Makefile.am
index b603444..10368d7 100644
--- a/libstdc++-v3/python/Makefile.am
+++ b/libstdc++-v3/python/Makefile.am
@@ -23,7 +23,7 @@
 include $(top_srcdir)/fragment.am
 
 ## Where to install the module code.
-pythondir = $(datadir)/gcc-$(gcc_version)/python
+pythondir = $(tooldir)/share/gcc-$(gcc_version)/python
 
 all-local: gdb.py
 
@@ -36,6 +36,7 @@ gdb.py: hook.in Makefile
 	sed -e 's,@pythondir@,$(pythondir),' \
 	    -e 's,@toolexeclibdir@,$(toolexeclibdir),' < $(srcdir)/hook.in > $@
 
+if GLIBCXX_HOSTED
 install-data-local: gdb.py
 	@$(mkdir_p) $(DESTDIR)$(toolexeclibdir)
 ## We want to install gdb.py as SOMETHING-gdb.py.  SOMETHING is the
@@ -57,3 +58,4 @@ install-data-local: gdb.py
 	cd $$here; \
 	echo " $(INSTALL_DATA) gdb.py $(DESTDIR)$(toolexeclibdir)/$$libname-gdb.py"; \
 	$(INSTALL_DATA) gdb.py $(DESTDIR)$(toolexeclibdir)/$$libname-gdb.py
+endif
diff --git a/libstdc++-v3/python/Makefile.in b/libstdc++-v3/python/Makefile.in
index b8ea31a..b510b0a 100644
--- a/libstdc++-v3/python/Makefile.in
+++ b/libstdc++-v3/python/Makefile.in
@@ -298,7 +298,7 @@ WARN_CXXFLAGS = \
 
 # -I/-D flags to pass when compiling.
 AM_CPPFLAGS = $(GLIBCXX_INCLUDES)
-pythondir = $(datadir)/gcc-$(gcc_version)/python
+pythondir = $(tooldir)/share/gcc-$(gcc_version)/python
 nobase_python_DATA = \
     libstdcxx/v6/printers.py \
     libstdcxx/v6/__init__.py \
@@ -404,6 +404,7 @@ distclean-generic:
 maintainer-clean-generic:
 	@echo "This command is intended for maintainers to use"
 	@echo "it deletes files that may require special tools to rebuild."
+@GLIBCXX_HOSTED_FALSE@install-data-local:
 clean: clean-am
 
 clean-am: clean-generic clean-libtool mostlyclean-am
@@ -492,22 +493,22 @@ gdb.py: hook.in Makefile
 	sed -e 's,@pythondir@,$(pythondir),' \
 	    -e 's,@toolexeclibdir@,$(toolexeclibdir),' < $(srcdir)/hook.in > $@
 
-install-data-local: gdb.py
-	@$(mkdir_p) $(DESTDIR)$(toolexeclibdir)
-	@here=`pwd`; cd $(DESTDIR)$(toolexeclibdir); \
-	  for file in libstdc++*; do \
-	    case $$file in \
-	      *-gdb.py) ;; \
-	      *.la) ;; \
-	      *) if test -h $$file; then \
-	           continue; \
-	         fi; \
-	         libname=$$file;; \
-	    esac; \
-	  done; \
-	cd $$here; \
-	echo " $(INSTALL_DATA) gdb.py $(DESTDIR)$(toolexeclibdir)/$$libname-gdb.py"; \
-	$(INSTALL_DATA) gdb.py $(DESTDIR)$(toolexeclibdir)/$$libname-gdb.py
+@GLIBCXX_HOSTED_TRUE@install-data-local: gdb.py
+@GLIBCXX_HOSTED_TRUE@	@$(mkdir_p) $(DESTDIR)$(toolexeclibdir)
+@GLIBCXX_HOSTED_TRUE@	@here=`pwd`; cd $(DESTDIR)$(toolexeclibdir); \
+@GLIBCXX_HOSTED_TRUE@	  for file in libstdc++*; do \
+@GLIBCXX_HOSTED_TRUE@	    case $$file in \
+@GLIBCXX_HOSTED_TRUE@	      *-gdb.py) ;; \
+@GLIBCXX_HOSTED_TRUE@	      *.la) ;; \
+@GLIBCXX_HOSTED_TRUE@	      *) if test -h $$file; then \
+@GLIBCXX_HOSTED_TRUE@	           continue; \
+@GLIBCXX_HOSTED_TRUE@	         fi; \
+@GLIBCXX_HOSTED_TRUE@	         libname=$$file;; \
+@GLIBCXX_HOSTED_TRUE@	    esac; \
+@GLIBCXX_HOSTED_TRUE@	  done; \
+@GLIBCXX_HOSTED_TRUE@	cd $$here; \
+@GLIBCXX_HOSTED_TRUE@	echo " $(INSTALL_DATA) gdb.py $(DESTDIR)$(toolexeclibdir)/$$libname-gdb.py"; \
+@GLIBCXX_HOSTED_TRUE@	$(INSTALL_DATA) gdb.py $(DESTDIR)$(toolexeclibdir)/$$libname-gdb.py
 
 # Tell versions [3.59,3.63) of GNU make to not export all variables.
 # Otherwise a system limit (for SysV at least) may be exceeded.
diff --git a/libstdc++-v3/src/codecvt.cc b/libstdc++-v3/src/codecvt.cc
index fdb0896..9be3df9 100644
--- a/libstdc++-v3/src/codecvt.cc
+++ b/libstdc++-v3/src/codecvt.cc
@@ -149,3 +149,29 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION
 
 _GLIBCXX_END_NAMESPACE_VERSION
 } // namespace
+
+#if _GLIBCXX_C_LOCALE_GNU
+/* Because of a bad cross-compilation fallback in a configure test,
+   Sourcery G++ toolchains for GNU/Linux targets formerly used the
+   "generic" locale model in libstdc++.  Improve compatibility with
+   those toolchains by exporting symbol aliases under the "generic"
+   names for the "gnu" functions.  */
+#define _GLIBCXX_LOCALE_COMPAT(generic, gnu) \
+  extern "C" void generic (void) __attribute__ ((alias (#gnu), weak))
+
+#ifdef _GLIBCXX_SIZE_T_IS_UINT
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7codecvtIcc11__mbstate_tEC1EPij, _ZNSt7codecvtIcc11__mbstate_tEC1EP15__locale_structj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7codecvtIcc11__mbstate_tEC2EPij, _ZNSt7codecvtIcc11__mbstate_tEC2EP15__locale_structj);
+#ifdef _GLIBCXX_USE_WCHAR_T
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7codecvtIwc11__mbstate_tEC1EPij, _ZNSt7codecvtIwc11__mbstate_tEC1EP15__locale_structj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7codecvtIwc11__mbstate_tEC2EPij, _ZNSt7codecvtIwc11__mbstate_tEC2EP15__locale_structj);
+#endif
+#else
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7codecvtIcc11__mbstate_tEC1EPim, _ZNSt7codecvtIcc11__mbstate_tEC1EP15__locale_structm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7codecvtIcc11__mbstate_tEC2EPim, _ZNSt7codecvtIcc11__mbstate_tEC2EP15__locale_structm);
+#ifdef _GLIBCXX_USE_WCHAR_T
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7codecvtIwc11__mbstate_tEC1EPim, _ZNSt7codecvtIwc11__mbstate_tEC1EP15__locale_structm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7codecvtIwc11__mbstate_tEC2EPim, _ZNSt7codecvtIwc11__mbstate_tEC2EP15__locale_structm);
+#endif
+#endif
+#endif
diff --git a/libstdc++-v3/src/ctype.cc b/libstdc++-v3/src/ctype.cc
index 04d31cc..e88f1f2 100644
--- a/libstdc++-v3/src/ctype.cc
+++ b/libstdc++-v3/src/ctype.cc
@@ -137,3 +137,29 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION
 
 _GLIBCXX_END_NAMESPACE_VERSION
 } // namespace
+
+#if _GLIBCXX_C_LOCALE_GNU
+/* Because of a bad cross-compilation fallback in a configure test,
+   Sourcery G++ toolchains for GNU/Linux targets formerly used the
+   "generic" locale model in libstdc++.  Improve compatibility with
+   those toolchains by exporting symbol aliases under the "generic"
+   names for the "gnu" functions.  */
+#define _GLIBCXX_LOCALE_COMPAT(generic, gnu) \
+  extern "C" void generic (void) __attribute__ ((alias (#gnu), weak))
+
+#ifdef _GLIBCXX_SIZE_T_IS_UINT
+_GLIBCXX_LOCALE_COMPAT (_ZNSt5ctypeIcEC1EPiPKtbj, _ZNSt5ctypeIcEC1EP15__locale_structPKtbj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt5ctypeIcEC2EPiPKtbj, _ZNSt5ctypeIcEC2EP15__locale_structPKtbj);
+#ifdef _GLIBCXX_USE_WCHAR_T
+_GLIBCXX_LOCALE_COMPAT (_ZNSt5ctypeIwEC1EPij, _ZNSt5ctypeIwEC1EP15__locale_structj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt5ctypeIwEC2EPij, _ZNSt5ctypeIwEC2EP15__locale_structj);
+#endif
+#else
+_GLIBCXX_LOCALE_COMPAT (_ZNSt5ctypeIcEC1EPiPKtbm, _ZNSt5ctypeIcEC1EP15__locale_structPKtbm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt5ctypeIcEC2EPiPKtbm, _ZNSt5ctypeIcEC2EP15__locale_structPKtbm);
+#ifdef _GLIBCXX_USE_WCHAR_T
+_GLIBCXX_LOCALE_COMPAT (_ZNSt5ctypeIwEC1EPim, _ZNSt5ctypeIwEC1EP15__locale_structm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt5ctypeIwEC2EPim, _ZNSt5ctypeIwEC2EP15__locale_structm);
+#endif
+#endif
+#endif
diff --git a/libstdc++-v3/src/locale-inst.cc b/libstdc++-v3/src/locale-inst.cc
index e77c5c9..7c372fb 100644
--- a/libstdc++-v3/src/locale-inst.cc
+++ b/libstdc++-v3/src/locale-inst.cc
@@ -362,3 +362,41 @@ _GLIBCXX_LDBL_COMPAT(_ZNKSt17__gnu_cxx_ldbl1289money_putIcSt19ostreambuf_iterato
 		     _ZNKSt9money_putIcSt19ostreambuf_iteratorIcSt11char_traitsIcEEE9_M_insertILb1EEES3_S3_RSt8ios_basecRKSs);
 
 #endif // _GLIBCXX_LONG_DOUBLE_COMPAT
+
+#if _GLIBCXX_C_LOCALE_GNU && defined C_is_char
+/* Because of a bad cross-compilation fallback in a configure test,
+   Sourcery G++ toolchains for GNU/Linux targets formerly used the
+   "generic" locale model in libstdc++.  Improve compatibility with
+   those toolchains by exporting symbol aliases under the "generic"
+   names for the "gnu" functions.  */
+#define _GLIBCXX_LOCALE_COMPAT(generic, gnu) \
+  extern "C" void generic (void) __attribute__ ((alias (#gnu), weak))
+
+#ifdef _GLIBCXX_SIZE_T_IS_UINT
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIcLb0EEC1EPiPKcj, _ZNSt10moneypunctIcLb0EEC1EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIcLb0EEC2EPiPKcj, _ZNSt10moneypunctIcLb0EEC2EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIcLb1EEC1EPiPKcj, _ZNSt10moneypunctIcLb1EEC1EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIcLb1EEC2EPiPKcj, _ZNSt10moneypunctIcLb1EEC2EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt11__timepunctIcEC1EPiPKcj, _ZNSt11__timepunctIcEC1EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt11__timepunctIcEC2EPiPKcj, _ZNSt11__timepunctIcEC2EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7collateIcEC1EPij, _ZNSt7collateIcEC1EP15__locale_structj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7collateIcEC2EPij, _ZNSt7collateIcEC2EP15__locale_structj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8messagesIcEC1EPiPKcj, _ZNSt8messagesIcEC1EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8messagesIcEC2EPiPKcj, _ZNSt8messagesIcEC2EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8numpunctIcEC1EPij, _ZNSt8numpunctIcEC1EP15__locale_structj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8numpunctIcEC2EPij, _ZNSt8numpunctIcEC2EP15__locale_structj);
+#else
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIcLb0EEC1EPiPKcm, _ZNSt10moneypunctIcLb0EEC1EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIcLb0EEC2EPiPKcm, _ZNSt10moneypunctIcLb0EEC2EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIcLb1EEC1EPiPKcm, _ZNSt10moneypunctIcLb1EEC1EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIcLb1EEC2EPiPKcm, _ZNSt10moneypunctIcLb1EEC2EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt11__timepunctIcEC1EPiPKcm, _ZNSt11__timepunctIcEC1EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt11__timepunctIcEC2EPiPKcm, _ZNSt11__timepunctIcEC2EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7collateIcEC1EPim, _ZNSt7collateIcEC1EP15__locale_structm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7collateIcEC2EPim, _ZNSt7collateIcEC2EP15__locale_structm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8messagesIcEC1EPiPKcm, _ZNSt8messagesIcEC1EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8messagesIcEC2EPiPKcm, _ZNSt8messagesIcEC2EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8numpunctIcEC1EPim, _ZNSt8numpunctIcEC1EP15__locale_structm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8numpunctIcEC2EPim, _ZNSt8numpunctIcEC2EP15__locale_structm);
+#endif
+#endif
diff --git a/libstdc++-v3/src/wlocale-inst.cc b/libstdc++-v3/src/wlocale-inst.cc
index cdfed0c..23faf04 100644
--- a/libstdc++-v3/src/wlocale-inst.cc
+++ b/libstdc++-v3/src/wlocale-inst.cc
@@ -73,4 +73,42 @@ _GLIBCXX_LDBL_COMPAT(_ZNKSt17__gnu_cxx_ldbl1289money_putIwSt19ostreambuf_iterato
 		     _ZNKSt9money_putIwSt19ostreambuf_iteratorIwSt11char_traitsIwEEE9_M_insertILb1EEES3_S3_RSt8ios_basewRKSbIwS2_SaIwEE);
 
 #endif // _GLIBCXX_LONG_DOUBLE_COMPAT
+
+#if _GLIBCXX_C_LOCALE_GNU
+/* Because of a bad cross-compilation fallback in a configure test,
+   Sourcery G++ toolchains for GNU/Linux targets formerly used the
+   "generic" locale model in libstdc++.  Improve compatibility with
+   those toolchains by exporting symbol aliases under the "generic"
+   names for the "gnu" functions.  */
+#define _GLIBCXX_LOCALE_COMPAT(generic, gnu) \
+  extern "C" void generic (void) __attribute__ ((alias (#gnu), weak))
+
+#ifdef _GLIBCXX_SIZE_T_IS_UINT
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIwLb0EEC1EPiPKcj, _ZNSt10moneypunctIwLb0EEC1EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIwLb0EEC2EPiPKcj, _ZNSt10moneypunctIwLb0EEC2EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIwLb1EEC1EPiPKcj, _ZNSt10moneypunctIwLb1EEC1EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIwLb1EEC2EPiPKcj, _ZNSt10moneypunctIwLb1EEC2EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt11__timepunctIwEC1EPiPKcj, _ZNSt11__timepunctIwEC1EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt11__timepunctIwEC2EPiPKcj, _ZNSt11__timepunctIwEC2EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7collateIwEC1EPij, _ZNSt7collateIwEC1EP15__locale_structj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7collateIwEC2EPij, _ZNSt7collateIwEC2EP15__locale_structj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8messagesIwEC1EPiPKcj, _ZNSt8messagesIwEC1EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8messagesIwEC2EPiPKcj, _ZNSt8messagesIwEC2EP15__locale_structPKcj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8numpunctIwEC1EPij, _ZNSt8numpunctIwEC1EP15__locale_structj);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8numpunctIwEC2EPij, _ZNSt8numpunctIwEC2EP15__locale_structj);
+#else
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIwLb0EEC1EPiPKcm, _ZNSt10moneypunctIwLb0EEC1EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIwLb0EEC2EPiPKcm, _ZNSt10moneypunctIwLb0EEC2EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIwLb1EEC1EPiPKcm, _ZNSt10moneypunctIwLb1EEC1EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt10moneypunctIwLb1EEC2EPiPKcm, _ZNSt10moneypunctIwLb1EEC2EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt11__timepunctIwEC1EPiPKcm, _ZNSt11__timepunctIwEC1EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt11__timepunctIwEC2EPiPKcm, _ZNSt11__timepunctIwEC2EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7collateIwEC1EPim, _ZNSt7collateIwEC1EP15__locale_structm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt7collateIwEC2EPim, _ZNSt7collateIwEC2EP15__locale_structm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8messagesIwEC1EPiPKcm, _ZNSt8messagesIwEC1EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8messagesIwEC2EPiPKcm, _ZNSt8messagesIwEC2EP15__locale_structPKcm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8numpunctIwEC1EPim, _ZNSt8numpunctIwEC1EP15__locale_structm);
+_GLIBCXX_LOCALE_COMPAT (_ZNSt8numpunctIwEC2EPim, _ZNSt8numpunctIwEC2EP15__locale_structm);
+#endif
+#endif
 #endif
diff --git a/libstdc++-v3/testsuite/25_algorithms/random_shuffle/moveable.cc b/libstdc++-v3/testsuite/25_algorithms/random_shuffle/moveable.cc
index cbf170b..1119d0d 100644
--- a/libstdc++-v3/testsuite/25_algorithms/random_shuffle/moveable.cc
+++ b/libstdc++-v3/testsuite/25_algorithms/random_shuffle/moveable.cc
@@ -1,4 +1,5 @@
 // { dg-options "-std=gnu++0x" }
+// { dg-add-options large_stack }
 
 // Copyright (C) 2009, 2010 Free Software Foundation, Inc.
 //
diff --git a/libtool.m4 b/libtool.m4
index 67321a7..052ec8c 100644
--- a/libtool.m4
+++ b/libtool.m4
@@ -2593,6 +2608,14 @@ sunos4*)
   need_version=yes
   ;;
 
+symbian*)
+  version_type=windows
+  shrext_cmds=".dll"
+  need_version=no
+  need_lib_prefix=no
+  library_names_spec='${libname}.dll'
+  ;;
+
 sysv4 | sysv4.3*)
   version_type=linux
   library_names_spec='${libname}${release}${shared_ext}$versuffix ${libname}${release}${shared_ext}$major $libname${shared_ext}'
@@ -3118,6 +3145,10 @@ solaris*)
   lt_cv_deplibs_check_method=pass_all
   ;;
 
+symbian*)
+  lt_cv_deplibs_check_method=pass_all
+  ;;
+
 sysv5* | sco3.2v5* | sco5v6* | unixware* | OpenUNIX* | sysv4*uw2*)
   lt_cv_deplibs_check_method=pass_all
   ;;
@@ -3619,6 +3650,10 @@ m4_if([$1], [CXX], [
       # Interix 3.x gcc -fpic/-fPIC options generate broken code.
       # Instead, we relocate shared libraries at runtime.
       ;;
+    symbian*)
+      # symbian does not have PIC, the loader relocates non-pic shared objects
+      _LT_TAGVAR(lt_prog_compiler_pic, $1)=
+      ;;
     sysv4*MP*)
       if test -d /usr/nec; then
 	_LT_TAGVAR(lt_prog_compiler_pic, $1)=-Kconform_pic
